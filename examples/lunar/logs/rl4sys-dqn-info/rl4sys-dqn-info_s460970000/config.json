{
    "__class__":	"DQN",
    "act_dim":	4,
    "aux_batch_size":	10,
    "batch_size":	54,
    "buf_size":	50000,
    "buffer_segments":	5,
    "env_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar",
    "epsilon":	1.0,
    "epsilon_decay":	0.0003,
    "epsilon_min":	0.01,
    "exp_name":	"rl4sys-dqn-info",
    "gamma":	0.95,
    "kernel_dim":	4,
    "kernel_size":	2,
    "log_data_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/",
    "logger_kwargs":	{
        "exp_name":	"rl4sys-dqn-info",
        "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s460970000"
    },
    "max_sample_age":	-1,
    "q_lr":	0.0003,
    "sample_decay":	0.8,
    "seed":	460970000,
    "self":	{
        "<algorithms.DQN.DQN.DQN object at 0x72e08bf473d0>":	{
            "_act_dim":	4,
            "_aux_batch_size":	10,
            "_batch_size":	54,
            "_buf_size":	50000,
            "_epsilon":	1.0,
            "_epsilon_decay":	0.0003,
            "_epsilon_min":	0.01,
            "_gamma":	0.95,
            "_kernel_dim":	4,
            "_kernel_size":	2,
            "_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.0003,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0348,  0.0555, -0.1084, -0.0026,  0.2144,  0.1627,  0.0799,  0.2721,\n         0.1422, -0.1869,  0.1868,  0.0077, -0.0450,  0.3084,  0.2678,  0.2523,\n        -0.3181, -0.2508,  0.1962,  0.2829, -0.3353, -0.2440, -0.0288, -0.3224,\n         0.0702,  0.0905, -0.3221,  0.2488, -0.2417,  0.3244,  0.0700,  0.2533,\n         0.1423, -0.3027,  0.3209, -0.3347,  0.0876, -0.0779,  0.3196, -0.3017,\n        -0.2299, -0.1388, -0.1748,  0.0845,  0.0545, -0.0180,  0.1161, -0.2902,\n         0.0896,  0.0770,  0.1368,  0.0650, -0.2903, -0.3380, -0.1596, -0.0313,\n         0.0597, -0.0069, -0.0012,  0.0476, -0.2000, -0.1565,  0.2090,  0.0387,\n         0.0159,  0.0518, -0.0547,  0.1107, -0.0615, -0.1275,  0.2671, -0.2034,\n        -0.2165,  0.2627,  0.2138,  0.2067,  0.1523, -0.0525, -0.0964, -0.3219,\n         0.1862,  0.3038, -0.3398,  0.2126,  0.3036,  0.1017,  0.1599,  0.0197,\n         0.3074, -0.0166, -0.0755, -0.1447,  0.0724, -0.1590,  0.2867,  0.1718,\n        -0.2902,  0.3521, -0.2874, -0.2642,  0.0555, -0.2494,  0.2659, -0.3135,\n         0.0605, -0.0434,  0.2206,  0.0568,  0.2707,  0.1970, -0.1396, -0.0906,\n        -0.2376,  0.0445, -0.0633,  0.1697,  0.0068,  0.1192,  0.1536, -0.0304,\n        -0.2737,  0.1611, -0.2631,  0.2299,  0.2095,  0.1395,  0.1299,  0.3073],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.1298, -0.2794,  0.2412,  ..., -0.1840,  0.2024, -0.0178],\n        [-0.1443,  0.2216, -0.2928,  ..., -0.2902, -0.3501,  0.1246],\n        [-0.3219, -0.2588, -0.0303,  ...,  0.0373, -0.0402,  0.1867],\n        ...,\n        [ 0.0184, -0.3245,  0.2032,  ..., -0.0077, -0.2095, -0.1950],\n        [-0.1022, -0.2183,  0.1166,  ...,  0.3504, -0.2887,  0.1797],\n        [ 0.2470, -0.0034, -0.3531,  ...,  0.2978, -0.2880,  0.2157]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	128,
                                "training":	true
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=128, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0246,  0.0353,  0.0455, -0.0220,  0.0727,  0.0029, -0.0531, -0.0662,\n        -0.0207, -0.0813, -0.0251, -0.0764, -0.0684, -0.0421,  0.0014, -0.0176,\n         0.0385,  0.0324,  0.0051, -0.0762, -0.0140,  0.0459, -0.0190,  0.0881,\n        -0.0038,  0.0070,  0.0400,  0.0826,  0.0141,  0.0709,  0.0059,  0.0044,\n         0.0785, -0.0762,  0.0323, -0.0597, -0.0492, -0.0498, -0.0869, -0.0807,\n         0.0513,  0.0237,  0.0478, -0.0492, -0.0556,  0.0849,  0.0096,  0.0330,\n         0.0337,  0.0855,  0.0653, -0.0671,  0.0423,  0.0784,  0.0708,  0.0508,\n        -0.0537, -0.0215,  0.0137, -0.0006, -0.0030, -0.0216,  0.0553,  0.0153,\n         0.0567,  0.0509,  0.0061,  0.0524, -0.0510, -0.0604, -0.0086, -0.0128,\n         0.0312, -0.0220,  0.0552, -0.0459, -0.0338, -0.0065,  0.0267,  0.0058,\n         0.0430,  0.0549,  0.0068,  0.0660, -0.0170, -0.0441, -0.0721, -0.0834,\n        -0.0170,  0.0179, -0.0555,  0.0533,  0.0040, -0.0036, -0.0551,  0.0149,\n         0.0492,  0.0277,  0.0163, -0.0577,  0.0012, -0.0081,  0.0079, -0.0588,\n        -0.0662, -0.0491, -0.0557, -0.0741,  0.0127, -0.0023, -0.0043, -0.0355,\n         0.0429,  0.0148,  0.0719,  0.0737, -0.0731, -0.0357,  0.0807, -0.0849,\n         0.0633, -0.0398, -0.0264,  0.0047, -0.0487,  0.0099,  0.0615,  0.0592],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0550, -0.0210, -0.0792,  ...,  0.0472,  0.0524, -0.0066],\n        [ 0.0623, -0.0788,  0.0202,  ..., -0.0182, -0.0506, -0.0588],\n        [ 0.0318, -0.0227,  0.0110,  ..., -0.0870,  0.0546,  0.0195],\n        ...,\n        [ 0.0676,  0.0761,  0.0163,  ..., -0.0281,  0.0448,  0.0590],\n        [ 0.0053,  0.0275, -0.0070,  ...,  0.0335, -0.0475, -0.0574],\n        [-0.0291, -0.0617, -0.0284,  ...,  0.0645,  0.0143,  0.0292]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	128,
                                "training":	true
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=128, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0319, -0.0060,  0.0711,  0.0869], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 3.7086e-02,  2.9230e-02, -3.3869e-03, -6.0696e-02, -6.0264e-02,\n         -4.7566e-02, -5.3510e-02, -7.8573e-02, -8.1968e-02, -5.8231e-02,\n          2.7275e-02,  5.6437e-03, -6.8215e-02,  4.3855e-02, -1.0851e-02,\n          4.5957e-02,  3.6069e-02,  3.0301e-02, -7.5686e-02, -3.9931e-02,\n          8.2034e-02,  3.7463e-02,  6.7733e-02, -3.7556e-02,  7.9686e-02,\n         -1.3364e-02, -6.0437e-02,  6.4346e-02, -3.4326e-02,  4.9940e-02,\n         -7.4618e-03, -3.3577e-02,  8.3575e-02, -8.6048e-02, -3.4000e-02,\n          2.9474e-02,  4.5876e-02,  4.1695e-02, -3.0232e-02,  8.3513e-02,\n         -2.8721e-02,  4.5824e-02, -8.2083e-02,  2.2859e-02,  4.7751e-02,\n          3.2738e-02, -2.1791e-02,  1.7991e-02, -3.5512e-02,  5.2397e-02,\n         -3.6117e-02, -2.4460e-02, -4.6126e-02,  2.3230e-03,  4.9517e-02,\n         -4.7738e-02,  5.7815e-02,  1.4023e-02, -4.3518e-02, -6.6399e-02,\n         -5.3477e-02,  3.7320e-02, -2.1750e-02,  7.0216e-02,  1.7459e-02,\n         -2.3523e-02, -3.0554e-02, -1.8893e-02, -8.7316e-02,  4.4707e-03,\n          2.6698e-02,  4.2550e-02, -9.5139e-03,  7.2968e-02, -4.7149e-02,\n         -7.0037e-02, -1.3769e-02, -1.8189e-02,  3.5336e-03,  3.9144e-02,\n          2.1081e-02,  6.3750e-02,  5.8302e-02,  6.4768e-02,  6.7000e-02,\n         -4.9364e-02, -8.6524e-02, -8.0708e-02, -3.3586e-02,  6.5563e-02,\n         -4.0838e-02,  2.1488e-02, -3.1968e-02,  5.2643e-02, -1.9886e-02,\n         -8.2074e-02, -2.2065e-02, -1.9343e-02, -7.0835e-02, -8.4411e-02,\n          2.1921e-02,  1.6479e-02, -2.6969e-02, -2.1299e-02, -2.9222e-02,\n         -2.8902e-03, -3.2295e-02, -3.3529e-02, -1.8004e-02,  6.2840e-02,\n          1.7311e-02,  4.6616e-02, -8.2066e-02, -4.0105e-02,  8.5460e-03,\n          2.2709e-02,  5.5036e-02, -5.8004e-02,  6.6429e-02,  7.9538e-02,\n          5.3897e-02,  2.9659e-02, -7.5677e-02,  4.9145e-02, -8.3235e-02,\n         -3.9052e-02,  1.5659e-02,  4.4401e-02],\n        [ 4.8688e-02, -1.4647e-02, -8.5696e-02, -4.9736e-02,  5.9231e-02,\n         -2.3118e-02,  2.1769e-03,  2.5149e-02,  7.5356e-02, -8.3686e-02,\n         -2.6389e-02,  8.7163e-02, -1.2225e-02,  5.1269e-02,  3.2417e-02,\n          4.5322e-02, -5.6933e-02,  5.6658e-02, -2.0313e-02, -4.8018e-02,\n         -1.1996e-02,  2.4914e-02, -1.1909e-02,  1.9427e-02, -6.4603e-02,\n          3.1005e-02, -2.1271e-02,  5.5577e-02, -1.7539e-02, -6.3222e-02,\n          5.5217e-02,  2.8012e-02,  8.5818e-02, -7.5547e-02, -1.1053e-02,\n         -1.9756e-02,  3.6994e-02, -8.4820e-02, -2.2950e-02, -6.4412e-02,\n         -5.0393e-02,  6.5455e-02,  5.4069e-02,  7.3399e-02, -4.3501e-02,\n         -3.9221e-02,  4.7676e-02, -2.5699e-02, -5.5052e-02,  4.5914e-02,\n          7.4711e-02, -4.2120e-02, -6.2436e-02,  2.2330e-02,  8.0190e-03,\n          3.6046e-05,  3.7945e-03, -9.1051e-03,  7.9370e-02, -8.3595e-02,\n          6.3680e-02, -1.0565e-02,  6.2256e-02, -3.7341e-02, -7.8745e-02,\n         -7.9178e-02,  6.6167e-02, -5.4437e-02,  8.5898e-02, -5.9292e-02,\n         -6.5722e-02,  4.9149e-02,  6.2813e-02,  5.3873e-03,  1.2725e-02,\n          4.6614e-02, -6.3071e-02, -6.3097e-03,  2.8158e-03,  2.5616e-02,\n         -4.5994e-02, -1.0228e-03, -5.3414e-02, -5.2273e-02,  6.7317e-02,\n          6.7531e-02, -2.1622e-02,  1.6782e-03, -4.0772e-02, -6.6939e-02,\n          2.9929e-02, -2.2181e-02, -7.0734e-02,  6.1613e-02, -1.7199e-02,\n          5.2988e-02,  5.4033e-03,  4.8695e-02,  7.2579e-02,  1.2731e-02,\n          7.9972e-02,  6.0669e-02, -3.4733e-02,  6.0544e-02, -7.8871e-02,\n          2.9419e-02, -4.8404e-02, -2.2398e-02, -3.5887e-02, -4.4325e-02,\n          6.8750e-02,  5.1225e-02, -6.4961e-02, -7.7372e-02, -7.0545e-02,\n         -2.9945e-04, -7.0344e-02, -9.9709e-05, -4.5358e-02, -5.8892e-02,\n         -8.6773e-02,  3.9851e-02, -3.4881e-02,  5.5147e-02, -1.1053e-02,\n         -3.3371e-03, -7.1621e-03,  3.7193e-02],\n        [ 5.3318e-02, -2.6584e-02, -4.4561e-02, -6.9717e-02, -1.5365e-02,\n         -2.7984e-02, -4.1497e-02,  6.0551e-02,  6.6942e-02,  5.3744e-03,\n         -4.3177e-02,  6.9258e-02, -7.0126e-02, -8.5181e-02,  2.5267e-02,\n          2.7818e-02, -8.8277e-02,  8.3642e-02, -2.7770e-02,  8.7384e-02,\n         -8.6526e-02,  6.2473e-03, -1.1499e-02, -6.6538e-02,  1.9092e-02,\n          4.5423e-02, -3.6902e-02, -1.3492e-02,  6.9711e-02, -7.4186e-02,\n         -6.5139e-02,  4.7476e-02, -6.8218e-02, -4.3080e-02,  2.6138e-04,\n          4.9716e-02,  2.3866e-02, -6.7858e-02, -7.2789e-02,  8.1180e-02,\n          4.5092e-02,  6.7710e-02,  3.6182e-02,  1.5969e-02, -7.0254e-02,\n         -4.8993e-02,  6.0093e-02, -7.6264e-02,  4.9418e-02, -8.2611e-02,\n          8.4975e-02,  1.4974e-02, -1.8576e-03, -6.6764e-02, -2.4259e-02,\n         -4.2568e-02,  4.6226e-02, -6.1281e-02,  5.4035e-02,  1.1241e-02,\n         -3.8713e-02, -8.0996e-02, -8.7317e-02, -5.5312e-02,  7.6776e-02,\n          6.7655e-02,  3.0673e-03, -7.4670e-02, -8.1714e-02,  1.0687e-02,\n          7.4374e-02,  2.5878e-02,  3.6206e-02, -4.0512e-02, -6.2572e-02,\n         -3.9183e-02,  3.8223e-02,  2.7876e-02, -8.3417e-02, -7.2676e-02,\n          5.2160e-02, -5.3172e-02, -6.0653e-02,  2.9152e-02,  5.8607e-02,\n         -5.7635e-02, -5.1850e-02, -5.4979e-02,  3.6922e-02,  5.3325e-02,\n         -7.6733e-02,  5.3552e-02, -1.8036e-02, -2.4583e-02,  8.3184e-02,\n         -5.2573e-02,  1.3882e-02,  4.1352e-02, -5.1885e-02, -1.3542e-02,\n         -5.0501e-02, -7.2963e-02, -7.8430e-02,  8.1719e-02, -4.8236e-02,\n          3.3140e-02, -4.8195e-02,  3.2729e-02,  7.9327e-02, -7.7134e-02,\n         -2.0500e-02, -5.2469e-02, -6.5944e-02,  3.3915e-02, -8.0399e-02,\n         -4.7126e-02, -4.4541e-02, -2.7553e-02,  6.3054e-02, -5.2794e-02,\n          3.4047e-02,  7.2041e-03,  1.0115e-02, -7.1533e-02, -5.0197e-04,\n         -6.7286e-02,  8.2626e-02,  8.0553e-02],\n        [ 6.1290e-02, -3.7549e-02, -7.6773e-03,  3.0662e-02, -7.6245e-02,\n         -5.6662e-02, -6.8032e-02, -7.5234e-02, -7.0232e-02,  3.5504e-03,\n         -1.2834e-02,  6.2122e-02, -7.1070e-02,  5.9992e-02,  7.8064e-02,\n          7.1821e-02,  8.6251e-02,  6.4521e-02, -7.5684e-02, -4.7109e-02,\n         -2.7094e-02,  1.0444e-02,  7.0052e-02, -2.5015e-02,  1.4689e-02,\n          1.8745e-02,  8.7917e-02,  3.0707e-02,  4.9758e-02, -3.1284e-02,\n         -1.7018e-02, -5.3702e-02,  5.0615e-02,  3.3140e-02,  8.6356e-02,\n         -3.1557e-02, -7.2198e-02,  2.7945e-02, -5.7384e-02, -7.5564e-03,\n          5.9210e-02,  7.8405e-02,  7.7921e-02, -8.0142e-02, -8.6594e-02,\n          3.4858e-02, -7.2842e-02, -3.3881e-02, -4.1709e-02, -6.1203e-02,\n         -6.1687e-02, -4.4415e-02, -2.7831e-02,  6.3281e-02,  1.5470e-02,\n          7.7018e-02,  6.3339e-02,  6.7238e-02, -2.6945e-02, -4.5318e-02,\n         -7.5094e-02, -2.2643e-02,  4.9437e-02, -1.4458e-02, -9.2981e-03,\n         -2.8980e-02, -6.6900e-03,  8.8286e-02,  4.1716e-02,  5.5089e-03,\n          5.0288e-02, -5.0634e-02, -5.8030e-02, -7.0299e-02, -5.2023e-02,\n         -3.5833e-02,  1.2044e-03,  7.1226e-02, -4.3655e-02,  2.1060e-02,\n          5.8484e-02, -7.6207e-03,  1.4908e-02, -6.9778e-02,  6.5884e-02,\n          6.0401e-02, -2.2130e-02,  3.4278e-02,  5.5323e-02, -1.0982e-02,\n          3.4908e-02, -1.4234e-02,  1.3823e-02, -7.5508e-02, -6.4722e-02,\n          8.0938e-02,  4.9553e-02,  5.0343e-02,  3.8233e-02,  1.8255e-02,\n          1.4160e-02, -5.4075e-02,  1.8801e-02,  3.9846e-02,  6.6443e-02,\n          2.6202e-02,  1.0176e-02, -4.8507e-02, -2.9113e-02, -9.1524e-03,\n         -5.5291e-02, -5.9534e-02, -3.9222e-02,  7.3594e-02,  1.3459e-02,\n         -6.0520e-02,  3.8237e-02, -6.0152e-02,  7.5417e-02,  6.5219e-02,\n          4.2088e-02,  7.0727e-02, -9.0430e-03,  6.3470e-02,  3.8514e-02,\n          3.0082e-02, -4.2862e-02,  8.8265e-02]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	4,
                                "training":	true
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	true
                }
            },
            "_q_optimizer":	{
                "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)":	{
                    "_optimizer_load_state_dict_post_hooks":	{},
                    "_optimizer_load_state_dict_pre_hooks":	{},
                    "_optimizer_state_dict_post_hooks":	{},
                    "_optimizer_state_dict_pre_hooks":	{},
                    "_optimizer_step_post_hooks":	{},
                    "_optimizer_step_pre_hooks":	{},
                    "_warned_capturable_if_run_uncaptured":	true,
                    "_zero_grad_profile_name":	"Optimizer.zero_grad#Adam.zero_grad",
                    "defaults":	{
                        "amsgrad":	false,
                        "betas":	[
                            0.9,
                            0.999
                        ],
                        "capturable":	false,
                        "differentiable":	false,
                        "eps":	1e-08,
                        "foreach":	null,
                        "fused":	null,
                        "lr":	0.0003,
                        "maximize":	false,
                        "weight_decay":	0
                    },
                    "param_groups":	[
                        {
                            "amsgrad":	false,
                            "betas":	[
                                0.9,
                                0.999
                            ],
                            "capturable":	false,
                            "differentiable":	false,
                            "eps":	1e-08,
                            "foreach":	null,
                            "fused":	null,
                            "lr":	0.0003,
                            "maximize":	false,
                            "params":	[
                                "Parameter containing:\ntensor([[-0.1298, -0.2794,  0.2412,  ..., -0.1840,  0.2024, -0.0178],\n        [-0.1443,  0.2216, -0.2928,  ..., -0.2902, -0.3501,  0.1246],\n        [-0.3219, -0.2588, -0.0303,  ...,  0.0373, -0.0402,  0.1867],\n        ...,\n        [ 0.0184, -0.3245,  0.2032,  ..., -0.0077, -0.2095, -0.1950],\n        [-0.1022, -0.2183,  0.1166,  ...,  0.3504, -0.2887,  0.1797],\n        [ 0.2470, -0.0034, -0.3531,  ...,  0.2978, -0.2880,  0.2157]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([-0.0348,  0.0555, -0.1084, -0.0026,  0.2144,  0.1627,  0.0799,  0.2721,\n         0.1422, -0.1869,  0.1868,  0.0077, -0.0450,  0.3084,  0.2678,  0.2523,\n        -0.3181, -0.2508,  0.1962,  0.2829, -0.3353, -0.2440, -0.0288, -0.3224,\n         0.0702,  0.0905, -0.3221,  0.2488, -0.2417,  0.3244,  0.0700,  0.2533,\n         0.1423, -0.3027,  0.3209, -0.3347,  0.0876, -0.0779,  0.3196, -0.3017,\n        -0.2299, -0.1388, -0.1748,  0.0845,  0.0545, -0.0180,  0.1161, -0.2902,\n         0.0896,  0.0770,  0.1368,  0.0650, -0.2903, -0.3380, -0.1596, -0.0313,\n         0.0597, -0.0069, -0.0012,  0.0476, -0.2000, -0.1565,  0.2090,  0.0387,\n         0.0159,  0.0518, -0.0547,  0.1107, -0.0615, -0.1275,  0.2671, -0.2034,\n        -0.2165,  0.2627,  0.2138,  0.2067,  0.1523, -0.0525, -0.0964, -0.3219,\n         0.1862,  0.3038, -0.3398,  0.2126,  0.3036,  0.1017,  0.1599,  0.0197,\n         0.3074, -0.0166, -0.0755, -0.1447,  0.0724, -0.1590,  0.2867,  0.1718,\n        -0.2902,  0.3521, -0.2874, -0.2642,  0.0555, -0.2494,  0.2659, -0.3135,\n         0.0605, -0.0434,  0.2206,  0.0568,  0.2707,  0.1970, -0.1396, -0.0906,\n        -0.2376,  0.0445, -0.0633,  0.1697,  0.0068,  0.1192,  0.1536, -0.0304,\n        -0.2737,  0.1611, -0.2631,  0.2299,  0.2095,  0.1395,  0.1299,  0.3073],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[ 0.0550, -0.0210, -0.0792,  ...,  0.0472,  0.0524, -0.0066],\n        [ 0.0623, -0.0788,  0.0202,  ..., -0.0182, -0.0506, -0.0588],\n        [ 0.0318, -0.0227,  0.0110,  ..., -0.0870,  0.0546,  0.0195],\n        ...,\n        [ 0.0676,  0.0761,  0.0163,  ..., -0.0281,  0.0448,  0.0590],\n        [ 0.0053,  0.0275, -0.0070,  ...,  0.0335, -0.0475, -0.0574],\n        [-0.0291, -0.0617, -0.0284,  ...,  0.0645,  0.0143,  0.0292]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([-0.0246,  0.0353,  0.0455, -0.0220,  0.0727,  0.0029, -0.0531, -0.0662,\n        -0.0207, -0.0813, -0.0251, -0.0764, -0.0684, -0.0421,  0.0014, -0.0176,\n         0.0385,  0.0324,  0.0051, -0.0762, -0.0140,  0.0459, -0.0190,  0.0881,\n        -0.0038,  0.0070,  0.0400,  0.0826,  0.0141,  0.0709,  0.0059,  0.0044,\n         0.0785, -0.0762,  0.0323, -0.0597, -0.0492, -0.0498, -0.0869, -0.0807,\n         0.0513,  0.0237,  0.0478, -0.0492, -0.0556,  0.0849,  0.0096,  0.0330,\n         0.0337,  0.0855,  0.0653, -0.0671,  0.0423,  0.0784,  0.0708,  0.0508,\n        -0.0537, -0.0215,  0.0137, -0.0006, -0.0030, -0.0216,  0.0553,  0.0153,\n         0.0567,  0.0509,  0.0061,  0.0524, -0.0510, -0.0604, -0.0086, -0.0128,\n         0.0312, -0.0220,  0.0552, -0.0459, -0.0338, -0.0065,  0.0267,  0.0058,\n         0.0430,  0.0549,  0.0068,  0.0660, -0.0170, -0.0441, -0.0721, -0.0834,\n        -0.0170,  0.0179, -0.0555,  0.0533,  0.0040, -0.0036, -0.0551,  0.0149,\n         0.0492,  0.0277,  0.0163, -0.0577,  0.0012, -0.0081,  0.0079, -0.0588,\n        -0.0662, -0.0491, -0.0557, -0.0741,  0.0127, -0.0023, -0.0043, -0.0355,\n         0.0429,  0.0148,  0.0719,  0.0737, -0.0731, -0.0357,  0.0807, -0.0849,\n         0.0633, -0.0398, -0.0264,  0.0047, -0.0487,  0.0099,  0.0615,  0.0592],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[ 3.7086e-02,  2.9230e-02, -3.3869e-03, -6.0696e-02, -6.0264e-02,\n         -4.7566e-02, -5.3510e-02, -7.8573e-02, -8.1968e-02, -5.8231e-02,\n          2.7275e-02,  5.6437e-03, -6.8215e-02,  4.3855e-02, -1.0851e-02,\n          4.5957e-02,  3.6069e-02,  3.0301e-02, -7.5686e-02, -3.9931e-02,\n          8.2034e-02,  3.7463e-02,  6.7733e-02, -3.7556e-02,  7.9686e-02,\n         -1.3364e-02, -6.0437e-02,  6.4346e-02, -3.4326e-02,  4.9940e-02,\n         -7.4618e-03, -3.3577e-02,  8.3575e-02, -8.6048e-02, -3.4000e-02,\n          2.9474e-02,  4.5876e-02,  4.1695e-02, -3.0232e-02,  8.3513e-02,\n         -2.8721e-02,  4.5824e-02, -8.2083e-02,  2.2859e-02,  4.7751e-02,\n          3.2738e-02, -2.1791e-02,  1.7991e-02, -3.5512e-02,  5.2397e-02,\n         -3.6117e-02, -2.4460e-02, -4.6126e-02,  2.3230e-03,  4.9517e-02,\n         -4.7738e-02,  5.7815e-02,  1.4023e-02, -4.3518e-02, -6.6399e-02,\n         -5.3477e-02,  3.7320e-02, -2.1750e-02,  7.0216e-02,  1.7459e-02,\n         -2.3523e-02, -3.0554e-02, -1.8893e-02, -8.7316e-02,  4.4707e-03,\n          2.6698e-02,  4.2550e-02, -9.5139e-03,  7.2968e-02, -4.7149e-02,\n         -7.0037e-02, -1.3769e-02, -1.8189e-02,  3.5336e-03,  3.9144e-02,\n          2.1081e-02,  6.3750e-02,  5.8302e-02,  6.4768e-02,  6.7000e-02,\n         -4.9364e-02, -8.6524e-02, -8.0708e-02, -3.3586e-02,  6.5563e-02,\n         -4.0838e-02,  2.1488e-02, -3.1968e-02,  5.2643e-02, -1.9886e-02,\n         -8.2074e-02, -2.2065e-02, -1.9343e-02, -7.0835e-02, -8.4411e-02,\n          2.1921e-02,  1.6479e-02, -2.6969e-02, -2.1299e-02, -2.9222e-02,\n         -2.8902e-03, -3.2295e-02, -3.3529e-02, -1.8004e-02,  6.2840e-02,\n          1.7311e-02,  4.6616e-02, -8.2066e-02, -4.0105e-02,  8.5460e-03,\n          2.2709e-02,  5.5036e-02, -5.8004e-02,  6.6429e-02,  7.9538e-02,\n          5.3897e-02,  2.9659e-02, -7.5677e-02,  4.9145e-02, -8.3235e-02,\n         -3.9052e-02,  1.5659e-02,  4.4401e-02],\n        [ 4.8688e-02, -1.4647e-02, -8.5696e-02, -4.9736e-02,  5.9231e-02,\n         -2.3118e-02,  2.1769e-03,  2.5149e-02,  7.5356e-02, -8.3686e-02,\n         -2.6389e-02,  8.7163e-02, -1.2225e-02,  5.1269e-02,  3.2417e-02,\n          4.5322e-02, -5.6933e-02,  5.6658e-02, -2.0313e-02, -4.8018e-02,\n         -1.1996e-02,  2.4914e-02, -1.1909e-02,  1.9427e-02, -6.4603e-02,\n          3.1005e-02, -2.1271e-02,  5.5577e-02, -1.7539e-02, -6.3222e-02,\n          5.5217e-02,  2.8012e-02,  8.5818e-02, -7.5547e-02, -1.1053e-02,\n         -1.9756e-02,  3.6994e-02, -8.4820e-02, -2.2950e-02, -6.4412e-02,\n         -5.0393e-02,  6.5455e-02,  5.4069e-02,  7.3399e-02, -4.3501e-02,\n         -3.9221e-02,  4.7676e-02, -2.5699e-02, -5.5052e-02,  4.5914e-02,\n          7.4711e-02, -4.2120e-02, -6.2436e-02,  2.2330e-02,  8.0190e-03,\n          3.6046e-05,  3.7945e-03, -9.1051e-03,  7.9370e-02, -8.3595e-02,\n          6.3680e-02, -1.0565e-02,  6.2256e-02, -3.7341e-02, -7.8745e-02,\n         -7.9178e-02,  6.6167e-02, -5.4437e-02,  8.5898e-02, -5.9292e-02,\n         -6.5722e-02,  4.9149e-02,  6.2813e-02,  5.3873e-03,  1.2725e-02,\n          4.6614e-02, -6.3071e-02, -6.3097e-03,  2.8158e-03,  2.5616e-02,\n         -4.5994e-02, -1.0228e-03, -5.3414e-02, -5.2273e-02,  6.7317e-02,\n          6.7531e-02, -2.1622e-02,  1.6782e-03, -4.0772e-02, -6.6939e-02,\n          2.9929e-02, -2.2181e-02, -7.0734e-02,  6.1613e-02, -1.7199e-02,\n          5.2988e-02,  5.4033e-03,  4.8695e-02,  7.2579e-02,  1.2731e-02,\n          7.9972e-02,  6.0669e-02, -3.4733e-02,  6.0544e-02, -7.8871e-02,\n          2.9419e-02, -4.8404e-02, -2.2398e-02, -3.5887e-02, -4.4325e-02,\n          6.8750e-02,  5.1225e-02, -6.4961e-02, -7.7372e-02, -7.0545e-02,\n         -2.9945e-04, -7.0344e-02, -9.9709e-05, -4.5358e-02, -5.8892e-02,\n         -8.6773e-02,  3.9851e-02, -3.4881e-02,  5.5147e-02, -1.1053e-02,\n         -3.3371e-03, -7.1621e-03,  3.7193e-02],\n        [ 5.3318e-02, -2.6584e-02, -4.4561e-02, -6.9717e-02, -1.5365e-02,\n         -2.7984e-02, -4.1497e-02,  6.0551e-02,  6.6942e-02,  5.3744e-03,\n         -4.3177e-02,  6.9258e-02, -7.0126e-02, -8.5181e-02,  2.5267e-02,\n          2.7818e-02, -8.8277e-02,  8.3642e-02, -2.7770e-02,  8.7384e-02,\n         -8.6526e-02,  6.2473e-03, -1.1499e-02, -6.6538e-02,  1.9092e-02,\n          4.5423e-02, -3.6902e-02, -1.3492e-02,  6.9711e-02, -7.4186e-02,\n         -6.5139e-02,  4.7476e-02, -6.8218e-02, -4.3080e-02,  2.6138e-04,\n          4.9716e-02,  2.3866e-02, -6.7858e-02, -7.2789e-02,  8.1180e-02,\n          4.5092e-02,  6.7710e-02,  3.6182e-02,  1.5969e-02, -7.0254e-02,\n         -4.8993e-02,  6.0093e-02, -7.6264e-02,  4.9418e-02, -8.2611e-02,\n          8.4975e-02,  1.4974e-02, -1.8576e-03, -6.6764e-02, -2.4259e-02,\n         -4.2568e-02,  4.6226e-02, -6.1281e-02,  5.4035e-02,  1.1241e-02,\n         -3.8713e-02, -8.0996e-02, -8.7317e-02, -5.5312e-02,  7.6776e-02,\n          6.7655e-02,  3.0673e-03, -7.4670e-02, -8.1714e-02,  1.0687e-02,\n          7.4374e-02,  2.5878e-02,  3.6206e-02, -4.0512e-02, -6.2572e-02,\n         -3.9183e-02,  3.8223e-02,  2.7876e-02, -8.3417e-02, -7.2676e-02,\n          5.2160e-02, -5.3172e-02, -6.0653e-02,  2.9152e-02,  5.8607e-02,\n         -5.7635e-02, -5.1850e-02, -5.4979e-02,  3.6922e-02,  5.3325e-02,\n         -7.6733e-02,  5.3552e-02, -1.8036e-02, -2.4583e-02,  8.3184e-02,\n         -5.2573e-02,  1.3882e-02,  4.1352e-02, -5.1885e-02, -1.3542e-02,\n         -5.0501e-02, -7.2963e-02, -7.8430e-02,  8.1719e-02, -4.8236e-02,\n          3.3140e-02, -4.8195e-02,  3.2729e-02,  7.9327e-02, -7.7134e-02,\n         -2.0500e-02, -5.2469e-02, -6.5944e-02,  3.3915e-02, -8.0399e-02,\n         -4.7126e-02, -4.4541e-02, -2.7553e-02,  6.3054e-02, -5.2794e-02,\n          3.4047e-02,  7.2041e-03,  1.0115e-02, -7.1533e-02, -5.0197e-04,\n         -6.7286e-02,  8.2626e-02,  8.0553e-02],\n        [ 6.1290e-02, -3.7549e-02, -7.6773e-03,  3.0662e-02, -7.6245e-02,\n         -5.6662e-02, -6.8032e-02, -7.5234e-02, -7.0232e-02,  3.5504e-03,\n         -1.2834e-02,  6.2122e-02, -7.1070e-02,  5.9992e-02,  7.8064e-02,\n          7.1821e-02,  8.6251e-02,  6.4521e-02, -7.5684e-02, -4.7109e-02,\n         -2.7094e-02,  1.0444e-02,  7.0052e-02, -2.5015e-02,  1.4689e-02,\n          1.8745e-02,  8.7917e-02,  3.0707e-02,  4.9758e-02, -3.1284e-02,\n         -1.7018e-02, -5.3702e-02,  5.0615e-02,  3.3140e-02,  8.6356e-02,\n         -3.1557e-02, -7.2198e-02,  2.7945e-02, -5.7384e-02, -7.5564e-03,\n          5.9210e-02,  7.8405e-02,  7.7921e-02, -8.0142e-02, -8.6594e-02,\n          3.4858e-02, -7.2842e-02, -3.3881e-02, -4.1709e-02, -6.1203e-02,\n         -6.1687e-02, -4.4415e-02, -2.7831e-02,  6.3281e-02,  1.5470e-02,\n          7.7018e-02,  6.3339e-02,  6.7238e-02, -2.6945e-02, -4.5318e-02,\n         -7.5094e-02, -2.2643e-02,  4.9437e-02, -1.4458e-02, -9.2981e-03,\n         -2.8980e-02, -6.6900e-03,  8.8286e-02,  4.1716e-02,  5.5089e-03,\n          5.0288e-02, -5.0634e-02, -5.8030e-02, -7.0299e-02, -5.2023e-02,\n         -3.5833e-02,  1.2044e-03,  7.1226e-02, -4.3655e-02,  2.1060e-02,\n          5.8484e-02, -7.6207e-03,  1.4908e-02, -6.9778e-02,  6.5884e-02,\n          6.0401e-02, -2.2130e-02,  3.4278e-02,  5.5323e-02, -1.0982e-02,\n          3.4908e-02, -1.4234e-02,  1.3823e-02, -7.5508e-02, -6.4722e-02,\n          8.0938e-02,  4.9553e-02,  5.0343e-02,  3.8233e-02,  1.8255e-02,\n          1.4160e-02, -5.4075e-02,  1.8801e-02,  3.9846e-02,  6.6443e-02,\n          2.6202e-02,  1.0176e-02, -4.8507e-02, -2.9113e-02, -9.1524e-03,\n         -5.5291e-02, -5.9534e-02, -3.9222e-02,  7.3594e-02,  1.3459e-02,\n         -6.0520e-02,  3.8237e-02, -6.0152e-02,  7.5417e-02,  6.5219e-02,\n          4.2088e-02,  7.0727e-02, -9.0430e-03,  6.3470e-02,  3.8514e-02,\n          3.0082e-02, -4.2862e-02,  8.8265e-02]], requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.0319, -0.0060,  0.0711,  0.0869], requires_grad=True)"
                            ],
                            "weight_decay":	0
                        }
                    ],
                    "state":	{}
                }
            },
            "_replay_buffer":	{
                "<algorithms.DQN.replay_buffer.EnhancedReplayBuffer object at 0x72e08c5ae790>":	{
                    "act_buf":	"[0 0 0 ... 0 0 0]",
                    "alpha":	0.6,
                    "aux_buf_size":	1000,
                    "beta":	0.4,
                    "beta_increment_per_sampling":	0.0001,
                    "buf_size":	50000,
                    "current_segment":	0,
                    "done_buf":	"[False False False ... False False False]",
                    "epsilon_per_priority":	1e-06,
                    "markers":	"[False False False ... False False False]",
                    "mask_buf":	"[[0. 0.]\n [0. 0.]\n [0. 0.]\n ...\n [0. 0.]\n [0. 0.]\n [0. 0.]]",
                    "mask_dim":	2,
                    "max_age":	50000,
                    "next_obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "num_segments":	5,
                    "obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "obs_dim":	8,
                    "path_start_idx":	0,
                    "ptr":	0,
                    "rew_buf":	"[0. 0. 0. ... 0. 0. 0.]",
                    "sample_decay":	0.8,
                    "segment_ptr":	0,
                    "segment_size":	10000,
                    "step":	0,
                    "sum_tree":	{
                        "<algorithms.DQN.replay_buffer.SumTree object at 0x72e08bc70f50>":	{
                            "capacity":	1000,
                            "data":	"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0]",
                            "n_entries":	0,
                            "tree":	"[0. 0. 0. ... 0. 0. 0.]",
                            "write":	0
                        }
                    },
                    "timestamps":	"[0 0 0 ... 0 0 0]"
                }
            },
            "_target_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.0003,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0348,  0.0555, -0.1084, -0.0026,  0.2144,  0.1627,  0.0799,  0.2721,\n         0.1422, -0.1869,  0.1868,  0.0077, -0.0450,  0.3084,  0.2678,  0.2523,\n        -0.3181, -0.2508,  0.1962,  0.2829, -0.3353, -0.2440, -0.0288, -0.3224,\n         0.0702,  0.0905, -0.3221,  0.2488, -0.2417,  0.3244,  0.0700,  0.2533,\n         0.1423, -0.3027,  0.3209, -0.3347,  0.0876, -0.0779,  0.3196, -0.3017,\n        -0.2299, -0.1388, -0.1748,  0.0845,  0.0545, -0.0180,  0.1161, -0.2902,\n         0.0896,  0.0770,  0.1368,  0.0650, -0.2903, -0.3380, -0.1596, -0.0313,\n         0.0597, -0.0069, -0.0012,  0.0476, -0.2000, -0.1565,  0.2090,  0.0387,\n         0.0159,  0.0518, -0.0547,  0.1107, -0.0615, -0.1275,  0.2671, -0.2034,\n        -0.2165,  0.2627,  0.2138,  0.2067,  0.1523, -0.0525, -0.0964, -0.3219,\n         0.1862,  0.3038, -0.3398,  0.2126,  0.3036,  0.1017,  0.1599,  0.0197,\n         0.3074, -0.0166, -0.0755, -0.1447,  0.0724, -0.1590,  0.2867,  0.1718,\n        -0.2902,  0.3521, -0.2874, -0.2642,  0.0555, -0.2494,  0.2659, -0.3135,\n         0.0605, -0.0434,  0.2206,  0.0568,  0.2707,  0.1970, -0.1396, -0.0906,\n        -0.2376,  0.0445, -0.0633,  0.1697,  0.0068,  0.1192,  0.1536, -0.0304,\n        -0.2737,  0.1611, -0.2631,  0.2299,  0.2095,  0.1395,  0.1299,  0.3073],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.1298, -0.2794,  0.2412,  ..., -0.1840,  0.2024, -0.0178],\n        [-0.1443,  0.2216, -0.2928,  ..., -0.2902, -0.3501,  0.1246],\n        [-0.3219, -0.2588, -0.0303,  ...,  0.0373, -0.0402,  0.1867],\n        ...,\n        [ 0.0184, -0.3245,  0.2032,  ..., -0.0077, -0.2095, -0.1950],\n        [-0.1022, -0.2183,  0.1166,  ...,  0.3504, -0.2887,  0.1797],\n        [ 0.2470, -0.0034, -0.3531,  ...,  0.2978, -0.2880,  0.2157]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	128,
                                "training":	false
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=128, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0246,  0.0353,  0.0455, -0.0220,  0.0727,  0.0029, -0.0531, -0.0662,\n        -0.0207, -0.0813, -0.0251, -0.0764, -0.0684, -0.0421,  0.0014, -0.0176,\n         0.0385,  0.0324,  0.0051, -0.0762, -0.0140,  0.0459, -0.0190,  0.0881,\n        -0.0038,  0.0070,  0.0400,  0.0826,  0.0141,  0.0709,  0.0059,  0.0044,\n         0.0785, -0.0762,  0.0323, -0.0597, -0.0492, -0.0498, -0.0869, -0.0807,\n         0.0513,  0.0237,  0.0478, -0.0492, -0.0556,  0.0849,  0.0096,  0.0330,\n         0.0337,  0.0855,  0.0653, -0.0671,  0.0423,  0.0784,  0.0708,  0.0508,\n        -0.0537, -0.0215,  0.0137, -0.0006, -0.0030, -0.0216,  0.0553,  0.0153,\n         0.0567,  0.0509,  0.0061,  0.0524, -0.0510, -0.0604, -0.0086, -0.0128,\n         0.0312, -0.0220,  0.0552, -0.0459, -0.0338, -0.0065,  0.0267,  0.0058,\n         0.0430,  0.0549,  0.0068,  0.0660, -0.0170, -0.0441, -0.0721, -0.0834,\n        -0.0170,  0.0179, -0.0555,  0.0533,  0.0040, -0.0036, -0.0551,  0.0149,\n         0.0492,  0.0277,  0.0163, -0.0577,  0.0012, -0.0081,  0.0079, -0.0588,\n        -0.0662, -0.0491, -0.0557, -0.0741,  0.0127, -0.0023, -0.0043, -0.0355,\n         0.0429,  0.0148,  0.0719,  0.0737, -0.0731, -0.0357,  0.0807, -0.0849,\n         0.0633, -0.0398, -0.0264,  0.0047, -0.0487,  0.0099,  0.0615,  0.0592],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0550, -0.0210, -0.0792,  ...,  0.0472,  0.0524, -0.0066],\n        [ 0.0623, -0.0788,  0.0202,  ..., -0.0182, -0.0506, -0.0588],\n        [ 0.0318, -0.0227,  0.0110,  ..., -0.0870,  0.0546,  0.0195],\n        ...,\n        [ 0.0676,  0.0761,  0.0163,  ..., -0.0281,  0.0448,  0.0590],\n        [ 0.0053,  0.0275, -0.0070,  ...,  0.0335, -0.0475, -0.0574],\n        [-0.0291, -0.0617, -0.0284,  ...,  0.0645,  0.0143,  0.0292]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	128,
                                "training":	false
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=128, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0319, -0.0060,  0.0711,  0.0869], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 3.7086e-02,  2.9230e-02, -3.3869e-03, -6.0696e-02, -6.0264e-02,\n         -4.7566e-02, -5.3510e-02, -7.8573e-02, -8.1968e-02, -5.8231e-02,\n          2.7275e-02,  5.6437e-03, -6.8215e-02,  4.3855e-02, -1.0851e-02,\n          4.5957e-02,  3.6069e-02,  3.0301e-02, -7.5686e-02, -3.9931e-02,\n          8.2034e-02,  3.7463e-02,  6.7733e-02, -3.7556e-02,  7.9686e-02,\n         -1.3364e-02, -6.0437e-02,  6.4346e-02, -3.4326e-02,  4.9940e-02,\n         -7.4618e-03, -3.3577e-02,  8.3575e-02, -8.6048e-02, -3.4000e-02,\n          2.9474e-02,  4.5876e-02,  4.1695e-02, -3.0232e-02,  8.3513e-02,\n         -2.8721e-02,  4.5824e-02, -8.2083e-02,  2.2859e-02,  4.7751e-02,\n          3.2738e-02, -2.1791e-02,  1.7991e-02, -3.5512e-02,  5.2397e-02,\n         -3.6117e-02, -2.4460e-02, -4.6126e-02,  2.3230e-03,  4.9517e-02,\n         -4.7738e-02,  5.7815e-02,  1.4023e-02, -4.3518e-02, -6.6399e-02,\n         -5.3477e-02,  3.7320e-02, -2.1750e-02,  7.0216e-02,  1.7459e-02,\n         -2.3523e-02, -3.0554e-02, -1.8893e-02, -8.7316e-02,  4.4707e-03,\n          2.6698e-02,  4.2550e-02, -9.5139e-03,  7.2968e-02, -4.7149e-02,\n         -7.0037e-02, -1.3769e-02, -1.8189e-02,  3.5336e-03,  3.9144e-02,\n          2.1081e-02,  6.3750e-02,  5.8302e-02,  6.4768e-02,  6.7000e-02,\n         -4.9364e-02, -8.6524e-02, -8.0708e-02, -3.3586e-02,  6.5563e-02,\n         -4.0838e-02,  2.1488e-02, -3.1968e-02,  5.2643e-02, -1.9886e-02,\n         -8.2074e-02, -2.2065e-02, -1.9343e-02, -7.0835e-02, -8.4411e-02,\n          2.1921e-02,  1.6479e-02, -2.6969e-02, -2.1299e-02, -2.9222e-02,\n         -2.8902e-03, -3.2295e-02, -3.3529e-02, -1.8004e-02,  6.2840e-02,\n          1.7311e-02,  4.6616e-02, -8.2066e-02, -4.0105e-02,  8.5460e-03,\n          2.2709e-02,  5.5036e-02, -5.8004e-02,  6.6429e-02,  7.9538e-02,\n          5.3897e-02,  2.9659e-02, -7.5677e-02,  4.9145e-02, -8.3235e-02,\n         -3.9052e-02,  1.5659e-02,  4.4401e-02],\n        [ 4.8688e-02, -1.4647e-02, -8.5696e-02, -4.9736e-02,  5.9231e-02,\n         -2.3118e-02,  2.1769e-03,  2.5149e-02,  7.5356e-02, -8.3686e-02,\n         -2.6389e-02,  8.7163e-02, -1.2225e-02,  5.1269e-02,  3.2417e-02,\n          4.5322e-02, -5.6933e-02,  5.6658e-02, -2.0313e-02, -4.8018e-02,\n         -1.1996e-02,  2.4914e-02, -1.1909e-02,  1.9427e-02, -6.4603e-02,\n          3.1005e-02, -2.1271e-02,  5.5577e-02, -1.7539e-02, -6.3222e-02,\n          5.5217e-02,  2.8012e-02,  8.5818e-02, -7.5547e-02, -1.1053e-02,\n         -1.9756e-02,  3.6994e-02, -8.4820e-02, -2.2950e-02, -6.4412e-02,\n         -5.0393e-02,  6.5455e-02,  5.4069e-02,  7.3399e-02, -4.3501e-02,\n         -3.9221e-02,  4.7676e-02, -2.5699e-02, -5.5052e-02,  4.5914e-02,\n          7.4711e-02, -4.2120e-02, -6.2436e-02,  2.2330e-02,  8.0190e-03,\n          3.6046e-05,  3.7945e-03, -9.1051e-03,  7.9370e-02, -8.3595e-02,\n          6.3680e-02, -1.0565e-02,  6.2256e-02, -3.7341e-02, -7.8745e-02,\n         -7.9178e-02,  6.6167e-02, -5.4437e-02,  8.5898e-02, -5.9292e-02,\n         -6.5722e-02,  4.9149e-02,  6.2813e-02,  5.3873e-03,  1.2725e-02,\n          4.6614e-02, -6.3071e-02, -6.3097e-03,  2.8158e-03,  2.5616e-02,\n         -4.5994e-02, -1.0228e-03, -5.3414e-02, -5.2273e-02,  6.7317e-02,\n          6.7531e-02, -2.1622e-02,  1.6782e-03, -4.0772e-02, -6.6939e-02,\n          2.9929e-02, -2.2181e-02, -7.0734e-02,  6.1613e-02, -1.7199e-02,\n          5.2988e-02,  5.4033e-03,  4.8695e-02,  7.2579e-02,  1.2731e-02,\n          7.9972e-02,  6.0669e-02, -3.4733e-02,  6.0544e-02, -7.8871e-02,\n          2.9419e-02, -4.8404e-02, -2.2398e-02, -3.5887e-02, -4.4325e-02,\n          6.8750e-02,  5.1225e-02, -6.4961e-02, -7.7372e-02, -7.0545e-02,\n         -2.9945e-04, -7.0344e-02, -9.9709e-05, -4.5358e-02, -5.8892e-02,\n         -8.6773e-02,  3.9851e-02, -3.4881e-02,  5.5147e-02, -1.1053e-02,\n         -3.3371e-03, -7.1621e-03,  3.7193e-02],\n        [ 5.3318e-02, -2.6584e-02, -4.4561e-02, -6.9717e-02, -1.5365e-02,\n         -2.7984e-02, -4.1497e-02,  6.0551e-02,  6.6942e-02,  5.3744e-03,\n         -4.3177e-02,  6.9258e-02, -7.0126e-02, -8.5181e-02,  2.5267e-02,\n          2.7818e-02, -8.8277e-02,  8.3642e-02, -2.7770e-02,  8.7384e-02,\n         -8.6526e-02,  6.2473e-03, -1.1499e-02, -6.6538e-02,  1.9092e-02,\n          4.5423e-02, -3.6902e-02, -1.3492e-02,  6.9711e-02, -7.4186e-02,\n         -6.5139e-02,  4.7476e-02, -6.8218e-02, -4.3080e-02,  2.6138e-04,\n          4.9716e-02,  2.3866e-02, -6.7858e-02, -7.2789e-02,  8.1180e-02,\n          4.5092e-02,  6.7710e-02,  3.6182e-02,  1.5969e-02, -7.0254e-02,\n         -4.8993e-02,  6.0093e-02, -7.6264e-02,  4.9418e-02, -8.2611e-02,\n          8.4975e-02,  1.4974e-02, -1.8576e-03, -6.6764e-02, -2.4259e-02,\n         -4.2568e-02,  4.6226e-02, -6.1281e-02,  5.4035e-02,  1.1241e-02,\n         -3.8713e-02, -8.0996e-02, -8.7317e-02, -5.5312e-02,  7.6776e-02,\n          6.7655e-02,  3.0673e-03, -7.4670e-02, -8.1714e-02,  1.0687e-02,\n          7.4374e-02,  2.5878e-02,  3.6206e-02, -4.0512e-02, -6.2572e-02,\n         -3.9183e-02,  3.8223e-02,  2.7876e-02, -8.3417e-02, -7.2676e-02,\n          5.2160e-02, -5.3172e-02, -6.0653e-02,  2.9152e-02,  5.8607e-02,\n         -5.7635e-02, -5.1850e-02, -5.4979e-02,  3.6922e-02,  5.3325e-02,\n         -7.6733e-02,  5.3552e-02, -1.8036e-02, -2.4583e-02,  8.3184e-02,\n         -5.2573e-02,  1.3882e-02,  4.1352e-02, -5.1885e-02, -1.3542e-02,\n         -5.0501e-02, -7.2963e-02, -7.8430e-02,  8.1719e-02, -4.8236e-02,\n          3.3140e-02, -4.8195e-02,  3.2729e-02,  7.9327e-02, -7.7134e-02,\n         -2.0500e-02, -5.2469e-02, -6.5944e-02,  3.3915e-02, -8.0399e-02,\n         -4.7126e-02, -4.4541e-02, -2.7553e-02,  6.3054e-02, -5.2794e-02,\n          3.4047e-02,  7.2041e-03,  1.0115e-02, -7.1533e-02, -5.0197e-04,\n         -6.7286e-02,  8.2626e-02,  8.0553e-02],\n        [ 6.1290e-02, -3.7549e-02, -7.6773e-03,  3.0662e-02, -7.6245e-02,\n         -5.6662e-02, -6.8032e-02, -7.5234e-02, -7.0232e-02,  3.5504e-03,\n         -1.2834e-02,  6.2122e-02, -7.1070e-02,  5.9992e-02,  7.8064e-02,\n          7.1821e-02,  8.6251e-02,  6.4521e-02, -7.5684e-02, -4.7109e-02,\n         -2.7094e-02,  1.0444e-02,  7.0052e-02, -2.5015e-02,  1.4689e-02,\n          1.8745e-02,  8.7917e-02,  3.0707e-02,  4.9758e-02, -3.1284e-02,\n         -1.7018e-02, -5.3702e-02,  5.0615e-02,  3.3140e-02,  8.6356e-02,\n         -3.1557e-02, -7.2198e-02,  2.7945e-02, -5.7384e-02, -7.5564e-03,\n          5.9210e-02,  7.8405e-02,  7.7921e-02, -8.0142e-02, -8.6594e-02,\n          3.4858e-02, -7.2842e-02, -3.3881e-02, -4.1709e-02, -6.1203e-02,\n         -6.1687e-02, -4.4415e-02, -2.7831e-02,  6.3281e-02,  1.5470e-02,\n          7.7018e-02,  6.3339e-02,  6.7238e-02, -2.6945e-02, -4.5318e-02,\n         -7.5094e-02, -2.2643e-02,  4.9437e-02, -1.4458e-02, -9.2981e-03,\n         -2.8980e-02, -6.6900e-03,  8.8286e-02,  4.1716e-02,  5.5089e-03,\n          5.0288e-02, -5.0634e-02, -5.8030e-02, -7.0299e-02, -5.2023e-02,\n         -3.5833e-02,  1.2044e-03,  7.1226e-02, -4.3655e-02,  2.1060e-02,\n          5.8484e-02, -7.6207e-03,  1.4908e-02, -6.9778e-02,  6.5884e-02,\n          6.0401e-02, -2.2130e-02,  3.4278e-02,  5.5323e-02, -1.0982e-02,\n          3.4908e-02, -1.4234e-02,  1.3823e-02, -7.5508e-02, -6.4722e-02,\n          8.0938e-02,  4.9553e-02,  5.0343e-02,  3.8233e-02,  1.8255e-02,\n          1.4160e-02, -5.4075e-02,  1.8801e-02,  3.9846e-02,  6.6443e-02,\n          2.6202e-02,  1.0176e-02, -4.8507e-02, -2.9113e-02, -9.1524e-03,\n         -5.5291e-02, -5.9534e-02, -3.9222e-02,  7.3594e-02,  1.3459e-02,\n         -6.0520e-02,  3.8237e-02, -6.0152e-02,  7.5417e-02,  6.5219e-02,\n          4.2088e-02,  7.0727e-02, -9.0430e-03,  6.3470e-02,  3.8514e-02,\n          3.0082e-02, -4.2862e-02,  8.8265e-02]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	4,
                                "training":	false
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	false
                }
            },
            "_target_net_update_freq":	100,
            "_train_q_iters":	80,
            "_train_update_freq":	3,
            "_traj_per_epoch":	6,
            "logger":	{
                "<utils.logger.EpochLogger object at 0x72e089e02a90>":	{
                    "epoch_dict":	{},
                    "exp_name":	"rl4sys-dqn-info",
                    "first_row":	true,
                    "log_current_row":	{},
                    "log_headers":	[],
                    "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s460970000",
                    "output_file":	{
                        "<_io.TextIOWrapper name='/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s460970000/progress.txt' mode='w' encoding='UTF-8'>":	{
                            "mode":	"w"
                        }
                    }
                }
            }
        }
    },
    "target_net_update_freq":	100,
    "train_q_iters":	80,
    "train_update_freq":	3,
    "traj_per_epoch":	6
}