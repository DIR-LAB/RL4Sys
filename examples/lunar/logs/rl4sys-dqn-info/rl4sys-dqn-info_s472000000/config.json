{
    "__class__":	"DQN",
    "act_dim":	4,
    "aux_batch_size":	0,
    "batch_size":	64,
    "buf_size":	50000,
    "buffer_segments":	5,
    "env_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar",
    "epsilon":	1.0,
    "epsilon_decay":	0.0005,
    "epsilon_min":	0.01,
    "exp_name":	"rl4sys-dqn-info",
    "gamma":	0.99,
    "kernel_dim":	4,
    "kernel_size":	2,
    "log_data_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/",
    "logger_kwargs":	{
        "exp_name":	"rl4sys-dqn-info",
        "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s472000000"
    },
    "max_sample_age":	-1,
    "q_lr":	0.0003,
    "sample_decay":	0.8,
    "seed":	472000000,
    "self":	{
        "<algorithms.DQN.DQN.DQN object at 0x79d11b818590>":	{
            "_act_dim":	4,
            "_aux_batch_size":	0,
            "_batch_size":	64,
            "_buf_size":	50000,
            "_epsilon":	1.0,
            "_epsilon_decay":	0.0005,
            "_epsilon_min":	0.01,
            "_gamma":	0.99,
            "_kernel_dim":	4,
            "_kernel_size":	2,
            "_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.0005,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.1678,  0.0339, -0.1494, -0.1933,  0.1627, -0.2005,  0.3499,  0.2298,\n         0.0818,  0.0954,  0.2433, -0.0436,  0.0085,  0.1860,  0.2392, -0.2033,\n         0.1594,  0.2888,  0.3194,  0.1396, -0.0832, -0.1263,  0.0787,  0.1147,\n         0.2736,  0.2191, -0.0664,  0.2073,  0.0828,  0.2463, -0.3356,  0.2003,\n        -0.0293, -0.3049, -0.1340,  0.3302,  0.2521, -0.1522,  0.1099, -0.0875,\n         0.0882, -0.3185,  0.0769,  0.0094, -0.2892, -0.0686,  0.3485,  0.1761,\n        -0.3448, -0.2173, -0.3189,  0.0119, -0.2709,  0.0366, -0.0584,  0.3131,\n         0.1165, -0.0283, -0.0047, -0.1771,  0.2787,  0.3305,  0.1370,  0.0083,\n        -0.1749, -0.0580,  0.1338,  0.1171,  0.0935,  0.1530,  0.1955, -0.1035,\n        -0.2267, -0.3363,  0.2530,  0.2108,  0.2262,  0.1420, -0.1898,  0.0907,\n        -0.2759,  0.3143,  0.2204, -0.3089,  0.0537, -0.0231, -0.1438, -0.0912,\n         0.1675,  0.3414, -0.2462, -0.2372,  0.1423,  0.1533, -0.0231,  0.0470,\n        -0.3513, -0.1101, -0.1333,  0.3347, -0.0681,  0.3488, -0.2156, -0.1209,\n         0.2608,  0.0196,  0.0506,  0.1365,  0.3197,  0.1051, -0.0250,  0.3184,\n         0.3234, -0.0153,  0.0557, -0.1178, -0.1096, -0.2347,  0.3335,  0.1497,\n        -0.3373,  0.2361,  0.1639,  0.3065, -0.2938, -0.2496,  0.0726,  0.2836],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.1221,  0.3156, -0.3061,  ..., -0.2856,  0.0323, -0.0214],\n        [-0.3207,  0.1525,  0.0823,  ...,  0.3214, -0.2110, -0.1757],\n        [-0.0298,  0.1829,  0.1372,  ...,  0.2828, -0.2539, -0.1425],\n        ...,\n        [ 0.2474,  0.2701,  0.1650,  ...,  0.2265,  0.0092,  0.1705],\n        [ 0.0940, -0.0642,  0.3076,  ...,  0.0769, -0.1131, -0.1607],\n        [-0.2862,  0.2340,  0.3354,  ..., -0.2703,  0.2698, -0.2547]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	128,
                                "training":	true
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=128, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0377,  0.0613, -0.0257,  0.0171,  0.0326, -0.0346,  0.0342, -0.0249,\n        -0.0778, -0.0238,  0.0624, -0.0503, -0.0430, -0.0252, -0.0421, -0.0415,\n        -0.0554,  0.0409, -0.0062, -0.0351, -0.0587, -0.0446,  0.0012, -0.0647,\n        -0.0330, -0.0063,  0.0577, -0.0413, -0.0500, -0.0821, -0.0790, -0.0009,\n         0.0279, -0.0617, -0.0155, -0.0754, -0.0020, -0.0055,  0.0800,  0.0202,\n         0.0724,  0.0727, -0.0168,  0.0569, -0.0370, -0.0099,  0.0856, -0.0708,\n         0.0563, -0.0047, -0.0843,  0.0138, -0.0258, -0.0870,  0.0038,  0.0789,\n         0.0334, -0.0576,  0.0070,  0.0322, -0.0844, -0.0022, -0.0498, -0.0135,\n         0.0272,  0.0362, -0.0341, -0.0213, -0.0193,  0.0256,  0.0528, -0.0225,\n        -0.0718,  0.0033, -0.0512,  0.0515,  0.0728, -0.0620,  0.0309, -0.0791,\n         0.0099,  0.0020, -0.0078, -0.0366, -0.0279,  0.0406,  0.0184, -0.0643,\n         0.0746, -0.0243, -0.0876, -0.0142,  0.0786, -0.0100,  0.0721, -0.0076,\n        -0.0431, -0.0238, -0.0372,  0.0315,  0.0348,  0.0292, -0.0444,  0.0520,\n         0.0655, -0.0629,  0.0468, -0.0225,  0.0785, -0.0174,  0.0274, -0.0623,\n         0.0832, -0.0295, -0.0474,  0.0652,  0.0161, -0.0464, -0.0728,  0.0440,\n         0.0111,  0.0714, -0.0758,  0.0307,  0.0871,  0.0081,  0.0226,  0.0756],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.0589, -0.0427, -0.0547,  ...,  0.0404,  0.0523, -0.0588],\n        [-0.0869,  0.0525, -0.0381,  ...,  0.0101, -0.0299, -0.0324],\n        [-0.0713, -0.0430,  0.0656,  ...,  0.0340,  0.0781,  0.0281],\n        ...,\n        [ 0.0337, -0.0195,  0.0318,  ..., -0.0103, -0.0556, -0.0062],\n        [-0.0807,  0.0196,  0.0089,  ...,  0.0622, -0.0505, -0.0418],\n        [-0.0799, -0.0817, -0.0868,  ..., -0.0732,  0.0743, -0.0649]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	128,
                                "training":	true
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=128, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0721, -0.0589, -0.0815,  0.0194], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 3.0760e-02, -6.3319e-03,  2.6579e-02, -3.8700e-03,  4.7726e-03,\n         -2.2383e-03, -8.7095e-02, -7.4575e-03, -2.8319e-02,  5.2629e-02,\n         -2.9483e-03,  1.7023e-02,  8.0125e-02, -4.6618e-02,  7.8126e-02,\n         -5.6478e-02,  8.3151e-02, -5.9233e-02, -5.6001e-03,  1.3185e-02,\n         -6.5981e-02, -4.9092e-02, -8.0500e-05, -3.0654e-02,  6.5160e-02,\n         -6.2362e-02, -8.6130e-02,  1.4424e-02, -7.8967e-02, -2.3674e-02,\n          7.8869e-02,  1.8081e-02, -3.9356e-02,  1.2873e-02,  1.5156e-02,\n         -8.1369e-02,  1.2578e-02,  7.5945e-02, -8.4374e-03,  5.8782e-02,\n          4.1959e-02, -3.3981e-02, -8.5847e-02, -3.2441e-03, -4.4975e-03,\n         -1.6353e-02, -6.4326e-02, -4.1447e-02,  8.5039e-02,  7.5716e-02,\n         -4.8713e-02,  3.9323e-02, -2.9118e-02,  1.5190e-02, -6.9061e-02,\n          1.5074e-02, -3.1328e-02,  8.6361e-02, -6.6093e-02,  2.8899e-02,\n          2.4180e-02, -4.0614e-02, -1.0652e-02,  7.4347e-02,  8.7235e-02,\n         -8.8355e-02, -3.6629e-02, -5.9197e-02,  8.5272e-02, -3.7717e-02,\n          3.8911e-02, -5.2075e-02,  3.3264e-02,  6.8168e-02,  2.1410e-02,\n          1.2440e-02,  7.5054e-02, -6.1270e-02, -8.7264e-02, -9.8237e-03,\n         -6.2586e-02,  1.2184e-02, -4.6092e-02, -4.1382e-02,  3.5495e-02,\n          3.4159e-02, -8.5484e-02,  5.1122e-03, -9.9550e-03, -6.6177e-02,\n          5.8795e-03, -3.0552e-02, -4.7317e-02,  2.9759e-02, -3.3711e-02,\n         -8.7280e-02,  3.6407e-02,  8.2903e-02, -3.0342e-02, -4.6930e-02,\n          3.5801e-02, -6.5430e-02, -7.3993e-02,  2.7408e-02, -4.1742e-03,\n          8.4945e-02,  4.8946e-02, -6.9103e-02, -1.4965e-02, -1.3452e-02,\n          3.1469e-02, -1.8027e-02, -8.2285e-03,  2.7691e-02,  5.0742e-02,\n         -6.6915e-02,  1.3696e-02,  2.8090e-02, -7.0985e-02, -7.3241e-02,\n          6.1845e-02, -2.4358e-02,  4.1897e-02,  7.7708e-02, -7.5558e-02,\n          2.7831e-02,  6.5158e-02, -1.0330e-02],\n        [ 3.6015e-02, -7.3721e-02, -6.1123e-02, -4.9137e-02, -6.6549e-02,\n         -2.1873e-02,  3.0007e-02,  5.2695e-02, -4.4366e-02, -5.0841e-02,\n         -5.7355e-02,  4.5533e-02, -7.5598e-03, -8.0647e-02,  1.4523e-02,\n          1.1325e-04, -1.8972e-02,  3.6294e-02,  5.7518e-02,  2.4669e-03,\n          8.2484e-02, -2.3655e-02, -9.9052e-03,  3.7759e-02, -4.6617e-03,\n          4.6573e-02,  5.8139e-02, -7.9319e-02, -6.9485e-02, -8.1460e-02,\n          1.7735e-02,  2.6079e-02, -1.4693e-02, -6.2267e-02, -1.5748e-02,\n         -2.9616e-02, -5.6807e-03, -2.3976e-02, -6.7057e-02, -1.8236e-02,\n         -2.4524e-02, -8.2632e-02,  5.8781e-02, -7.9006e-02,  1.8927e-02,\n         -8.5246e-02,  6.4636e-02,  6.2560e-02,  7.2967e-02,  2.1528e-02,\n          5.7575e-02,  4.0108e-02, -7.7663e-04,  5.5049e-02,  4.1117e-02,\n         -1.9186e-02,  2.3038e-02, -4.1557e-02, -6.9387e-02,  3.7276e-02,\n          7.0192e-02, -7.7572e-02,  7.5860e-04,  4.2696e-02,  7.7851e-02,\n         -1.8854e-02, -4.0223e-02, -3.2182e-02, -7.2016e-03,  5.5428e-02,\n          3.2204e-02,  6.7323e-02, -4.5762e-02,  5.3395e-02, -1.7530e-02,\n          8.0932e-03, -6.2816e-02, -2.0151e-02,  5.0267e-02,  5.1991e-03,\n         -3.9421e-02,  5.2663e-02,  2.2626e-02, -4.9664e-02,  7.9072e-02,\n          2.7733e-02,  1.3360e-02,  2.4339e-02, -1.0591e-02, -5.9875e-02,\n         -7.3395e-02, -8.1470e-02, -5.3831e-02,  4.4267e-03,  7.1639e-02,\n          1.1854e-03, -7.6613e-02, -6.3231e-02, -1.1722e-02,  7.7123e-02,\n          3.7508e-02, -7.5630e-02,  5.2960e-02,  2.1958e-02,  6.9444e-02,\n         -6.3334e-02, -6.7661e-02, -3.8695e-02,  4.5678e-02,  3.1068e-02,\n          4.2695e-02,  1.3162e-02, -7.1864e-02,  6.5303e-02,  8.1385e-02,\n          6.7686e-03, -7.8458e-02,  7.0889e-02,  5.7410e-02,  8.7130e-02,\n         -7.7562e-02, -3.4290e-03,  1.0354e-03, -4.4126e-02,  3.3712e-02,\n         -7.7294e-02, -5.6201e-02,  6.5130e-02],\n        [-2.3688e-02, -6.8426e-02, -1.8985e-02, -4.6445e-02,  7.3860e-02,\n          1.1385e-03, -8.1314e-02, -5.2774e-02,  7.7640e-02, -5.5959e-02,\n         -3.2507e-02,  6.3651e-02, -5.8515e-02,  7.6868e-02, -1.0540e-02,\n         -4.1281e-02, -1.6841e-02,  6.2740e-02,  6.4730e-02,  3.0176e-02,\n          4.9334e-02,  7.0135e-02, -3.5927e-02,  3.6833e-02, -5.9092e-02,\n          5.0705e-02,  3.1045e-02, -8.2114e-02,  6.4813e-02, -7.4685e-03,\n         -3.6039e-02,  8.1741e-02,  3.8580e-02,  6.1165e-02,  3.4243e-02,\n         -2.3910e-02,  3.5682e-02,  7.9129e-02, -5.2285e-02, -5.7467e-02,\n         -7.3882e-02,  5.3618e-02,  5.3679e-02,  5.2618e-02,  4.3722e-02,\n          1.0105e-02, -7.7591e-02, -6.2400e-02, -3.1544e-02, -1.0538e-02,\n          5.2627e-03, -5.3583e-02,  4.7509e-02, -4.6989e-02,  7.5846e-02,\n          7.5605e-02, -4.5845e-02,  7.4795e-02,  6.6199e-02,  6.6384e-02,\n          3.5480e-02, -8.6208e-02,  4.9803e-02,  8.4029e-02,  8.3675e-03,\n         -4.3219e-02,  2.6954e-02, -1.1359e-02,  7.4010e-02, -6.2754e-02,\n          6.6166e-02, -4.3641e-02, -1.9596e-02,  2.6711e-02, -8.4652e-02,\n          1.3311e-02,  6.4819e-02, -1.4718e-02,  8.6927e-02, -3.1876e-02,\n         -6.7440e-02, -1.7868e-02, -8.4854e-02,  5.6679e-02, -3.3479e-02,\n          8.5279e-02,  7.3957e-02,  5.6554e-02, -1.2435e-02, -4.0435e-02,\n         -1.7292e-02,  7.3504e-02, -8.3607e-02, -8.7279e-02,  1.4382e-03,\n          2.6749e-02,  8.1761e-02,  4.6842e-02,  6.5704e-02,  3.6395e-02,\n         -4.9719e-02, -3.7317e-02,  8.0747e-02, -2.7066e-02,  8.5698e-02,\n          6.9265e-02, -1.0543e-03,  5.0751e-02, -4.0387e-02,  6.8823e-02,\n         -4.8957e-02, -5.8486e-02, -6.1623e-02, -4.9287e-02,  6.8530e-02,\n          8.2820e-02,  7.7628e-02, -8.7675e-02, -5.8246e-03, -5.0328e-03,\n         -7.0053e-02,  1.3139e-02,  9.2743e-03,  4.3842e-02,  6.0395e-02,\n         -8.7650e-02, -5.4166e-03,  3.3553e-02],\n        [ 2.6867e-02,  4.9358e-02,  6.3412e-02,  1.3391e-03, -8.0698e-02,\n         -1.1033e-03,  4.8721e-02,  6.9900e-02, -7.8890e-02,  3.2287e-02,\n         -3.4317e-02, -7.5140e-02, -7.7991e-02,  6.9596e-02,  1.3400e-02,\n          1.4992e-02, -7.0715e-02,  8.5301e-02,  6.5743e-02,  2.8936e-02,\n         -1.3684e-02,  8.1467e-03, -4.3627e-02,  7.5665e-03,  6.8646e-02,\n         -4.0232e-02, -1.9841e-02,  6.9191e-02, -5.5459e-02, -7.5797e-02,\n          3.6137e-02, -4.4530e-02,  4.7757e-02, -6.0465e-02, -1.1987e-02,\n          6.7953e-02, -5.8202e-02, -2.2955e-02, -6.2094e-02, -7.7191e-02,\n         -1.0553e-02, -6.0260e-02, -4.6730e-02,  8.3404e-02,  8.4873e-02,\n         -4.4816e-02,  7.4921e-02, -1.6436e-02, -7.6090e-02,  7.5996e-02,\n         -5.7025e-02, -5.8042e-02, -7.6339e-02,  7.8636e-02, -5.1609e-02,\n          3.9668e-02, -6.5982e-02, -8.9555e-03, -2.9717e-02, -1.9535e-02,\n          4.1722e-02,  7.2241e-02,  3.4582e-02,  5.6830e-03, -4.1162e-02,\n          7.9693e-02,  7.2568e-03, -2.9245e-04, -2.7227e-02,  8.5642e-02,\n          3.0164e-02,  2.3778e-02, -8.6668e-03, -9.7977e-03,  6.5856e-02,\n          7.2592e-03,  4.3165e-02,  4.6822e-02,  1.2199e-02, -1.1169e-02,\n         -7.2326e-02,  4.3575e-02,  5.8917e-02, -3.3072e-02, -5.5889e-02,\n         -5.7430e-02, -7.1511e-02, -6.6756e-02, -1.0955e-02, -4.9458e-03,\n          7.4011e-02, -2.1322e-02,  5.9850e-02,  3.3888e-02,  5.0232e-02,\n         -8.1514e-02, -4.9269e-02,  3.3956e-02,  4.9277e-02, -8.3440e-02,\n         -1.2412e-02,  8.7221e-02,  6.6664e-02,  9.8756e-03,  4.1980e-02,\n         -1.9782e-02, -2.8388e-04,  7.8769e-02, -8.2593e-04,  2.6048e-02,\n         -6.0687e-02, -3.9573e-02,  4.3490e-02,  1.8300e-02,  6.0348e-02,\n          1.4032e-02, -5.2495e-02, -1.8915e-02, -2.7210e-03,  1.5046e-02,\n         -8.2144e-02,  1.0951e-02,  2.8631e-02, -8.4557e-02,  7.3725e-02,\n          4.9267e-03, -7.9453e-03, -7.1437e-02]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	4,
                                "training":	true
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	true
                }
            },
            "_q_optimizer":	{
                "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)":	{
                    "_optimizer_load_state_dict_post_hooks":	{},
                    "_optimizer_load_state_dict_pre_hooks":	{},
                    "_optimizer_state_dict_post_hooks":	{},
                    "_optimizer_state_dict_pre_hooks":	{},
                    "_optimizer_step_post_hooks":	{},
                    "_optimizer_step_pre_hooks":	{},
                    "_warned_capturable_if_run_uncaptured":	true,
                    "_zero_grad_profile_name":	"Optimizer.zero_grad#Adam.zero_grad",
                    "defaults":	{
                        "amsgrad":	false,
                        "betas":	[
                            0.9,
                            0.999
                        ],
                        "capturable":	false,
                        "differentiable":	false,
                        "eps":	1e-08,
                        "foreach":	null,
                        "fused":	null,
                        "lr":	0.0003,
                        "maximize":	false,
                        "weight_decay":	0
                    },
                    "param_groups":	[
                        {
                            "amsgrad":	false,
                            "betas":	[
                                0.9,
                                0.999
                            ],
                            "capturable":	false,
                            "differentiable":	false,
                            "eps":	1e-08,
                            "foreach":	null,
                            "fused":	null,
                            "lr":	0.0003,
                            "maximize":	false,
                            "params":	[
                                "Parameter containing:\ntensor([[ 0.1221,  0.3156, -0.3061,  ..., -0.2856,  0.0323, -0.0214],\n        [-0.3207,  0.1525,  0.0823,  ...,  0.3214, -0.2110, -0.1757],\n        [-0.0298,  0.1829,  0.1372,  ...,  0.2828, -0.2539, -0.1425],\n        ...,\n        [ 0.2474,  0.2701,  0.1650,  ...,  0.2265,  0.0092,  0.1705],\n        [ 0.0940, -0.0642,  0.3076,  ...,  0.0769, -0.1131, -0.1607],\n        [-0.2862,  0.2340,  0.3354,  ..., -0.2703,  0.2698, -0.2547]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([-0.1678,  0.0339, -0.1494, -0.1933,  0.1627, -0.2005,  0.3499,  0.2298,\n         0.0818,  0.0954,  0.2433, -0.0436,  0.0085,  0.1860,  0.2392, -0.2033,\n         0.1594,  0.2888,  0.3194,  0.1396, -0.0832, -0.1263,  0.0787,  0.1147,\n         0.2736,  0.2191, -0.0664,  0.2073,  0.0828,  0.2463, -0.3356,  0.2003,\n        -0.0293, -0.3049, -0.1340,  0.3302,  0.2521, -0.1522,  0.1099, -0.0875,\n         0.0882, -0.3185,  0.0769,  0.0094, -0.2892, -0.0686,  0.3485,  0.1761,\n        -0.3448, -0.2173, -0.3189,  0.0119, -0.2709,  0.0366, -0.0584,  0.3131,\n         0.1165, -0.0283, -0.0047, -0.1771,  0.2787,  0.3305,  0.1370,  0.0083,\n        -0.1749, -0.0580,  0.1338,  0.1171,  0.0935,  0.1530,  0.1955, -0.1035,\n        -0.2267, -0.3363,  0.2530,  0.2108,  0.2262,  0.1420, -0.1898,  0.0907,\n        -0.2759,  0.3143,  0.2204, -0.3089,  0.0537, -0.0231, -0.1438, -0.0912,\n         0.1675,  0.3414, -0.2462, -0.2372,  0.1423,  0.1533, -0.0231,  0.0470,\n        -0.3513, -0.1101, -0.1333,  0.3347, -0.0681,  0.3488, -0.2156, -0.1209,\n         0.2608,  0.0196,  0.0506,  0.1365,  0.3197,  0.1051, -0.0250,  0.3184,\n         0.3234, -0.0153,  0.0557, -0.1178, -0.1096, -0.2347,  0.3335,  0.1497,\n        -0.3373,  0.2361,  0.1639,  0.3065, -0.2938, -0.2496,  0.0726,  0.2836],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[-0.0589, -0.0427, -0.0547,  ...,  0.0404,  0.0523, -0.0588],\n        [-0.0869,  0.0525, -0.0381,  ...,  0.0101, -0.0299, -0.0324],\n        [-0.0713, -0.0430,  0.0656,  ...,  0.0340,  0.0781,  0.0281],\n        ...,\n        [ 0.0337, -0.0195,  0.0318,  ..., -0.0103, -0.0556, -0.0062],\n        [-0.0807,  0.0196,  0.0089,  ...,  0.0622, -0.0505, -0.0418],\n        [-0.0799, -0.0817, -0.0868,  ..., -0.0732,  0.0743, -0.0649]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([-0.0377,  0.0613, -0.0257,  0.0171,  0.0326, -0.0346,  0.0342, -0.0249,\n        -0.0778, -0.0238,  0.0624, -0.0503, -0.0430, -0.0252, -0.0421, -0.0415,\n        -0.0554,  0.0409, -0.0062, -0.0351, -0.0587, -0.0446,  0.0012, -0.0647,\n        -0.0330, -0.0063,  0.0577, -0.0413, -0.0500, -0.0821, -0.0790, -0.0009,\n         0.0279, -0.0617, -0.0155, -0.0754, -0.0020, -0.0055,  0.0800,  0.0202,\n         0.0724,  0.0727, -0.0168,  0.0569, -0.0370, -0.0099,  0.0856, -0.0708,\n         0.0563, -0.0047, -0.0843,  0.0138, -0.0258, -0.0870,  0.0038,  0.0789,\n         0.0334, -0.0576,  0.0070,  0.0322, -0.0844, -0.0022, -0.0498, -0.0135,\n         0.0272,  0.0362, -0.0341, -0.0213, -0.0193,  0.0256,  0.0528, -0.0225,\n        -0.0718,  0.0033, -0.0512,  0.0515,  0.0728, -0.0620,  0.0309, -0.0791,\n         0.0099,  0.0020, -0.0078, -0.0366, -0.0279,  0.0406,  0.0184, -0.0643,\n         0.0746, -0.0243, -0.0876, -0.0142,  0.0786, -0.0100,  0.0721, -0.0076,\n        -0.0431, -0.0238, -0.0372,  0.0315,  0.0348,  0.0292, -0.0444,  0.0520,\n         0.0655, -0.0629,  0.0468, -0.0225,  0.0785, -0.0174,  0.0274, -0.0623,\n         0.0832, -0.0295, -0.0474,  0.0652,  0.0161, -0.0464, -0.0728,  0.0440,\n         0.0111,  0.0714, -0.0758,  0.0307,  0.0871,  0.0081,  0.0226,  0.0756],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[ 3.0760e-02, -6.3319e-03,  2.6579e-02, -3.8700e-03,  4.7726e-03,\n         -2.2383e-03, -8.7095e-02, -7.4575e-03, -2.8319e-02,  5.2629e-02,\n         -2.9483e-03,  1.7023e-02,  8.0125e-02, -4.6618e-02,  7.8126e-02,\n         -5.6478e-02,  8.3151e-02, -5.9233e-02, -5.6001e-03,  1.3185e-02,\n         -6.5981e-02, -4.9092e-02, -8.0500e-05, -3.0654e-02,  6.5160e-02,\n         -6.2362e-02, -8.6130e-02,  1.4424e-02, -7.8967e-02, -2.3674e-02,\n          7.8869e-02,  1.8081e-02, -3.9356e-02,  1.2873e-02,  1.5156e-02,\n         -8.1369e-02,  1.2578e-02,  7.5945e-02, -8.4374e-03,  5.8782e-02,\n          4.1959e-02, -3.3981e-02, -8.5847e-02, -3.2441e-03, -4.4975e-03,\n         -1.6353e-02, -6.4326e-02, -4.1447e-02,  8.5039e-02,  7.5716e-02,\n         -4.8713e-02,  3.9323e-02, -2.9118e-02,  1.5190e-02, -6.9061e-02,\n          1.5074e-02, -3.1328e-02,  8.6361e-02, -6.6093e-02,  2.8899e-02,\n          2.4180e-02, -4.0614e-02, -1.0652e-02,  7.4347e-02,  8.7235e-02,\n         -8.8355e-02, -3.6629e-02, -5.9197e-02,  8.5272e-02, -3.7717e-02,\n          3.8911e-02, -5.2075e-02,  3.3264e-02,  6.8168e-02,  2.1410e-02,\n          1.2440e-02,  7.5054e-02, -6.1270e-02, -8.7264e-02, -9.8237e-03,\n         -6.2586e-02,  1.2184e-02, -4.6092e-02, -4.1382e-02,  3.5495e-02,\n          3.4159e-02, -8.5484e-02,  5.1122e-03, -9.9550e-03, -6.6177e-02,\n          5.8795e-03, -3.0552e-02, -4.7317e-02,  2.9759e-02, -3.3711e-02,\n         -8.7280e-02,  3.6407e-02,  8.2903e-02, -3.0342e-02, -4.6930e-02,\n          3.5801e-02, -6.5430e-02, -7.3993e-02,  2.7408e-02, -4.1742e-03,\n          8.4945e-02,  4.8946e-02, -6.9103e-02, -1.4965e-02, -1.3452e-02,\n          3.1469e-02, -1.8027e-02, -8.2285e-03,  2.7691e-02,  5.0742e-02,\n         -6.6915e-02,  1.3696e-02,  2.8090e-02, -7.0985e-02, -7.3241e-02,\n          6.1845e-02, -2.4358e-02,  4.1897e-02,  7.7708e-02, -7.5558e-02,\n          2.7831e-02,  6.5158e-02, -1.0330e-02],\n        [ 3.6015e-02, -7.3721e-02, -6.1123e-02, -4.9137e-02, -6.6549e-02,\n         -2.1873e-02,  3.0007e-02,  5.2695e-02, -4.4366e-02, -5.0841e-02,\n         -5.7355e-02,  4.5533e-02, -7.5598e-03, -8.0647e-02,  1.4523e-02,\n          1.1325e-04, -1.8972e-02,  3.6294e-02,  5.7518e-02,  2.4669e-03,\n          8.2484e-02, -2.3655e-02, -9.9052e-03,  3.7759e-02, -4.6617e-03,\n          4.6573e-02,  5.8139e-02, -7.9319e-02, -6.9485e-02, -8.1460e-02,\n          1.7735e-02,  2.6079e-02, -1.4693e-02, -6.2267e-02, -1.5748e-02,\n         -2.9616e-02, -5.6807e-03, -2.3976e-02, -6.7057e-02, -1.8236e-02,\n         -2.4524e-02, -8.2632e-02,  5.8781e-02, -7.9006e-02,  1.8927e-02,\n         -8.5246e-02,  6.4636e-02,  6.2560e-02,  7.2967e-02,  2.1528e-02,\n          5.7575e-02,  4.0108e-02, -7.7663e-04,  5.5049e-02,  4.1117e-02,\n         -1.9186e-02,  2.3038e-02, -4.1557e-02, -6.9387e-02,  3.7276e-02,\n          7.0192e-02, -7.7572e-02,  7.5860e-04,  4.2696e-02,  7.7851e-02,\n         -1.8854e-02, -4.0223e-02, -3.2182e-02, -7.2016e-03,  5.5428e-02,\n          3.2204e-02,  6.7323e-02, -4.5762e-02,  5.3395e-02, -1.7530e-02,\n          8.0932e-03, -6.2816e-02, -2.0151e-02,  5.0267e-02,  5.1991e-03,\n         -3.9421e-02,  5.2663e-02,  2.2626e-02, -4.9664e-02,  7.9072e-02,\n          2.7733e-02,  1.3360e-02,  2.4339e-02, -1.0591e-02, -5.9875e-02,\n         -7.3395e-02, -8.1470e-02, -5.3831e-02,  4.4267e-03,  7.1639e-02,\n          1.1854e-03, -7.6613e-02, -6.3231e-02, -1.1722e-02,  7.7123e-02,\n          3.7508e-02, -7.5630e-02,  5.2960e-02,  2.1958e-02,  6.9444e-02,\n         -6.3334e-02, -6.7661e-02, -3.8695e-02,  4.5678e-02,  3.1068e-02,\n          4.2695e-02,  1.3162e-02, -7.1864e-02,  6.5303e-02,  8.1385e-02,\n          6.7686e-03, -7.8458e-02,  7.0889e-02,  5.7410e-02,  8.7130e-02,\n         -7.7562e-02, -3.4290e-03,  1.0354e-03, -4.4126e-02,  3.3712e-02,\n         -7.7294e-02, -5.6201e-02,  6.5130e-02],\n        [-2.3688e-02, -6.8426e-02, -1.8985e-02, -4.6445e-02,  7.3860e-02,\n          1.1385e-03, -8.1314e-02, -5.2774e-02,  7.7640e-02, -5.5959e-02,\n         -3.2507e-02,  6.3651e-02, -5.8515e-02,  7.6868e-02, -1.0540e-02,\n         -4.1281e-02, -1.6841e-02,  6.2740e-02,  6.4730e-02,  3.0176e-02,\n          4.9334e-02,  7.0135e-02, -3.5927e-02,  3.6833e-02, -5.9092e-02,\n          5.0705e-02,  3.1045e-02, -8.2114e-02,  6.4813e-02, -7.4685e-03,\n         -3.6039e-02,  8.1741e-02,  3.8580e-02,  6.1165e-02,  3.4243e-02,\n         -2.3910e-02,  3.5682e-02,  7.9129e-02, -5.2285e-02, -5.7467e-02,\n         -7.3882e-02,  5.3618e-02,  5.3679e-02,  5.2618e-02,  4.3722e-02,\n          1.0105e-02, -7.7591e-02, -6.2400e-02, -3.1544e-02, -1.0538e-02,\n          5.2627e-03, -5.3583e-02,  4.7509e-02, -4.6989e-02,  7.5846e-02,\n          7.5605e-02, -4.5845e-02,  7.4795e-02,  6.6199e-02,  6.6384e-02,\n          3.5480e-02, -8.6208e-02,  4.9803e-02,  8.4029e-02,  8.3675e-03,\n         -4.3219e-02,  2.6954e-02, -1.1359e-02,  7.4010e-02, -6.2754e-02,\n          6.6166e-02, -4.3641e-02, -1.9596e-02,  2.6711e-02, -8.4652e-02,\n          1.3311e-02,  6.4819e-02, -1.4718e-02,  8.6927e-02, -3.1876e-02,\n         -6.7440e-02, -1.7868e-02, -8.4854e-02,  5.6679e-02, -3.3479e-02,\n          8.5279e-02,  7.3957e-02,  5.6554e-02, -1.2435e-02, -4.0435e-02,\n         -1.7292e-02,  7.3504e-02, -8.3607e-02, -8.7279e-02,  1.4382e-03,\n          2.6749e-02,  8.1761e-02,  4.6842e-02,  6.5704e-02,  3.6395e-02,\n         -4.9719e-02, -3.7317e-02,  8.0747e-02, -2.7066e-02,  8.5698e-02,\n          6.9265e-02, -1.0543e-03,  5.0751e-02, -4.0387e-02,  6.8823e-02,\n         -4.8957e-02, -5.8486e-02, -6.1623e-02, -4.9287e-02,  6.8530e-02,\n          8.2820e-02,  7.7628e-02, -8.7675e-02, -5.8246e-03, -5.0328e-03,\n         -7.0053e-02,  1.3139e-02,  9.2743e-03,  4.3842e-02,  6.0395e-02,\n         -8.7650e-02, -5.4166e-03,  3.3553e-02],\n        [ 2.6867e-02,  4.9358e-02,  6.3412e-02,  1.3391e-03, -8.0698e-02,\n         -1.1033e-03,  4.8721e-02,  6.9900e-02, -7.8890e-02,  3.2287e-02,\n         -3.4317e-02, -7.5140e-02, -7.7991e-02,  6.9596e-02,  1.3400e-02,\n          1.4992e-02, -7.0715e-02,  8.5301e-02,  6.5743e-02,  2.8936e-02,\n         -1.3684e-02,  8.1467e-03, -4.3627e-02,  7.5665e-03,  6.8646e-02,\n         -4.0232e-02, -1.9841e-02,  6.9191e-02, -5.5459e-02, -7.5797e-02,\n          3.6137e-02, -4.4530e-02,  4.7757e-02, -6.0465e-02, -1.1987e-02,\n          6.7953e-02, -5.8202e-02, -2.2955e-02, -6.2094e-02, -7.7191e-02,\n         -1.0553e-02, -6.0260e-02, -4.6730e-02,  8.3404e-02,  8.4873e-02,\n         -4.4816e-02,  7.4921e-02, -1.6436e-02, -7.6090e-02,  7.5996e-02,\n         -5.7025e-02, -5.8042e-02, -7.6339e-02,  7.8636e-02, -5.1609e-02,\n          3.9668e-02, -6.5982e-02, -8.9555e-03, -2.9717e-02, -1.9535e-02,\n          4.1722e-02,  7.2241e-02,  3.4582e-02,  5.6830e-03, -4.1162e-02,\n          7.9693e-02,  7.2568e-03, -2.9245e-04, -2.7227e-02,  8.5642e-02,\n          3.0164e-02,  2.3778e-02, -8.6668e-03, -9.7977e-03,  6.5856e-02,\n          7.2592e-03,  4.3165e-02,  4.6822e-02,  1.2199e-02, -1.1169e-02,\n         -7.2326e-02,  4.3575e-02,  5.8917e-02, -3.3072e-02, -5.5889e-02,\n         -5.7430e-02, -7.1511e-02, -6.6756e-02, -1.0955e-02, -4.9458e-03,\n          7.4011e-02, -2.1322e-02,  5.9850e-02,  3.3888e-02,  5.0232e-02,\n         -8.1514e-02, -4.9269e-02,  3.3956e-02,  4.9277e-02, -8.3440e-02,\n         -1.2412e-02,  8.7221e-02,  6.6664e-02,  9.8756e-03,  4.1980e-02,\n         -1.9782e-02, -2.8388e-04,  7.8769e-02, -8.2593e-04,  2.6048e-02,\n         -6.0687e-02, -3.9573e-02,  4.3490e-02,  1.8300e-02,  6.0348e-02,\n          1.4032e-02, -5.2495e-02, -1.8915e-02, -2.7210e-03,  1.5046e-02,\n         -8.2144e-02,  1.0951e-02,  2.8631e-02, -8.4557e-02,  7.3725e-02,\n          4.9267e-03, -7.9453e-03, -7.1437e-02]], requires_grad=True)",
                                "Parameter containing:\ntensor([-0.0721, -0.0589, -0.0815,  0.0194], requires_grad=True)"
                            ],
                            "weight_decay":	0
                        }
                    ],
                    "state":	{}
                }
            },
            "_replay_buffer":	{
                "<algorithms.DQN.replay_buffer.EnhancedReplayBuffer object at 0x79d11a51e890>":	{
                    "act_buf":	"[0 0 0 ... 0 0 0]",
                    "alpha":	0.6,
                    "aux_buf_size":	5000,
                    "beta":	0.4,
                    "beta_increment_per_sampling":	0.0001,
                    "buf_size":	50000,
                    "current_segment":	0,
                    "done_buf":	"[False False False ... False False False]",
                    "epsilon":	-1,
                    "epsilon_per_priority":	1e-06,
                    "gamma":	5,
                    "markers":	"[False False False ... False False False]",
                    "mask_buf":	"[[0. 0.]\n [0. 0.]\n [0. 0.]\n ...\n [0. 0.]\n [0. 0.]\n [0. 0.]]",
                    "mask_dim":	2,
                    "max_age":	20000,
                    "next_obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "num_segments":	0,
                    "obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "obs_dim":	8,
                    "path_start_idx":	0,
                    "ptr":	0,
                    "rew_buf":	"[0. 0. 0. ... 0. 0. 0.]",
                    "sample_decay":	0.5,
                    "segment_ptr":	0,
                    "segment_size":	62499.0,
                    "step":	0,
                    "sum_tree":	{
                        "<algorithms.DQN.replay_buffer.SumTree object at 0x79d11a760ed0>":	{
                            "capacity":	5000,
                            "data":	"[0 0 0 ... 0 0 0]",
                            "n_entries":	0,
                            "tree":	"[0. 0. 0. ... 0. 0. 0.]",
                            "write":	0
                        }
                    },
                    "timestamps":	"[0 0 0 ... 0 0 0]"
                }
            },
            "_target_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.0005,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.1678,  0.0339, -0.1494, -0.1933,  0.1627, -0.2005,  0.3499,  0.2298,\n         0.0818,  0.0954,  0.2433, -0.0436,  0.0085,  0.1860,  0.2392, -0.2033,\n         0.1594,  0.2888,  0.3194,  0.1396, -0.0832, -0.1263,  0.0787,  0.1147,\n         0.2736,  0.2191, -0.0664,  0.2073,  0.0828,  0.2463, -0.3356,  0.2003,\n        -0.0293, -0.3049, -0.1340,  0.3302,  0.2521, -0.1522,  0.1099, -0.0875,\n         0.0882, -0.3185,  0.0769,  0.0094, -0.2892, -0.0686,  0.3485,  0.1761,\n        -0.3448, -0.2173, -0.3189,  0.0119, -0.2709,  0.0366, -0.0584,  0.3131,\n         0.1165, -0.0283, -0.0047, -0.1771,  0.2787,  0.3305,  0.1370,  0.0083,\n        -0.1749, -0.0580,  0.1338,  0.1171,  0.0935,  0.1530,  0.1955, -0.1035,\n        -0.2267, -0.3363,  0.2530,  0.2108,  0.2262,  0.1420, -0.1898,  0.0907,\n        -0.2759,  0.3143,  0.2204, -0.3089,  0.0537, -0.0231, -0.1438, -0.0912,\n         0.1675,  0.3414, -0.2462, -0.2372,  0.1423,  0.1533, -0.0231,  0.0470,\n        -0.3513, -0.1101, -0.1333,  0.3347, -0.0681,  0.3488, -0.2156, -0.1209,\n         0.2608,  0.0196,  0.0506,  0.1365,  0.3197,  0.1051, -0.0250,  0.3184,\n         0.3234, -0.0153,  0.0557, -0.1178, -0.1096, -0.2347,  0.3335,  0.1497,\n        -0.3373,  0.2361,  0.1639,  0.3065, -0.2938, -0.2496,  0.0726,  0.2836],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.1221,  0.3156, -0.3061,  ..., -0.2856,  0.0323, -0.0214],\n        [-0.3207,  0.1525,  0.0823,  ...,  0.3214, -0.2110, -0.1757],\n        [-0.0298,  0.1829,  0.1372,  ...,  0.2828, -0.2539, -0.1425],\n        ...,\n        [ 0.2474,  0.2701,  0.1650,  ...,  0.2265,  0.0092,  0.1705],\n        [ 0.0940, -0.0642,  0.3076,  ...,  0.0769, -0.1131, -0.1607],\n        [-0.2862,  0.2340,  0.3354,  ..., -0.2703,  0.2698, -0.2547]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	128,
                                "training":	false
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=128, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0377,  0.0613, -0.0257,  0.0171,  0.0326, -0.0346,  0.0342, -0.0249,\n        -0.0778, -0.0238,  0.0624, -0.0503, -0.0430, -0.0252, -0.0421, -0.0415,\n        -0.0554,  0.0409, -0.0062, -0.0351, -0.0587, -0.0446,  0.0012, -0.0647,\n        -0.0330, -0.0063,  0.0577, -0.0413, -0.0500, -0.0821, -0.0790, -0.0009,\n         0.0279, -0.0617, -0.0155, -0.0754, -0.0020, -0.0055,  0.0800,  0.0202,\n         0.0724,  0.0727, -0.0168,  0.0569, -0.0370, -0.0099,  0.0856, -0.0708,\n         0.0563, -0.0047, -0.0843,  0.0138, -0.0258, -0.0870,  0.0038,  0.0789,\n         0.0334, -0.0576,  0.0070,  0.0322, -0.0844, -0.0022, -0.0498, -0.0135,\n         0.0272,  0.0362, -0.0341, -0.0213, -0.0193,  0.0256,  0.0528, -0.0225,\n        -0.0718,  0.0033, -0.0512,  0.0515,  0.0728, -0.0620,  0.0309, -0.0791,\n         0.0099,  0.0020, -0.0078, -0.0366, -0.0279,  0.0406,  0.0184, -0.0643,\n         0.0746, -0.0243, -0.0876, -0.0142,  0.0786, -0.0100,  0.0721, -0.0076,\n        -0.0431, -0.0238, -0.0372,  0.0315,  0.0348,  0.0292, -0.0444,  0.0520,\n         0.0655, -0.0629,  0.0468, -0.0225,  0.0785, -0.0174,  0.0274, -0.0623,\n         0.0832, -0.0295, -0.0474,  0.0652,  0.0161, -0.0464, -0.0728,  0.0440,\n         0.0111,  0.0714, -0.0758,  0.0307,  0.0871,  0.0081,  0.0226,  0.0756],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.0589, -0.0427, -0.0547,  ...,  0.0404,  0.0523, -0.0588],\n        [-0.0869,  0.0525, -0.0381,  ...,  0.0101, -0.0299, -0.0324],\n        [-0.0713, -0.0430,  0.0656,  ...,  0.0340,  0.0781,  0.0281],\n        ...,\n        [ 0.0337, -0.0195,  0.0318,  ..., -0.0103, -0.0556, -0.0062],\n        [-0.0807,  0.0196,  0.0089,  ...,  0.0622, -0.0505, -0.0418],\n        [-0.0799, -0.0817, -0.0868,  ..., -0.0732,  0.0743, -0.0649]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	128,
                                "training":	false
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=128, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0721, -0.0589, -0.0815,  0.0194], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 3.0760e-02, -6.3319e-03,  2.6579e-02, -3.8700e-03,  4.7726e-03,\n         -2.2383e-03, -8.7095e-02, -7.4575e-03, -2.8319e-02,  5.2629e-02,\n         -2.9483e-03,  1.7023e-02,  8.0125e-02, -4.6618e-02,  7.8126e-02,\n         -5.6478e-02,  8.3151e-02, -5.9233e-02, -5.6001e-03,  1.3185e-02,\n         -6.5981e-02, -4.9092e-02, -8.0500e-05, -3.0654e-02,  6.5160e-02,\n         -6.2362e-02, -8.6130e-02,  1.4424e-02, -7.8967e-02, -2.3674e-02,\n          7.8869e-02,  1.8081e-02, -3.9356e-02,  1.2873e-02,  1.5156e-02,\n         -8.1369e-02,  1.2578e-02,  7.5945e-02, -8.4374e-03,  5.8782e-02,\n          4.1959e-02, -3.3981e-02, -8.5847e-02, -3.2441e-03, -4.4975e-03,\n         -1.6353e-02, -6.4326e-02, -4.1447e-02,  8.5039e-02,  7.5716e-02,\n         -4.8713e-02,  3.9323e-02, -2.9118e-02,  1.5190e-02, -6.9061e-02,\n          1.5074e-02, -3.1328e-02,  8.6361e-02, -6.6093e-02,  2.8899e-02,\n          2.4180e-02, -4.0614e-02, -1.0652e-02,  7.4347e-02,  8.7235e-02,\n         -8.8355e-02, -3.6629e-02, -5.9197e-02,  8.5272e-02, -3.7717e-02,\n          3.8911e-02, -5.2075e-02,  3.3264e-02,  6.8168e-02,  2.1410e-02,\n          1.2440e-02,  7.5054e-02, -6.1270e-02, -8.7264e-02, -9.8237e-03,\n         -6.2586e-02,  1.2184e-02, -4.6092e-02, -4.1382e-02,  3.5495e-02,\n          3.4159e-02, -8.5484e-02,  5.1122e-03, -9.9550e-03, -6.6177e-02,\n          5.8795e-03, -3.0552e-02, -4.7317e-02,  2.9759e-02, -3.3711e-02,\n         -8.7280e-02,  3.6407e-02,  8.2903e-02, -3.0342e-02, -4.6930e-02,\n          3.5801e-02, -6.5430e-02, -7.3993e-02,  2.7408e-02, -4.1742e-03,\n          8.4945e-02,  4.8946e-02, -6.9103e-02, -1.4965e-02, -1.3452e-02,\n          3.1469e-02, -1.8027e-02, -8.2285e-03,  2.7691e-02,  5.0742e-02,\n         -6.6915e-02,  1.3696e-02,  2.8090e-02, -7.0985e-02, -7.3241e-02,\n          6.1845e-02, -2.4358e-02,  4.1897e-02,  7.7708e-02, -7.5558e-02,\n          2.7831e-02,  6.5158e-02, -1.0330e-02],\n        [ 3.6015e-02, -7.3721e-02, -6.1123e-02, -4.9137e-02, -6.6549e-02,\n         -2.1873e-02,  3.0007e-02,  5.2695e-02, -4.4366e-02, -5.0841e-02,\n         -5.7355e-02,  4.5533e-02, -7.5598e-03, -8.0647e-02,  1.4523e-02,\n          1.1325e-04, -1.8972e-02,  3.6294e-02,  5.7518e-02,  2.4669e-03,\n          8.2484e-02, -2.3655e-02, -9.9052e-03,  3.7759e-02, -4.6617e-03,\n          4.6573e-02,  5.8139e-02, -7.9319e-02, -6.9485e-02, -8.1460e-02,\n          1.7735e-02,  2.6079e-02, -1.4693e-02, -6.2267e-02, -1.5748e-02,\n         -2.9616e-02, -5.6807e-03, -2.3976e-02, -6.7057e-02, -1.8236e-02,\n         -2.4524e-02, -8.2632e-02,  5.8781e-02, -7.9006e-02,  1.8927e-02,\n         -8.5246e-02,  6.4636e-02,  6.2560e-02,  7.2967e-02,  2.1528e-02,\n          5.7575e-02,  4.0108e-02, -7.7663e-04,  5.5049e-02,  4.1117e-02,\n         -1.9186e-02,  2.3038e-02, -4.1557e-02, -6.9387e-02,  3.7276e-02,\n          7.0192e-02, -7.7572e-02,  7.5860e-04,  4.2696e-02,  7.7851e-02,\n         -1.8854e-02, -4.0223e-02, -3.2182e-02, -7.2016e-03,  5.5428e-02,\n          3.2204e-02,  6.7323e-02, -4.5762e-02,  5.3395e-02, -1.7530e-02,\n          8.0932e-03, -6.2816e-02, -2.0151e-02,  5.0267e-02,  5.1991e-03,\n         -3.9421e-02,  5.2663e-02,  2.2626e-02, -4.9664e-02,  7.9072e-02,\n          2.7733e-02,  1.3360e-02,  2.4339e-02, -1.0591e-02, -5.9875e-02,\n         -7.3395e-02, -8.1470e-02, -5.3831e-02,  4.4267e-03,  7.1639e-02,\n          1.1854e-03, -7.6613e-02, -6.3231e-02, -1.1722e-02,  7.7123e-02,\n          3.7508e-02, -7.5630e-02,  5.2960e-02,  2.1958e-02,  6.9444e-02,\n         -6.3334e-02, -6.7661e-02, -3.8695e-02,  4.5678e-02,  3.1068e-02,\n          4.2695e-02,  1.3162e-02, -7.1864e-02,  6.5303e-02,  8.1385e-02,\n          6.7686e-03, -7.8458e-02,  7.0889e-02,  5.7410e-02,  8.7130e-02,\n         -7.7562e-02, -3.4290e-03,  1.0354e-03, -4.4126e-02,  3.3712e-02,\n         -7.7294e-02, -5.6201e-02,  6.5130e-02],\n        [-2.3688e-02, -6.8426e-02, -1.8985e-02, -4.6445e-02,  7.3860e-02,\n          1.1385e-03, -8.1314e-02, -5.2774e-02,  7.7640e-02, -5.5959e-02,\n         -3.2507e-02,  6.3651e-02, -5.8515e-02,  7.6868e-02, -1.0540e-02,\n         -4.1281e-02, -1.6841e-02,  6.2740e-02,  6.4730e-02,  3.0176e-02,\n          4.9334e-02,  7.0135e-02, -3.5927e-02,  3.6833e-02, -5.9092e-02,\n          5.0705e-02,  3.1045e-02, -8.2114e-02,  6.4813e-02, -7.4685e-03,\n         -3.6039e-02,  8.1741e-02,  3.8580e-02,  6.1165e-02,  3.4243e-02,\n         -2.3910e-02,  3.5682e-02,  7.9129e-02, -5.2285e-02, -5.7467e-02,\n         -7.3882e-02,  5.3618e-02,  5.3679e-02,  5.2618e-02,  4.3722e-02,\n          1.0105e-02, -7.7591e-02, -6.2400e-02, -3.1544e-02, -1.0538e-02,\n          5.2627e-03, -5.3583e-02,  4.7509e-02, -4.6989e-02,  7.5846e-02,\n          7.5605e-02, -4.5845e-02,  7.4795e-02,  6.6199e-02,  6.6384e-02,\n          3.5480e-02, -8.6208e-02,  4.9803e-02,  8.4029e-02,  8.3675e-03,\n         -4.3219e-02,  2.6954e-02, -1.1359e-02,  7.4010e-02, -6.2754e-02,\n          6.6166e-02, -4.3641e-02, -1.9596e-02,  2.6711e-02, -8.4652e-02,\n          1.3311e-02,  6.4819e-02, -1.4718e-02,  8.6927e-02, -3.1876e-02,\n         -6.7440e-02, -1.7868e-02, -8.4854e-02,  5.6679e-02, -3.3479e-02,\n          8.5279e-02,  7.3957e-02,  5.6554e-02, -1.2435e-02, -4.0435e-02,\n         -1.7292e-02,  7.3504e-02, -8.3607e-02, -8.7279e-02,  1.4382e-03,\n          2.6749e-02,  8.1761e-02,  4.6842e-02,  6.5704e-02,  3.6395e-02,\n         -4.9719e-02, -3.7317e-02,  8.0747e-02, -2.7066e-02,  8.5698e-02,\n          6.9265e-02, -1.0543e-03,  5.0751e-02, -4.0387e-02,  6.8823e-02,\n         -4.8957e-02, -5.8486e-02, -6.1623e-02, -4.9287e-02,  6.8530e-02,\n          8.2820e-02,  7.7628e-02, -8.7675e-02, -5.8246e-03, -5.0328e-03,\n         -7.0053e-02,  1.3139e-02,  9.2743e-03,  4.3842e-02,  6.0395e-02,\n         -8.7650e-02, -5.4166e-03,  3.3553e-02],\n        [ 2.6867e-02,  4.9358e-02,  6.3412e-02,  1.3391e-03, -8.0698e-02,\n         -1.1033e-03,  4.8721e-02,  6.9900e-02, -7.8890e-02,  3.2287e-02,\n         -3.4317e-02, -7.5140e-02, -7.7991e-02,  6.9596e-02,  1.3400e-02,\n          1.4992e-02, -7.0715e-02,  8.5301e-02,  6.5743e-02,  2.8936e-02,\n         -1.3684e-02,  8.1467e-03, -4.3627e-02,  7.5665e-03,  6.8646e-02,\n         -4.0232e-02, -1.9841e-02,  6.9191e-02, -5.5459e-02, -7.5797e-02,\n          3.6137e-02, -4.4530e-02,  4.7757e-02, -6.0465e-02, -1.1987e-02,\n          6.7953e-02, -5.8202e-02, -2.2955e-02, -6.2094e-02, -7.7191e-02,\n         -1.0553e-02, -6.0260e-02, -4.6730e-02,  8.3404e-02,  8.4873e-02,\n         -4.4816e-02,  7.4921e-02, -1.6436e-02, -7.6090e-02,  7.5996e-02,\n         -5.7025e-02, -5.8042e-02, -7.6339e-02,  7.8636e-02, -5.1609e-02,\n          3.9668e-02, -6.5982e-02, -8.9555e-03, -2.9717e-02, -1.9535e-02,\n          4.1722e-02,  7.2241e-02,  3.4582e-02,  5.6830e-03, -4.1162e-02,\n          7.9693e-02,  7.2568e-03, -2.9245e-04, -2.7227e-02,  8.5642e-02,\n          3.0164e-02,  2.3778e-02, -8.6668e-03, -9.7977e-03,  6.5856e-02,\n          7.2592e-03,  4.3165e-02,  4.6822e-02,  1.2199e-02, -1.1169e-02,\n         -7.2326e-02,  4.3575e-02,  5.8917e-02, -3.3072e-02, -5.5889e-02,\n         -5.7430e-02, -7.1511e-02, -6.6756e-02, -1.0955e-02, -4.9458e-03,\n          7.4011e-02, -2.1322e-02,  5.9850e-02,  3.3888e-02,  5.0232e-02,\n         -8.1514e-02, -4.9269e-02,  3.3956e-02,  4.9277e-02, -8.3440e-02,\n         -1.2412e-02,  8.7221e-02,  6.6664e-02,  9.8756e-03,  4.1980e-02,\n         -1.9782e-02, -2.8388e-04,  7.8769e-02, -8.2593e-04,  2.6048e-02,\n         -6.0687e-02, -3.9573e-02,  4.3490e-02,  1.8300e-02,  6.0348e-02,\n          1.4032e-02, -5.2495e-02, -1.8915e-02, -2.7210e-03,  1.5046e-02,\n         -8.2144e-02,  1.0951e-02,  2.8631e-02, -8.4557e-02,  7.3725e-02,\n          4.9267e-03, -7.9453e-03, -7.1437e-02]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	4,
                                "training":	false
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	false
                }
            },
            "_target_net_update_freq":	100,
            "_train_q_iters":	80,
            "_train_update_freq":	3,
            "_traj_per_epoch":	6,
            "logger":	{
                "<utils.logger.EpochLogger object at 0x79d1183d4890>":	{
                    "epoch_dict":	{},
                    "exp_name":	"rl4sys-dqn-info",
                    "first_row":	true,
                    "log_current_row":	{},
                    "log_headers":	[],
                    "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s472000000",
                    "output_file":	{
                        "<_io.TextIOWrapper name='/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s472000000/progress.txt' mode='w' encoding='UTF-8'>":	{
                            "mode":	"w"
                        }
                    }
                }
            }
        }
    },
    "target_net_update_freq":	100,
    "train_q_iters":	80,
    "train_update_freq":	3,
    "traj_per_epoch":	6
}