{
    "__class__":	"DQN",
    "act_dim":	4,
    "batch_size":	128,
    "buf_size":	50000,
    "env_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar",
    "epsilon":	1.0,
    "epsilon_decay":	0.005,
    "epsilon_min":	0.01,
    "exp_name":	"rl4sys-dqn-info",
    "gamma":	0.99,
    "kernel_dim":	4,
    "kernel_size":	2,
    "log_data_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/",
    "logger_kwargs":	{
        "exp_name":	"rl4sys-dqn-info",
        "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s479060000"
    },
    "q_lr":	0.003,
    "seed":	479060000,
    "self":	{
        "<algorithms.DQN.DQN.DQN object at 0x7164734136d0>":	{
            "_act_dim":	4,
            "_batch_size":	128,
            "_buf_size":	50000,
            "_epsilon":	1.0,
            "_epsilon_decay":	0.005,
            "_epsilon_min":	0.01,
            "_gamma":	0.99,
            "_kernel_dim":	4,
            "_kernel_size":	2,
            "_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=100, bias=True)\n  (fc3): Linear(in_features=100, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.005,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=100, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.2776,  0.0530, -0.0413, -0.2068,  0.0085, -0.0816, -0.1873,  0.2525,\n        -0.3176,  0.2332, -0.0310, -0.0602,  0.1095, -0.3188,  0.0679, -0.1124,\n        -0.0107,  0.3527, -0.1240, -0.0695,  0.2589,  0.0595, -0.1630, -0.1309,\n         0.1576, -0.2995,  0.0826, -0.3050, -0.0412,  0.2724, -0.0108, -0.0444,\n         0.1360, -0.1247, -0.0115, -0.1502, -0.2397,  0.2200,  0.1022,  0.0082,\n         0.0324,  0.1130, -0.2807, -0.0391,  0.2549,  0.2725, -0.1733, -0.3262,\n        -0.2927, -0.0028,  0.2267, -0.2232, -0.0025, -0.1832, -0.1855,  0.2648,\n        -0.0535,  0.1467,  0.0122, -0.2811, -0.1049, -0.0279, -0.1289,  0.1941,\n        -0.1939,  0.0760, -0.1299,  0.2089, -0.3291, -0.1505,  0.1747,  0.0879,\n         0.3355, -0.1145,  0.1307, -0.3032,  0.1830,  0.0221, -0.2908, -0.0398,\n        -0.0411, -0.2375,  0.2473, -0.0925, -0.0620,  0.2743, -0.2277,  0.1279,\n        -0.1590,  0.2656, -0.0533, -0.3051,  0.3171, -0.1015,  0.1768, -0.0439,\n        -0.0114, -0.3054, -0.1438,  0.2123], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-3.1922e-01,  9.3278e-02,  9.0130e-02,  2.9280e-01,  2.2603e-01,\n          1.3055e-01, -2.1449e-02, -2.0891e-01],\n        [-1.6534e-01,  1.0081e-01,  3.1594e-01, -2.5100e-01,  3.4934e-02,\n          2.7505e-02,  2.9254e-01,  2.7859e-01],\n        [ 3.4758e-01, -7.5131e-02,  3.4708e-01,  3.0609e-01,  3.1915e-01,\n         -1.3338e-01,  4.9414e-02, -2.1607e-01],\n        [ 1.5013e-01, -3.5349e-02,  1.4531e-01,  3.5148e-02, -1.3870e-01,\n         -2.8126e-01,  8.3982e-02,  1.9716e-01],\n        [ 2.2253e-01,  1.7833e-02, -2.9716e-02,  2.5503e-01,  7.7748e-03,\n         -6.0817e-02, -1.2872e-01, -5.1816e-02],\n        [ 1.5541e-01, -1.3660e-01,  7.4146e-02, -5.0384e-02, -1.3891e-01,\n         -1.0892e-01,  1.2612e-01, -3.1118e-01],\n        [-2.4859e-01, -2.6574e-01,  1.9045e-01, -2.6092e-01,  1.3966e-01,\n         -7.9672e-02, -2.0820e-01, -2.7812e-01],\n        [ 3.3852e-01,  2.2146e-01,  1.0356e-01,  6.8931e-02, -5.3033e-02,\n         -6.6212e-02,  8.6013e-02, -3.4141e-02],\n        [-2.3655e-01, -2.6068e-01, -1.3784e-01,  2.0691e-01,  3.1976e-01,\n          2.2000e-01,  2.4764e-01, -5.4643e-02],\n        [-1.2710e-01,  2.6466e-01, -1.1192e-01,  3.0407e-01,  2.8081e-01,\n         -5.7168e-03,  1.1752e-01, -1.8463e-02],\n        [ 3.3612e-01, -2.3264e-01,  2.9211e-01,  2.7187e-01, -1.9306e-01,\n          3.2373e-01, -1.2094e-01,  3.1527e-01],\n        [ 1.8989e-02,  2.3833e-01,  3.4400e-01, -2.9806e-01, -4.8142e-02,\n          3.1572e-01, -1.6642e-03, -1.0383e-01],\n        [ 1.9480e-01,  7.8094e-03,  3.4949e-01,  2.6412e-01, -4.1616e-02,\n         -3.1865e-01, -1.2291e-02, -4.3876e-02],\n        [ 1.2321e-01,  2.2697e-01, -3.0804e-02,  3.0816e-01,  6.1992e-02,\n         -2.4048e-01, -9.0704e-02, -7.1180e-02],\n        [ 1.8862e-01, -2.5234e-01,  2.0715e-01,  1.0592e-02, -8.7255e-02,\n         -2.8739e-01,  2.0671e-01, -1.5088e-01],\n        [-7.6047e-02, -4.5502e-02,  9.9739e-02,  3.0032e-01,  2.0214e-01,\n         -2.8067e-01, -2.7893e-01, -1.9939e-01],\n        [ 1.1785e-01, -2.4699e-01, -2.1821e-01,  1.8299e-02,  1.6944e-01,\n          3.5281e-01,  3.4911e-01,  5.8828e-02],\n        [-1.6374e-01,  3.4982e-01, -2.0381e-01,  2.8965e-01,  3.3085e-01,\n         -2.4311e-01, -3.1576e-01,  2.8505e-01],\n        [ 1.4446e-01,  3.4026e-01, -1.1013e-01, -2.7856e-01,  1.5957e-01,\n          3.2595e-01,  3.5265e-01,  2.3948e-01],\n        [-2.5434e-01, -2.5863e-01, -2.7481e-01, -2.9822e-01,  1.0530e-01,\n          6.8252e-02, -2.8607e-01, -1.9976e-01],\n        [-3.8182e-02, -3.0945e-01,  2.8368e-02,  3.3276e-01, -2.3604e-01,\n         -1.9761e-01, -1.5368e-01, -1.6786e-01],\n        [ 1.5037e-01, -2.0285e-01, -3.0123e-01,  8.4302e-02,  1.8010e-01,\n          2.1215e-01,  1.2484e-01, -1.7013e-01],\n        [-1.7598e-01, -1.0265e-01, -3.0789e-01, -1.2274e-01,  2.5389e-01,\n          2.0449e-01,  3.1798e-01,  2.4840e-01],\n        [ 1.1927e-01,  2.8323e-01, -1.7218e-02,  3.9173e-02, -1.7430e-01,\n          2.5728e-01,  3.1723e-01, -3.2676e-01],\n        [-1.6581e-01,  2.7567e-01,  3.4973e-01,  2.5490e-01,  1.5681e-01,\n          3.4952e-01,  2.2022e-02,  1.4101e-02],\n        [-3.4573e-01,  1.1864e-01, -1.9011e-01, -2.2155e-01,  3.3068e-01,\n         -1.6266e-02, -3.0327e-01, -1.6996e-01],\n        [-3.5072e-01,  9.6342e-02,  2.7218e-01, -2.9991e-01,  3.2447e-01,\n         -2.7956e-01,  3.4911e-01, -1.3909e-01],\n        [ 2.5468e-01,  2.3535e-01,  1.4984e-01, -6.6065e-02, -2.5744e-01,\n         -4.2600e-02,  2.1035e-01,  2.2260e-01],\n        [-7.1128e-02,  6.9909e-02, -3.2276e-01, -2.3864e-01, -7.5238e-02,\n         -2.3181e-01,  2.5561e-01, -3.3206e-01],\n        [ 6.4685e-02, -7.2659e-02,  1.6604e-01,  4.5441e-02, -2.6455e-01,\n         -2.3534e-01,  3.1573e-01,  4.6440e-02],\n        [ 2.1067e-02, -1.4312e-01,  3.5088e-01,  2.9193e-01, -1.1029e-01,\n         -1.5230e-01, -6.8073e-02, -5.6001e-02],\n        [ 3.7908e-02, -1.5527e-01, -1.6537e-01,  1.4684e-01,  1.1855e-01,\n          5.1190e-02, -1.9740e-01,  3.5208e-01],\n        [-3.3271e-01, -1.0517e-01, -3.0792e-01, -1.1823e-01,  3.8822e-02,\n          1.9965e-01,  2.4444e-02,  9.1596e-03],\n        [ 1.6546e-01,  3.3711e-02,  2.2571e-01,  2.5713e-01,  1.4741e-01,\n          3.3366e-01, -2.2138e-01,  1.8061e-01],\n        [ 2.6740e-01,  1.4232e-02,  3.1999e-01, -2.9254e-01, -5.6907e-02,\n         -1.5437e-01,  2.3097e-01, -1.7708e-02],\n        [ 1.0794e-01,  3.4108e-01,  4.3912e-03, -2.6832e-01,  2.8362e-01,\n          1.7980e-01,  1.2606e-01, -3.7640e-02],\n        [ 3.1967e-01,  2.1871e-01, -1.6560e-01, -1.5258e-01,  1.5834e-01,\n          3.0516e-02,  3.7642e-02, -3.5347e-01],\n        [-3.1562e-01, -2.8459e-01, -1.5309e-03,  1.6388e-01,  3.3396e-01,\n         -7.5530e-02,  1.0627e-01, -2.6842e-01],\n        [-1.7690e-03, -2.5224e-01, -2.2485e-01, -2.5359e-01,  2.1882e-01,\n          3.0849e-01, -1.1099e-01,  1.5357e-01],\n        [ 1.6939e-01,  3.2140e-01, -1.3511e-01,  1.0424e-01, -1.8480e-01,\n          1.5777e-01, -1.1803e-02, -1.3192e-01],\n        [-1.6806e-01,  2.7535e-02, -3.0537e-01, -2.4086e-01,  1.8407e-01,\n         -2.0604e-01,  2.6289e-01, -1.1235e-01],\n        [-1.5354e-01, -1.7874e-01,  3.5165e-02,  3.1525e-01, -3.2249e-01,\n         -1.1689e-01, -2.1943e-01, -1.3357e-01],\n        [-1.9141e-01,  1.5565e-01,  2.7117e-01,  3.6696e-02,  1.2706e-01,\n         -3.1369e-01,  2.3693e-01, -3.0167e-01],\n        [-2.1108e-01, -2.1069e-01, -3.5691e-02,  3.1538e-01, -3.1490e-01,\n         -9.6023e-02,  1.4950e-01,  3.7883e-02],\n        [-1.2162e-01,  5.8197e-02,  1.7346e-01,  3.4748e-01,  3.2137e-04,\n         -7.9469e-02, -1.0383e-01, -6.1253e-02],\n        [-2.8191e-01, -5.8923e-02,  2.8509e-02, -3.3419e-01,  1.4046e-01,\n         -2.4765e-01,  1.2631e-01, -1.1416e-01],\n        [ 2.2142e-01, -3.4260e-01,  8.9769e-02,  9.8026e-03, -1.1831e-02,\n          2.9902e-01,  1.1576e-01,  3.0124e-01],\n        [ 1.2057e-01,  2.0518e-01, -3.3562e-01,  9.1711e-02,  1.9580e-01,\n          3.2825e-02, -7.3790e-02,  3.0942e-02],\n        [-2.2420e-01, -2.2781e-01, -3.3520e-02, -1.4758e-01, -1.9543e-01,\n         -1.2963e-01, -4.4438e-02, -7.2638e-02],\n        [ 1.2773e-01, -1.9965e-01,  1.8127e-01,  2.7369e-01, -3.1053e-01,\n          2.1033e-01, -1.0747e-01,  2.2119e-02],\n        [ 1.6348e-01,  2.7254e-01,  1.5772e-01, -2.1073e-03, -1.3818e-01,\n          2.0972e-01, -2.5590e-01, -2.3592e-01],\n        [ 3.5347e-01,  2.6746e-01, -4.4575e-02, -1.5229e-01,  3.2690e-02,\n         -1.8402e-01, -2.6329e-01,  2.4827e-01],\n        [ 5.9745e-02, -3.0530e-02,  3.1931e-01, -2.4419e-01,  1.7674e-01,\n          3.5007e-01,  1.3304e-01,  3.3420e-01],\n        [-2.5321e-01,  2.2049e-01, -2.2227e-01,  2.3157e-01, -2.4278e-02,\n         -3.4960e-02,  2.0345e-01, -1.9791e-01],\n        [-3.2827e-01, -2.0040e-01, -2.0618e-04,  1.9930e-01, -3.1272e-01,\n          1.7143e-01, -3.5975e-02, -2.2311e-01],\n        [-1.5075e-01,  1.3559e-01,  3.2461e-01,  2.6862e-01, -3.3325e-01,\n          4.1204e-02, -3.0543e-01,  2.7460e-01],\n        [-4.0536e-03, -3.4372e-02,  2.5922e-01, -4.0896e-02,  1.3669e-03,\n          8.5784e-02, -2.4732e-01,  6.8486e-02],\n        [-2.0530e-01, -3.4931e-01,  2.0387e-01, -2.2170e-01,  2.0338e-01,\n         -2.7510e-01, -2.4940e-01,  1.1742e-01],\n        [-9.4980e-02,  1.8782e-01, -1.6181e-01, -1.5314e-01, -7.1072e-03,\n          2.4887e-01,  1.9739e-01, -2.0675e-01],\n        [-1.1418e-01, -2.0766e-01, -3.3514e-01,  3.7418e-02,  1.0773e-01,\n          2.4198e-01,  7.2478e-02, -1.3706e-01],\n        [-2.7435e-01, -1.5455e-01,  8.9023e-02, -1.7309e-01,  1.8425e-01,\n         -8.7541e-02, -1.6240e-01,  2.8427e-01],\n        [-8.5467e-02,  2.4396e-01, -4.1071e-02,  1.0652e-01,  1.0700e-01,\n         -1.3494e-01, -4.1027e-02,  1.7823e-01],\n        [-2.7986e-01, -2.3683e-02, -2.6537e-01, -1.2732e-01,  1.7631e-01,\n          1.6680e-01,  3.1816e-01, -8.9218e-02],\n        [-1.7054e-01, -3.4914e-01,  3.4843e-01,  3.4487e-01, -1.3209e-01,\n         -7.2408e-02, -3.1479e-01,  3.4954e-01],\n        [ 1.9501e-02,  1.5451e-02, -1.8048e-01, -7.0031e-02,  2.9877e-01,\n          6.3828e-02,  6.1491e-03,  5.3356e-02],\n        [ 5.5373e-02,  2.3689e-01,  3.9461e-02, -2.0945e-01, -1.2743e-01,\n          2.3950e-01,  3.0908e-01, -3.5178e-01],\n        [-3.1298e-01, -3.2798e-02, -1.0810e-01, -8.0712e-02, -3.4647e-01,\n          1.4301e-01,  3.3178e-01,  7.1103e-02],\n        [ 3.0970e-02,  2.7236e-01,  2.0120e-01,  1.0558e-01, -1.1977e-01,\n         -1.3241e-01, -6.5320e-02, -1.4339e-01],\n        [-3.1808e-01,  3.3692e-01,  1.8080e-01,  1.4360e-01, -3.0815e-01,\n          2.3528e-01,  2.3100e-01, -7.0868e-02],\n        [-2.6458e-01,  3.4464e-01, -2.9880e-01,  8.4121e-02, -2.7989e-01,\n          3.1478e-01, -2.2226e-01, -7.0509e-02],\n        [-9.3366e-02,  2.2748e-01, -1.9686e-01, -2.6484e-01, -1.9613e-01,\n          4.2614e-02, -4.1827e-02, -1.2885e-01],\n        [-7.0684e-02, -1.9427e-01, -1.0835e-01, -4.3332e-03, -2.2312e-01,\n          2.0998e-01, -3.4862e-02,  1.8536e-01],\n        [ 2.3414e-01, -3.0382e-01,  1.4218e-01,  5.0442e-02, -2.4038e-02,\n         -6.1415e-02,  2.7367e-01, -1.9841e-01],\n        [ 1.5479e-01, -2.3351e-01,  5.3166e-02,  9.3925e-02, -5.2788e-02,\n         -2.3860e-01, -3.5228e-01,  1.9050e-01],\n        [ 3.1541e-01,  2.5426e-01, -1.3145e-01,  3.5228e-01,  2.5785e-01,\n          1.0438e-01, -3.1788e-01, -2.8974e-01],\n        [ 1.8730e-01,  2.3957e-01, -2.0769e-01,  3.3500e-01,  6.5880e-02,\n          8.0244e-02,  2.5361e-01, -3.0189e-01],\n        [-2.1888e-01, -3.2565e-01, -1.2990e-01, -1.3715e-01,  1.4201e-01,\n         -1.0913e-01,  2.8911e-01, -3.8782e-02],\n        [-9.6755e-02,  8.7821e-02,  2.5759e-02,  3.4378e-01,  2.2967e-01,\n         -2.5764e-01,  3.4478e-01,  1.5716e-01],\n        [-1.2460e-01,  1.8678e-01, -2.5812e-01,  1.4501e-01,  5.0575e-02,\n          2.8969e-02, -1.8767e-01, -2.1813e-01],\n        [ 2.8460e-01,  3.2698e-01,  1.1490e-01,  5.9911e-02, -3.1886e-01,\n          7.5435e-02, -8.2079e-02,  1.0861e-01],\n        [ 3.3157e-01,  2.2191e-01,  4.8928e-02,  1.5834e-01, -1.4549e-01,\n          2.3289e-01, -2.4554e-01, -1.4028e-01],\n        [-3.2409e-01,  3.3042e-01,  1.6498e-01, -4.5298e-02, -3.2156e-01,\n         -1.1633e-01,  1.5415e-01, -1.1759e-01],\n        [ 1.9539e-01,  2.3210e-01,  2.2263e-01, -2.0625e-01, -2.4210e-01,\n          1.4621e-01,  2.2772e-01, -1.5552e-01],\n        [ 3.1210e-01, -2.9310e-01, -1.9393e-01,  2.2803e-01,  2.3250e-02,\n         -2.2712e-01, -2.5649e-01, -3.2218e-02],\n        [ 1.5494e-01,  5.3380e-02,  2.1551e-01,  2.7522e-01,  4.5909e-02,\n          1.0232e-01, -3.2694e-01,  1.5668e-01],\n        [ 4.2652e-02, -2.5231e-01,  2.6416e-01,  2.4911e-01,  4.7567e-02,\n          2.7137e-01,  5.1763e-02, -3.0143e-01],\n        [ 1.0171e-01, -9.8774e-02, -3.3491e-01,  1.2146e-02,  2.2164e-01,\n          1.4785e-01,  3.4070e-01, -3.2427e-01],\n        [ 9.9798e-02,  1.6978e-01, -3.3085e-01, -2.1088e-01, -1.4902e-02,\n          1.0814e-01, -2.5966e-01,  2.9027e-01],\n        [ 1.9486e-01, -2.0559e-01, -3.0492e-01,  1.0428e-01, -2.8900e-01,\n         -1.9700e-02,  2.9457e-01, -6.0927e-03],\n        [-2.5604e-01,  5.2337e-02, -3.5110e-01, -9.8497e-02, -4.4926e-02,\n         -7.6212e-02, -1.3961e-01,  2.3674e-01],\n        [ 3.1414e-01,  3.4911e-03,  7.1929e-02, -3.3852e-01, -1.7408e-01,\n         -6.9814e-02,  1.1832e-01, -2.3062e-01],\n        [ 1.0276e-01,  2.6364e-01, -7.1565e-02, -2.3374e-02, -9.1750e-02,\n          2.8528e-01, -3.2967e-01,  2.1809e-01],\n        [ 6.9341e-02,  7.7805e-02,  2.8369e-02, -1.1476e-01,  1.4929e-01,\n          2.3138e-01, -7.6657e-04,  2.0008e-01],\n        [-1.8558e-01,  2.7449e-01, -2.2061e-01, -2.7522e-01, -1.6400e-01,\n          6.8450e-02, -3.3674e-01,  2.3542e-01],\n        [-8.9247e-02, -8.0830e-02,  2.5569e-01, -2.4203e-01, -2.5560e-01,\n          6.1101e-02, -1.6382e-01, -1.9433e-01],\n        [-1.4068e-01, -2.2135e-02,  5.6069e-02, -2.5688e-01, -6.6992e-02,\n         -2.9525e-01, -1.1744e-01,  3.5327e-01],\n        [-9.0105e-02,  7.4319e-02,  2.3129e-01, -2.1945e-01,  2.5038e-01,\n         -1.0419e-01, -1.0968e-01,  5.2642e-02],\n        [ 3.3900e-01, -5.4152e-02, -3.5231e-01,  4.5757e-02,  2.5595e-01,\n         -3.3944e-01, -2.7257e-01, -2.6324e-01],\n        [-1.5208e-01, -3.4020e-01,  3.1490e-01, -4.3713e-02,  6.4194e-02,\n          2.5920e-01, -3.4312e-01,  3.1044e-01],\n        [ 1.7882e-01,  5.8606e-02,  1.1621e-02, -1.4691e-02, -2.8749e-01,\n          9.4207e-03,  1.8655e-01,  2.6741e-01]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	100,
                                "training":	true
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=100, out_features=100, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0429,  0.0171, -0.0854, -0.0824, -0.0024,  0.0228,  0.0931, -0.0832,\n        -0.0436,  0.0444, -0.0824, -0.0099,  0.0338,  0.0946, -0.0469,  0.0913,\n        -0.0149,  0.0591, -0.0493,  0.0238, -0.0739,  0.0762,  0.0231, -0.0696,\n         0.0136,  0.0229,  0.0361,  0.0810,  0.0700,  0.0326,  0.0172,  0.0558,\n         0.0837, -0.0658, -0.0267,  0.0101, -0.0871, -0.0969,  0.0356,  0.0875,\n         0.0730,  0.0900, -0.0333,  0.0740, -0.0508,  0.0905, -0.0267, -0.0716,\n        -0.0999, -0.0811,  0.0403,  0.0766,  0.0264,  0.0797,  0.0660,  0.0259,\n        -0.0208,  0.0984, -0.0110, -0.0566,  0.0247, -0.0326, -0.0835, -0.0333,\n         0.0990, -0.0100,  0.0324,  0.0068, -0.0481, -0.0896,  0.0667, -0.0682,\n        -0.0038, -0.0649, -0.0257, -0.0131, -0.0848, -0.0191,  0.0160,  0.0572,\n         0.0692, -0.0247,  0.0271,  0.0101,  0.0372, -0.0996, -0.0449,  0.0462,\n         0.0435, -0.0925, -0.0842, -0.0133, -0.0384, -0.0390,  0.0035,  0.0868,\n        -0.0220, -0.0894,  0.0778,  0.0721], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.0270,  0.0960,  0.0300,  ...,  0.0333, -0.0567, -0.0411],\n        [-0.0099,  0.0612, -0.0199,  ..., -0.0151,  0.0606,  0.0733],\n        [-0.0937,  0.0396, -0.0090,  ..., -0.0721, -0.0170, -0.0487],\n        ...,\n        [ 0.0440,  0.0176, -0.0304,  ...,  0.0528, -0.0049, -0.0865],\n        [ 0.0952,  0.0511, -0.0270,  ..., -0.0106, -0.0888,  0.0325],\n        [ 0.0498,  0.0847,  0.0542,  ..., -0.0906, -0.0392, -0.0440]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	100,
                                "out_features":	100,
                                "training":	true
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=100, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0228,  0.0834, -0.0500,  0.0038], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0497,  0.0946, -0.0603, -0.0080,  0.0847,  0.0702,  0.0558,  0.0014,\n          0.0660, -0.0961,  0.0122,  0.0470,  0.0444,  0.0654, -0.0650, -0.0153,\n          0.0563, -0.0712,  0.0476,  0.0150,  0.0139, -0.0784, -0.0423,  0.0378,\n         -0.0303,  0.0858, -0.0626,  0.0637,  0.0387, -0.0605,  0.0762, -0.0801,\n          0.0596, -0.0122,  0.0226, -0.0660, -0.0675, -0.0827,  0.0399,  0.0487,\n          0.0211,  0.0585,  0.0813,  0.0512,  0.0980, -0.0518, -0.0879, -0.0421,\n          0.0235,  0.0206,  0.0129, -0.0687, -0.0700, -0.0269,  0.0388,  0.0152,\n         -0.0393, -0.0473,  0.0623,  0.0198, -0.0082,  0.0491,  0.0724,  0.0675,\n         -0.0501, -0.0887, -0.0202, -0.0161, -0.0129,  0.0022, -0.0182,  0.0951,\n         -0.0988,  0.0407,  0.0180, -0.0324,  0.0248,  0.0697,  0.0835, -0.0364,\n         -0.0030, -0.0679,  0.0284,  0.0265,  0.0072,  0.0016, -0.0761,  0.0873,\n         -0.0486, -0.0730, -0.0245, -0.0640,  0.0042, -0.0540,  0.0673,  0.0935,\n          0.0631, -0.0922,  0.0090,  0.0703],\n        [-0.0853, -0.0074,  0.0783,  0.0236,  0.0051, -0.0622, -0.0679, -0.0519,\n         -0.0365,  0.0159,  0.0587,  0.0107, -0.0233,  0.0921, -0.0794, -0.0539,\n          0.0187, -0.0545, -0.0212, -0.0080, -0.0858,  0.0703,  0.0621,  0.0835,\n         -0.0809,  0.0155, -0.0004, -0.0530,  0.0593, -0.0922,  0.0267, -0.0902,\n          0.0695, -0.0856, -0.0731,  0.0282, -0.0656,  0.0515,  0.0896, -0.0864,\n         -0.0691,  0.0892, -0.0868, -0.0698,  0.0389, -0.0862, -0.0645,  0.0430,\n         -0.0850,  0.0811,  0.0546,  0.0967,  0.0848,  0.0452, -0.0719, -0.0048,\n         -0.0568, -0.0626,  0.0779,  0.0653, -0.0745, -0.0878,  0.0822,  0.0537,\n          0.0392, -0.0304,  0.0971,  0.0200, -0.0467, -0.0177,  0.0441, -0.0128,\n         -0.0953,  0.0699, -0.0102,  0.0868,  0.0152,  0.0047, -0.0325,  0.0861,\n          0.0877,  0.0802,  0.0997, -0.0984, -0.0004,  0.0476,  0.0788, -0.0346,\n         -0.0994,  0.0179, -0.0304,  0.0624,  0.0995,  0.0191,  0.0002, -0.0853,\n          0.0940,  0.0414, -0.0650,  0.0565],\n        [ 0.0307, -0.0526,  0.0541, -0.0323, -0.0103, -0.0318, -0.0127, -0.0709,\n          0.0562,  0.0999, -0.0518,  0.0289, -0.0639, -0.0551,  0.0370, -0.0785,\n          0.0618,  0.0727, -0.0032, -0.0463,  0.0143,  0.0969,  0.0778, -0.0582,\n          0.0579,  0.0702, -0.0926,  0.0541, -0.0043,  0.0783, -0.0252, -0.0364,\n         -0.0827, -0.0649,  0.0404,  0.0017, -0.0228,  0.0920, -0.0114, -0.0424,\n         -0.0323, -0.0759, -0.0789, -0.0926, -0.0502, -0.0570,  0.0398,  0.0620,\n         -0.0713, -0.0327,  0.0048, -0.0544,  0.0510, -0.0678, -0.0150,  0.0529,\n         -0.0656,  0.0606,  0.0717, -0.0061,  0.0953,  0.0626,  0.0152,  0.0400,\n         -0.0174, -0.0509, -0.0500, -0.0467,  0.0880, -0.0675,  0.0050,  0.0256,\n          0.0441, -0.0448,  0.0670,  0.0288, -0.0916,  0.0598,  0.0770,  0.0397,\n          0.0632, -0.0090,  0.0558, -0.0919,  0.0136,  0.0151, -0.0455,  0.0800,\n          0.0960,  0.0906,  0.0527,  0.0420, -0.0667, -0.0322, -0.0771,  0.0998,\n         -0.0236, -0.0956, -0.0327,  0.0902],\n        [-0.0698, -0.0427, -0.0571,  0.0300, -0.0420, -0.0759,  0.0164, -0.0157,\n         -0.0715,  0.0387,  0.0334,  0.0414,  0.0364,  0.0301, -0.0253,  0.0291,\n          0.0723, -0.0759, -0.0471, -0.0521,  0.0548,  0.0239,  0.0251,  0.0695,\n         -0.0230, -0.0986,  0.0833, -0.0429,  0.0104, -0.0519, -0.0084, -0.0882,\n          0.0106,  0.0861, -0.0156, -0.0883, -0.0097,  0.0558,  0.0249,  0.0936,\n          0.0657, -0.0356,  0.0852,  0.0161, -0.0190, -0.0966, -0.0406, -0.0011,\n          0.0141,  0.0044, -0.0488,  0.0674,  0.0866,  0.0073, -0.0998,  0.0368,\n         -0.0348, -0.0737,  0.0126,  0.0964, -0.0277, -0.0693,  0.0940, -0.0671,\n         -0.0500, -0.0917, -0.0460, -0.0806,  0.0801,  0.0012, -0.0947,  0.0072,\n         -0.0334,  0.0918, -0.0383,  0.0780,  0.0484, -0.0862, -0.0196, -0.0997,\n          0.0305,  0.0043,  0.0160,  0.0176, -0.0548,  0.0295, -0.0282, -0.0421,\n         -0.0501,  0.0326,  0.0336,  0.0095, -0.0087,  0.0218, -0.0675, -0.0751,\n          0.0019,  0.0987,  0.0550, -0.0760]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	100,
                                "out_features":	4,
                                "training":	true
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	true
                }
            },
            "_q_optimizer":	{
                "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.003\n    maximize: False\n    weight_decay: 0\n)":	{
                    "_optimizer_load_state_dict_post_hooks":	{},
                    "_optimizer_load_state_dict_pre_hooks":	{},
                    "_optimizer_state_dict_post_hooks":	{},
                    "_optimizer_state_dict_pre_hooks":	{},
                    "_optimizer_step_post_hooks":	{},
                    "_optimizer_step_pre_hooks":	{},
                    "_warned_capturable_if_run_uncaptured":	true,
                    "_zero_grad_profile_name":	"Optimizer.zero_grad#Adam.zero_grad",
                    "defaults":	{
                        "amsgrad":	false,
                        "betas":	[
                            0.9,
                            0.999
                        ],
                        "capturable":	false,
                        "differentiable":	false,
                        "eps":	1e-08,
                        "foreach":	null,
                        "fused":	null,
                        "lr":	0.003,
                        "maximize":	false,
                        "weight_decay":	0
                    },
                    "param_groups":	[
                        {
                            "amsgrad":	false,
                            "betas":	[
                                0.9,
                                0.999
                            ],
                            "capturable":	false,
                            "differentiable":	false,
                            "eps":	1e-08,
                            "foreach":	null,
                            "fused":	null,
                            "lr":	0.003,
                            "maximize":	false,
                            "params":	[
                                "Parameter containing:\ntensor([[-3.1922e-01,  9.3278e-02,  9.0130e-02,  2.9280e-01,  2.2603e-01,\n          1.3055e-01, -2.1449e-02, -2.0891e-01],\n        [-1.6534e-01,  1.0081e-01,  3.1594e-01, -2.5100e-01,  3.4934e-02,\n          2.7505e-02,  2.9254e-01,  2.7859e-01],\n        [ 3.4758e-01, -7.5131e-02,  3.4708e-01,  3.0609e-01,  3.1915e-01,\n         -1.3338e-01,  4.9414e-02, -2.1607e-01],\n        [ 1.5013e-01, -3.5349e-02,  1.4531e-01,  3.5148e-02, -1.3870e-01,\n         -2.8126e-01,  8.3982e-02,  1.9716e-01],\n        [ 2.2253e-01,  1.7833e-02, -2.9716e-02,  2.5503e-01,  7.7748e-03,\n         -6.0817e-02, -1.2872e-01, -5.1816e-02],\n        [ 1.5541e-01, -1.3660e-01,  7.4146e-02, -5.0384e-02, -1.3891e-01,\n         -1.0892e-01,  1.2612e-01, -3.1118e-01],\n        [-2.4859e-01, -2.6574e-01,  1.9045e-01, -2.6092e-01,  1.3966e-01,\n         -7.9672e-02, -2.0820e-01, -2.7812e-01],\n        [ 3.3852e-01,  2.2146e-01,  1.0356e-01,  6.8931e-02, -5.3033e-02,\n         -6.6212e-02,  8.6013e-02, -3.4141e-02],\n        [-2.3655e-01, -2.6068e-01, -1.3784e-01,  2.0691e-01,  3.1976e-01,\n          2.2000e-01,  2.4764e-01, -5.4643e-02],\n        [-1.2710e-01,  2.6466e-01, -1.1192e-01,  3.0407e-01,  2.8081e-01,\n         -5.7168e-03,  1.1752e-01, -1.8463e-02],\n        [ 3.3612e-01, -2.3264e-01,  2.9211e-01,  2.7187e-01, -1.9306e-01,\n          3.2373e-01, -1.2094e-01,  3.1527e-01],\n        [ 1.8989e-02,  2.3833e-01,  3.4400e-01, -2.9806e-01, -4.8142e-02,\n          3.1572e-01, -1.6642e-03, -1.0383e-01],\n        [ 1.9480e-01,  7.8094e-03,  3.4949e-01,  2.6412e-01, -4.1616e-02,\n         -3.1865e-01, -1.2291e-02, -4.3876e-02],\n        [ 1.2321e-01,  2.2697e-01, -3.0804e-02,  3.0816e-01,  6.1992e-02,\n         -2.4048e-01, -9.0704e-02, -7.1180e-02],\n        [ 1.8862e-01, -2.5234e-01,  2.0715e-01,  1.0592e-02, -8.7255e-02,\n         -2.8739e-01,  2.0671e-01, -1.5088e-01],\n        [-7.6047e-02, -4.5502e-02,  9.9739e-02,  3.0032e-01,  2.0214e-01,\n         -2.8067e-01, -2.7893e-01, -1.9939e-01],\n        [ 1.1785e-01, -2.4699e-01, -2.1821e-01,  1.8299e-02,  1.6944e-01,\n          3.5281e-01,  3.4911e-01,  5.8828e-02],\n        [-1.6374e-01,  3.4982e-01, -2.0381e-01,  2.8965e-01,  3.3085e-01,\n         -2.4311e-01, -3.1576e-01,  2.8505e-01],\n        [ 1.4446e-01,  3.4026e-01, -1.1013e-01, -2.7856e-01,  1.5957e-01,\n          3.2595e-01,  3.5265e-01,  2.3948e-01],\n        [-2.5434e-01, -2.5863e-01, -2.7481e-01, -2.9822e-01,  1.0530e-01,\n          6.8252e-02, -2.8607e-01, -1.9976e-01],\n        [-3.8182e-02, -3.0945e-01,  2.8368e-02,  3.3276e-01, -2.3604e-01,\n         -1.9761e-01, -1.5368e-01, -1.6786e-01],\n        [ 1.5037e-01, -2.0285e-01, -3.0123e-01,  8.4302e-02,  1.8010e-01,\n          2.1215e-01,  1.2484e-01, -1.7013e-01],\n        [-1.7598e-01, -1.0265e-01, -3.0789e-01, -1.2274e-01,  2.5389e-01,\n          2.0449e-01,  3.1798e-01,  2.4840e-01],\n        [ 1.1927e-01,  2.8323e-01, -1.7218e-02,  3.9173e-02, -1.7430e-01,\n          2.5728e-01,  3.1723e-01, -3.2676e-01],\n        [-1.6581e-01,  2.7567e-01,  3.4973e-01,  2.5490e-01,  1.5681e-01,\n          3.4952e-01,  2.2022e-02,  1.4101e-02],\n        [-3.4573e-01,  1.1864e-01, -1.9011e-01, -2.2155e-01,  3.3068e-01,\n         -1.6266e-02, -3.0327e-01, -1.6996e-01],\n        [-3.5072e-01,  9.6342e-02,  2.7218e-01, -2.9991e-01,  3.2447e-01,\n         -2.7956e-01,  3.4911e-01, -1.3909e-01],\n        [ 2.5468e-01,  2.3535e-01,  1.4984e-01, -6.6065e-02, -2.5744e-01,\n         -4.2600e-02,  2.1035e-01,  2.2260e-01],\n        [-7.1128e-02,  6.9909e-02, -3.2276e-01, -2.3864e-01, -7.5238e-02,\n         -2.3181e-01,  2.5561e-01, -3.3206e-01],\n        [ 6.4685e-02, -7.2659e-02,  1.6604e-01,  4.5441e-02, -2.6455e-01,\n         -2.3534e-01,  3.1573e-01,  4.6440e-02],\n        [ 2.1067e-02, -1.4312e-01,  3.5088e-01,  2.9193e-01, -1.1029e-01,\n         -1.5230e-01, -6.8073e-02, -5.6001e-02],\n        [ 3.7908e-02, -1.5527e-01, -1.6537e-01,  1.4684e-01,  1.1855e-01,\n          5.1190e-02, -1.9740e-01,  3.5208e-01],\n        [-3.3271e-01, -1.0517e-01, -3.0792e-01, -1.1823e-01,  3.8822e-02,\n          1.9965e-01,  2.4444e-02,  9.1596e-03],\n        [ 1.6546e-01,  3.3711e-02,  2.2571e-01,  2.5713e-01,  1.4741e-01,\n          3.3366e-01, -2.2138e-01,  1.8061e-01],\n        [ 2.6740e-01,  1.4232e-02,  3.1999e-01, -2.9254e-01, -5.6907e-02,\n         -1.5437e-01,  2.3097e-01, -1.7708e-02],\n        [ 1.0794e-01,  3.4108e-01,  4.3912e-03, -2.6832e-01,  2.8362e-01,\n          1.7980e-01,  1.2606e-01, -3.7640e-02],\n        [ 3.1967e-01,  2.1871e-01, -1.6560e-01, -1.5258e-01,  1.5834e-01,\n          3.0516e-02,  3.7642e-02, -3.5347e-01],\n        [-3.1562e-01, -2.8459e-01, -1.5309e-03,  1.6388e-01,  3.3396e-01,\n         -7.5530e-02,  1.0627e-01, -2.6842e-01],\n        [-1.7690e-03, -2.5224e-01, -2.2485e-01, -2.5359e-01,  2.1882e-01,\n          3.0849e-01, -1.1099e-01,  1.5357e-01],\n        [ 1.6939e-01,  3.2140e-01, -1.3511e-01,  1.0424e-01, -1.8480e-01,\n          1.5777e-01, -1.1803e-02, -1.3192e-01],\n        [-1.6806e-01,  2.7535e-02, -3.0537e-01, -2.4086e-01,  1.8407e-01,\n         -2.0604e-01,  2.6289e-01, -1.1235e-01],\n        [-1.5354e-01, -1.7874e-01,  3.5165e-02,  3.1525e-01, -3.2249e-01,\n         -1.1689e-01, -2.1943e-01, -1.3357e-01],\n        [-1.9141e-01,  1.5565e-01,  2.7117e-01,  3.6696e-02,  1.2706e-01,\n         -3.1369e-01,  2.3693e-01, -3.0167e-01],\n        [-2.1108e-01, -2.1069e-01, -3.5691e-02,  3.1538e-01, -3.1490e-01,\n         -9.6023e-02,  1.4950e-01,  3.7883e-02],\n        [-1.2162e-01,  5.8197e-02,  1.7346e-01,  3.4748e-01,  3.2137e-04,\n         -7.9469e-02, -1.0383e-01, -6.1253e-02],\n        [-2.8191e-01, -5.8923e-02,  2.8509e-02, -3.3419e-01,  1.4046e-01,\n         -2.4765e-01,  1.2631e-01, -1.1416e-01],\n        [ 2.2142e-01, -3.4260e-01,  8.9769e-02,  9.8026e-03, -1.1831e-02,\n          2.9902e-01,  1.1576e-01,  3.0124e-01],\n        [ 1.2057e-01,  2.0518e-01, -3.3562e-01,  9.1711e-02,  1.9580e-01,\n          3.2825e-02, -7.3790e-02,  3.0942e-02],\n        [-2.2420e-01, -2.2781e-01, -3.3520e-02, -1.4758e-01, -1.9543e-01,\n         -1.2963e-01, -4.4438e-02, -7.2638e-02],\n        [ 1.2773e-01, -1.9965e-01,  1.8127e-01,  2.7369e-01, -3.1053e-01,\n          2.1033e-01, -1.0747e-01,  2.2119e-02],\n        [ 1.6348e-01,  2.7254e-01,  1.5772e-01, -2.1073e-03, -1.3818e-01,\n          2.0972e-01, -2.5590e-01, -2.3592e-01],\n        [ 3.5347e-01,  2.6746e-01, -4.4575e-02, -1.5229e-01,  3.2690e-02,\n         -1.8402e-01, -2.6329e-01,  2.4827e-01],\n        [ 5.9745e-02, -3.0530e-02,  3.1931e-01, -2.4419e-01,  1.7674e-01,\n          3.5007e-01,  1.3304e-01,  3.3420e-01],\n        [-2.5321e-01,  2.2049e-01, -2.2227e-01,  2.3157e-01, -2.4278e-02,\n         -3.4960e-02,  2.0345e-01, -1.9791e-01],\n        [-3.2827e-01, -2.0040e-01, -2.0618e-04,  1.9930e-01, -3.1272e-01,\n          1.7143e-01, -3.5975e-02, -2.2311e-01],\n        [-1.5075e-01,  1.3559e-01,  3.2461e-01,  2.6862e-01, -3.3325e-01,\n          4.1204e-02, -3.0543e-01,  2.7460e-01],\n        [-4.0536e-03, -3.4372e-02,  2.5922e-01, -4.0896e-02,  1.3669e-03,\n          8.5784e-02, -2.4732e-01,  6.8486e-02],\n        [-2.0530e-01, -3.4931e-01,  2.0387e-01, -2.2170e-01,  2.0338e-01,\n         -2.7510e-01, -2.4940e-01,  1.1742e-01],\n        [-9.4980e-02,  1.8782e-01, -1.6181e-01, -1.5314e-01, -7.1072e-03,\n          2.4887e-01,  1.9739e-01, -2.0675e-01],\n        [-1.1418e-01, -2.0766e-01, -3.3514e-01,  3.7418e-02,  1.0773e-01,\n          2.4198e-01,  7.2478e-02, -1.3706e-01],\n        [-2.7435e-01, -1.5455e-01,  8.9023e-02, -1.7309e-01,  1.8425e-01,\n         -8.7541e-02, -1.6240e-01,  2.8427e-01],\n        [-8.5467e-02,  2.4396e-01, -4.1071e-02,  1.0652e-01,  1.0700e-01,\n         -1.3494e-01, -4.1027e-02,  1.7823e-01],\n        [-2.7986e-01, -2.3683e-02, -2.6537e-01, -1.2732e-01,  1.7631e-01,\n          1.6680e-01,  3.1816e-01, -8.9218e-02],\n        [-1.7054e-01, -3.4914e-01,  3.4843e-01,  3.4487e-01, -1.3209e-01,\n         -7.2408e-02, -3.1479e-01,  3.4954e-01],\n        [ 1.9501e-02,  1.5451e-02, -1.8048e-01, -7.0031e-02,  2.9877e-01,\n          6.3828e-02,  6.1491e-03,  5.3356e-02],\n        [ 5.5373e-02,  2.3689e-01,  3.9461e-02, -2.0945e-01, -1.2743e-01,\n          2.3950e-01,  3.0908e-01, -3.5178e-01],\n        [-3.1298e-01, -3.2798e-02, -1.0810e-01, -8.0712e-02, -3.4647e-01,\n          1.4301e-01,  3.3178e-01,  7.1103e-02],\n        [ 3.0970e-02,  2.7236e-01,  2.0120e-01,  1.0558e-01, -1.1977e-01,\n         -1.3241e-01, -6.5320e-02, -1.4339e-01],\n        [-3.1808e-01,  3.3692e-01,  1.8080e-01,  1.4360e-01, -3.0815e-01,\n          2.3528e-01,  2.3100e-01, -7.0868e-02],\n        [-2.6458e-01,  3.4464e-01, -2.9880e-01,  8.4121e-02, -2.7989e-01,\n          3.1478e-01, -2.2226e-01, -7.0509e-02],\n        [-9.3366e-02,  2.2748e-01, -1.9686e-01, -2.6484e-01, -1.9613e-01,\n          4.2614e-02, -4.1827e-02, -1.2885e-01],\n        [-7.0684e-02, -1.9427e-01, -1.0835e-01, -4.3332e-03, -2.2312e-01,\n          2.0998e-01, -3.4862e-02,  1.8536e-01],\n        [ 2.3414e-01, -3.0382e-01,  1.4218e-01,  5.0442e-02, -2.4038e-02,\n         -6.1415e-02,  2.7367e-01, -1.9841e-01],\n        [ 1.5479e-01, -2.3351e-01,  5.3166e-02,  9.3925e-02, -5.2788e-02,\n         -2.3860e-01, -3.5228e-01,  1.9050e-01],\n        [ 3.1541e-01,  2.5426e-01, -1.3145e-01,  3.5228e-01,  2.5785e-01,\n          1.0438e-01, -3.1788e-01, -2.8974e-01],\n        [ 1.8730e-01,  2.3957e-01, -2.0769e-01,  3.3500e-01,  6.5880e-02,\n          8.0244e-02,  2.5361e-01, -3.0189e-01],\n        [-2.1888e-01, -3.2565e-01, -1.2990e-01, -1.3715e-01,  1.4201e-01,\n         -1.0913e-01,  2.8911e-01, -3.8782e-02],\n        [-9.6755e-02,  8.7821e-02,  2.5759e-02,  3.4378e-01,  2.2967e-01,\n         -2.5764e-01,  3.4478e-01,  1.5716e-01],\n        [-1.2460e-01,  1.8678e-01, -2.5812e-01,  1.4501e-01,  5.0575e-02,\n          2.8969e-02, -1.8767e-01, -2.1813e-01],\n        [ 2.8460e-01,  3.2698e-01,  1.1490e-01,  5.9911e-02, -3.1886e-01,\n          7.5435e-02, -8.2079e-02,  1.0861e-01],\n        [ 3.3157e-01,  2.2191e-01,  4.8928e-02,  1.5834e-01, -1.4549e-01,\n          2.3289e-01, -2.4554e-01, -1.4028e-01],\n        [-3.2409e-01,  3.3042e-01,  1.6498e-01, -4.5298e-02, -3.2156e-01,\n         -1.1633e-01,  1.5415e-01, -1.1759e-01],\n        [ 1.9539e-01,  2.3210e-01,  2.2263e-01, -2.0625e-01, -2.4210e-01,\n          1.4621e-01,  2.2772e-01, -1.5552e-01],\n        [ 3.1210e-01, -2.9310e-01, -1.9393e-01,  2.2803e-01,  2.3250e-02,\n         -2.2712e-01, -2.5649e-01, -3.2218e-02],\n        [ 1.5494e-01,  5.3380e-02,  2.1551e-01,  2.7522e-01,  4.5909e-02,\n          1.0232e-01, -3.2694e-01,  1.5668e-01],\n        [ 4.2652e-02, -2.5231e-01,  2.6416e-01,  2.4911e-01,  4.7567e-02,\n          2.7137e-01,  5.1763e-02, -3.0143e-01],\n        [ 1.0171e-01, -9.8774e-02, -3.3491e-01,  1.2146e-02,  2.2164e-01,\n          1.4785e-01,  3.4070e-01, -3.2427e-01],\n        [ 9.9798e-02,  1.6978e-01, -3.3085e-01, -2.1088e-01, -1.4902e-02,\n          1.0814e-01, -2.5966e-01,  2.9027e-01],\n        [ 1.9486e-01, -2.0559e-01, -3.0492e-01,  1.0428e-01, -2.8900e-01,\n         -1.9700e-02,  2.9457e-01, -6.0927e-03],\n        [-2.5604e-01,  5.2337e-02, -3.5110e-01, -9.8497e-02, -4.4926e-02,\n         -7.6212e-02, -1.3961e-01,  2.3674e-01],\n        [ 3.1414e-01,  3.4911e-03,  7.1929e-02, -3.3852e-01, -1.7408e-01,\n         -6.9814e-02,  1.1832e-01, -2.3062e-01],\n        [ 1.0276e-01,  2.6364e-01, -7.1565e-02, -2.3374e-02, -9.1750e-02,\n          2.8528e-01, -3.2967e-01,  2.1809e-01],\n        [ 6.9341e-02,  7.7805e-02,  2.8369e-02, -1.1476e-01,  1.4929e-01,\n          2.3138e-01, -7.6657e-04,  2.0008e-01],\n        [-1.8558e-01,  2.7449e-01, -2.2061e-01, -2.7522e-01, -1.6400e-01,\n          6.8450e-02, -3.3674e-01,  2.3542e-01],\n        [-8.9247e-02, -8.0830e-02,  2.5569e-01, -2.4203e-01, -2.5560e-01,\n          6.1101e-02, -1.6382e-01, -1.9433e-01],\n        [-1.4068e-01, -2.2135e-02,  5.6069e-02, -2.5688e-01, -6.6992e-02,\n         -2.9525e-01, -1.1744e-01,  3.5327e-01],\n        [-9.0105e-02,  7.4319e-02,  2.3129e-01, -2.1945e-01,  2.5038e-01,\n         -1.0419e-01, -1.0968e-01,  5.2642e-02],\n        [ 3.3900e-01, -5.4152e-02, -3.5231e-01,  4.5757e-02,  2.5595e-01,\n         -3.3944e-01, -2.7257e-01, -2.6324e-01],\n        [-1.5208e-01, -3.4020e-01,  3.1490e-01, -4.3713e-02,  6.4194e-02,\n          2.5920e-01, -3.4312e-01,  3.1044e-01],\n        [ 1.7882e-01,  5.8606e-02,  1.1621e-02, -1.4691e-02, -2.8749e-01,\n          9.4207e-03,  1.8655e-01,  2.6741e-01]], requires_grad=True)",
                                "Parameter containing:\ntensor([-0.2776,  0.0530, -0.0413, -0.2068,  0.0085, -0.0816, -0.1873,  0.2525,\n        -0.3176,  0.2332, -0.0310, -0.0602,  0.1095, -0.3188,  0.0679, -0.1124,\n        -0.0107,  0.3527, -0.1240, -0.0695,  0.2589,  0.0595, -0.1630, -0.1309,\n         0.1576, -0.2995,  0.0826, -0.3050, -0.0412,  0.2724, -0.0108, -0.0444,\n         0.1360, -0.1247, -0.0115, -0.1502, -0.2397,  0.2200,  0.1022,  0.0082,\n         0.0324,  0.1130, -0.2807, -0.0391,  0.2549,  0.2725, -0.1733, -0.3262,\n        -0.2927, -0.0028,  0.2267, -0.2232, -0.0025, -0.1832, -0.1855,  0.2648,\n        -0.0535,  0.1467,  0.0122, -0.2811, -0.1049, -0.0279, -0.1289,  0.1941,\n        -0.1939,  0.0760, -0.1299,  0.2089, -0.3291, -0.1505,  0.1747,  0.0879,\n         0.3355, -0.1145,  0.1307, -0.3032,  0.1830,  0.0221, -0.2908, -0.0398,\n        -0.0411, -0.2375,  0.2473, -0.0925, -0.0620,  0.2743, -0.2277,  0.1279,\n        -0.1590,  0.2656, -0.0533, -0.3051,  0.3171, -0.1015,  0.1768, -0.0439,\n        -0.0114, -0.3054, -0.1438,  0.2123], requires_grad=True)",
                                "Parameter containing:\ntensor([[-0.0270,  0.0960,  0.0300,  ...,  0.0333, -0.0567, -0.0411],\n        [-0.0099,  0.0612, -0.0199,  ..., -0.0151,  0.0606,  0.0733],\n        [-0.0937,  0.0396, -0.0090,  ..., -0.0721, -0.0170, -0.0487],\n        ...,\n        [ 0.0440,  0.0176, -0.0304,  ...,  0.0528, -0.0049, -0.0865],\n        [ 0.0952,  0.0511, -0.0270,  ..., -0.0106, -0.0888,  0.0325],\n        [ 0.0498,  0.0847,  0.0542,  ..., -0.0906, -0.0392, -0.0440]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.0429,  0.0171, -0.0854, -0.0824, -0.0024,  0.0228,  0.0931, -0.0832,\n        -0.0436,  0.0444, -0.0824, -0.0099,  0.0338,  0.0946, -0.0469,  0.0913,\n        -0.0149,  0.0591, -0.0493,  0.0238, -0.0739,  0.0762,  0.0231, -0.0696,\n         0.0136,  0.0229,  0.0361,  0.0810,  0.0700,  0.0326,  0.0172,  0.0558,\n         0.0837, -0.0658, -0.0267,  0.0101, -0.0871, -0.0969,  0.0356,  0.0875,\n         0.0730,  0.0900, -0.0333,  0.0740, -0.0508,  0.0905, -0.0267, -0.0716,\n        -0.0999, -0.0811,  0.0403,  0.0766,  0.0264,  0.0797,  0.0660,  0.0259,\n        -0.0208,  0.0984, -0.0110, -0.0566,  0.0247, -0.0326, -0.0835, -0.0333,\n         0.0990, -0.0100,  0.0324,  0.0068, -0.0481, -0.0896,  0.0667, -0.0682,\n        -0.0038, -0.0649, -0.0257, -0.0131, -0.0848, -0.0191,  0.0160,  0.0572,\n         0.0692, -0.0247,  0.0271,  0.0101,  0.0372, -0.0996, -0.0449,  0.0462,\n         0.0435, -0.0925, -0.0842, -0.0133, -0.0384, -0.0390,  0.0035,  0.0868,\n        -0.0220, -0.0894,  0.0778,  0.0721], requires_grad=True)",
                                "Parameter containing:\ntensor([[ 0.0497,  0.0946, -0.0603, -0.0080,  0.0847,  0.0702,  0.0558,  0.0014,\n          0.0660, -0.0961,  0.0122,  0.0470,  0.0444,  0.0654, -0.0650, -0.0153,\n          0.0563, -0.0712,  0.0476,  0.0150,  0.0139, -0.0784, -0.0423,  0.0378,\n         -0.0303,  0.0858, -0.0626,  0.0637,  0.0387, -0.0605,  0.0762, -0.0801,\n          0.0596, -0.0122,  0.0226, -0.0660, -0.0675, -0.0827,  0.0399,  0.0487,\n          0.0211,  0.0585,  0.0813,  0.0512,  0.0980, -0.0518, -0.0879, -0.0421,\n          0.0235,  0.0206,  0.0129, -0.0687, -0.0700, -0.0269,  0.0388,  0.0152,\n         -0.0393, -0.0473,  0.0623,  0.0198, -0.0082,  0.0491,  0.0724,  0.0675,\n         -0.0501, -0.0887, -0.0202, -0.0161, -0.0129,  0.0022, -0.0182,  0.0951,\n         -0.0988,  0.0407,  0.0180, -0.0324,  0.0248,  0.0697,  0.0835, -0.0364,\n         -0.0030, -0.0679,  0.0284,  0.0265,  0.0072,  0.0016, -0.0761,  0.0873,\n         -0.0486, -0.0730, -0.0245, -0.0640,  0.0042, -0.0540,  0.0673,  0.0935,\n          0.0631, -0.0922,  0.0090,  0.0703],\n        [-0.0853, -0.0074,  0.0783,  0.0236,  0.0051, -0.0622, -0.0679, -0.0519,\n         -0.0365,  0.0159,  0.0587,  0.0107, -0.0233,  0.0921, -0.0794, -0.0539,\n          0.0187, -0.0545, -0.0212, -0.0080, -0.0858,  0.0703,  0.0621,  0.0835,\n         -0.0809,  0.0155, -0.0004, -0.0530,  0.0593, -0.0922,  0.0267, -0.0902,\n          0.0695, -0.0856, -0.0731,  0.0282, -0.0656,  0.0515,  0.0896, -0.0864,\n         -0.0691,  0.0892, -0.0868, -0.0698,  0.0389, -0.0862, -0.0645,  0.0430,\n         -0.0850,  0.0811,  0.0546,  0.0967,  0.0848,  0.0452, -0.0719, -0.0048,\n         -0.0568, -0.0626,  0.0779,  0.0653, -0.0745, -0.0878,  0.0822,  0.0537,\n          0.0392, -0.0304,  0.0971,  0.0200, -0.0467, -0.0177,  0.0441, -0.0128,\n         -0.0953,  0.0699, -0.0102,  0.0868,  0.0152,  0.0047, -0.0325,  0.0861,\n          0.0877,  0.0802,  0.0997, -0.0984, -0.0004,  0.0476,  0.0788, -0.0346,\n         -0.0994,  0.0179, -0.0304,  0.0624,  0.0995,  0.0191,  0.0002, -0.0853,\n          0.0940,  0.0414, -0.0650,  0.0565],\n        [ 0.0307, -0.0526,  0.0541, -0.0323, -0.0103, -0.0318, -0.0127, -0.0709,\n          0.0562,  0.0999, -0.0518,  0.0289, -0.0639, -0.0551,  0.0370, -0.0785,\n          0.0618,  0.0727, -0.0032, -0.0463,  0.0143,  0.0969,  0.0778, -0.0582,\n          0.0579,  0.0702, -0.0926,  0.0541, -0.0043,  0.0783, -0.0252, -0.0364,\n         -0.0827, -0.0649,  0.0404,  0.0017, -0.0228,  0.0920, -0.0114, -0.0424,\n         -0.0323, -0.0759, -0.0789, -0.0926, -0.0502, -0.0570,  0.0398,  0.0620,\n         -0.0713, -0.0327,  0.0048, -0.0544,  0.0510, -0.0678, -0.0150,  0.0529,\n         -0.0656,  0.0606,  0.0717, -0.0061,  0.0953,  0.0626,  0.0152,  0.0400,\n         -0.0174, -0.0509, -0.0500, -0.0467,  0.0880, -0.0675,  0.0050,  0.0256,\n          0.0441, -0.0448,  0.0670,  0.0288, -0.0916,  0.0598,  0.0770,  0.0397,\n          0.0632, -0.0090,  0.0558, -0.0919,  0.0136,  0.0151, -0.0455,  0.0800,\n          0.0960,  0.0906,  0.0527,  0.0420, -0.0667, -0.0322, -0.0771,  0.0998,\n         -0.0236, -0.0956, -0.0327,  0.0902],\n        [-0.0698, -0.0427, -0.0571,  0.0300, -0.0420, -0.0759,  0.0164, -0.0157,\n         -0.0715,  0.0387,  0.0334,  0.0414,  0.0364,  0.0301, -0.0253,  0.0291,\n          0.0723, -0.0759, -0.0471, -0.0521,  0.0548,  0.0239,  0.0251,  0.0695,\n         -0.0230, -0.0986,  0.0833, -0.0429,  0.0104, -0.0519, -0.0084, -0.0882,\n          0.0106,  0.0861, -0.0156, -0.0883, -0.0097,  0.0558,  0.0249,  0.0936,\n          0.0657, -0.0356,  0.0852,  0.0161, -0.0190, -0.0966, -0.0406, -0.0011,\n          0.0141,  0.0044, -0.0488,  0.0674,  0.0866,  0.0073, -0.0998,  0.0368,\n         -0.0348, -0.0737,  0.0126,  0.0964, -0.0277, -0.0693,  0.0940, -0.0671,\n         -0.0500, -0.0917, -0.0460, -0.0806,  0.0801,  0.0012, -0.0947,  0.0072,\n         -0.0334,  0.0918, -0.0383,  0.0780,  0.0484, -0.0862, -0.0196, -0.0997,\n          0.0305,  0.0043,  0.0160,  0.0176, -0.0548,  0.0295, -0.0282, -0.0421,\n         -0.0501,  0.0326,  0.0336,  0.0095, -0.0087,  0.0218, -0.0675, -0.0751,\n          0.0019,  0.0987,  0.0550, -0.0760]], requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.0228,  0.0834, -0.0500,  0.0038], requires_grad=True)"
                            ],
                            "weight_decay":	0
                        }
                    ],
                    "state":	{}
                }
            },
            "_replay_buffer":	{
                "<algorithms.DQN.replay_buffer.ReplayBuffer object at 0x7164736a9290>":	{
                    "act_buf":	"[0 0 0 ... 0 0 0]",
                    "capacity":	50000,
                    "done_buf":	"[False False False ... False False False]",
                    "epsilon":	1.0,
                    "gamma":	0.99,
                    "mask_buf":	"[[0. 0.]\n [0. 0.]\n [0. 0.]\n ...\n [0. 0.]\n [0. 0.]\n [0. 0.]]",
                    "max_size":	50000,
                    "next_obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "path_start_idx":	0,
                    "ptr":	0,
                    "ret_buf":	"[0. 0. 0. ... 0. 0. 0.]",
                    "rew_buf":	"[0. 0. 0. ... 0. 0. 0.]"
                }
            },
            "_target_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=100, bias=True)\n  (fc3): Linear(in_features=100, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.005,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=100, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.2776,  0.0530, -0.0413, -0.2068,  0.0085, -0.0816, -0.1873,  0.2525,\n        -0.3176,  0.2332, -0.0310, -0.0602,  0.1095, -0.3188,  0.0679, -0.1124,\n        -0.0107,  0.3527, -0.1240, -0.0695,  0.2589,  0.0595, -0.1630, -0.1309,\n         0.1576, -0.2995,  0.0826, -0.3050, -0.0412,  0.2724, -0.0108, -0.0444,\n         0.1360, -0.1247, -0.0115, -0.1502, -0.2397,  0.2200,  0.1022,  0.0082,\n         0.0324,  0.1130, -0.2807, -0.0391,  0.2549,  0.2725, -0.1733, -0.3262,\n        -0.2927, -0.0028,  0.2267, -0.2232, -0.0025, -0.1832, -0.1855,  0.2648,\n        -0.0535,  0.1467,  0.0122, -0.2811, -0.1049, -0.0279, -0.1289,  0.1941,\n        -0.1939,  0.0760, -0.1299,  0.2089, -0.3291, -0.1505,  0.1747,  0.0879,\n         0.3355, -0.1145,  0.1307, -0.3032,  0.1830,  0.0221, -0.2908, -0.0398,\n        -0.0411, -0.2375,  0.2473, -0.0925, -0.0620,  0.2743, -0.2277,  0.1279,\n        -0.1590,  0.2656, -0.0533, -0.3051,  0.3171, -0.1015,  0.1768, -0.0439,\n        -0.0114, -0.3054, -0.1438,  0.2123], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-3.1922e-01,  9.3278e-02,  9.0130e-02,  2.9280e-01,  2.2603e-01,\n          1.3055e-01, -2.1449e-02, -2.0891e-01],\n        [-1.6534e-01,  1.0081e-01,  3.1594e-01, -2.5100e-01,  3.4934e-02,\n          2.7505e-02,  2.9254e-01,  2.7859e-01],\n        [ 3.4758e-01, -7.5131e-02,  3.4708e-01,  3.0609e-01,  3.1915e-01,\n         -1.3338e-01,  4.9414e-02, -2.1607e-01],\n        [ 1.5013e-01, -3.5349e-02,  1.4531e-01,  3.5148e-02, -1.3870e-01,\n         -2.8126e-01,  8.3982e-02,  1.9716e-01],\n        [ 2.2253e-01,  1.7833e-02, -2.9716e-02,  2.5503e-01,  7.7748e-03,\n         -6.0817e-02, -1.2872e-01, -5.1816e-02],\n        [ 1.5541e-01, -1.3660e-01,  7.4146e-02, -5.0384e-02, -1.3891e-01,\n         -1.0892e-01,  1.2612e-01, -3.1118e-01],\n        [-2.4859e-01, -2.6574e-01,  1.9045e-01, -2.6092e-01,  1.3966e-01,\n         -7.9672e-02, -2.0820e-01, -2.7812e-01],\n        [ 3.3852e-01,  2.2146e-01,  1.0356e-01,  6.8931e-02, -5.3033e-02,\n         -6.6212e-02,  8.6013e-02, -3.4141e-02],\n        [-2.3655e-01, -2.6068e-01, -1.3784e-01,  2.0691e-01,  3.1976e-01,\n          2.2000e-01,  2.4764e-01, -5.4643e-02],\n        [-1.2710e-01,  2.6466e-01, -1.1192e-01,  3.0407e-01,  2.8081e-01,\n         -5.7168e-03,  1.1752e-01, -1.8463e-02],\n        [ 3.3612e-01, -2.3264e-01,  2.9211e-01,  2.7187e-01, -1.9306e-01,\n          3.2373e-01, -1.2094e-01,  3.1527e-01],\n        [ 1.8989e-02,  2.3833e-01,  3.4400e-01, -2.9806e-01, -4.8142e-02,\n          3.1572e-01, -1.6642e-03, -1.0383e-01],\n        [ 1.9480e-01,  7.8094e-03,  3.4949e-01,  2.6412e-01, -4.1616e-02,\n         -3.1865e-01, -1.2291e-02, -4.3876e-02],\n        [ 1.2321e-01,  2.2697e-01, -3.0804e-02,  3.0816e-01,  6.1992e-02,\n         -2.4048e-01, -9.0704e-02, -7.1180e-02],\n        [ 1.8862e-01, -2.5234e-01,  2.0715e-01,  1.0592e-02, -8.7255e-02,\n         -2.8739e-01,  2.0671e-01, -1.5088e-01],\n        [-7.6047e-02, -4.5502e-02,  9.9739e-02,  3.0032e-01,  2.0214e-01,\n         -2.8067e-01, -2.7893e-01, -1.9939e-01],\n        [ 1.1785e-01, -2.4699e-01, -2.1821e-01,  1.8299e-02,  1.6944e-01,\n          3.5281e-01,  3.4911e-01,  5.8828e-02],\n        [-1.6374e-01,  3.4982e-01, -2.0381e-01,  2.8965e-01,  3.3085e-01,\n         -2.4311e-01, -3.1576e-01,  2.8505e-01],\n        [ 1.4446e-01,  3.4026e-01, -1.1013e-01, -2.7856e-01,  1.5957e-01,\n          3.2595e-01,  3.5265e-01,  2.3948e-01],\n        [-2.5434e-01, -2.5863e-01, -2.7481e-01, -2.9822e-01,  1.0530e-01,\n          6.8252e-02, -2.8607e-01, -1.9976e-01],\n        [-3.8182e-02, -3.0945e-01,  2.8368e-02,  3.3276e-01, -2.3604e-01,\n         -1.9761e-01, -1.5368e-01, -1.6786e-01],\n        [ 1.5037e-01, -2.0285e-01, -3.0123e-01,  8.4302e-02,  1.8010e-01,\n          2.1215e-01,  1.2484e-01, -1.7013e-01],\n        [-1.7598e-01, -1.0265e-01, -3.0789e-01, -1.2274e-01,  2.5389e-01,\n          2.0449e-01,  3.1798e-01,  2.4840e-01],\n        [ 1.1927e-01,  2.8323e-01, -1.7218e-02,  3.9173e-02, -1.7430e-01,\n          2.5728e-01,  3.1723e-01, -3.2676e-01],\n        [-1.6581e-01,  2.7567e-01,  3.4973e-01,  2.5490e-01,  1.5681e-01,\n          3.4952e-01,  2.2022e-02,  1.4101e-02],\n        [-3.4573e-01,  1.1864e-01, -1.9011e-01, -2.2155e-01,  3.3068e-01,\n         -1.6266e-02, -3.0327e-01, -1.6996e-01],\n        [-3.5072e-01,  9.6342e-02,  2.7218e-01, -2.9991e-01,  3.2447e-01,\n         -2.7956e-01,  3.4911e-01, -1.3909e-01],\n        [ 2.5468e-01,  2.3535e-01,  1.4984e-01, -6.6065e-02, -2.5744e-01,\n         -4.2600e-02,  2.1035e-01,  2.2260e-01],\n        [-7.1128e-02,  6.9909e-02, -3.2276e-01, -2.3864e-01, -7.5238e-02,\n         -2.3181e-01,  2.5561e-01, -3.3206e-01],\n        [ 6.4685e-02, -7.2659e-02,  1.6604e-01,  4.5441e-02, -2.6455e-01,\n         -2.3534e-01,  3.1573e-01,  4.6440e-02],\n        [ 2.1067e-02, -1.4312e-01,  3.5088e-01,  2.9193e-01, -1.1029e-01,\n         -1.5230e-01, -6.8073e-02, -5.6001e-02],\n        [ 3.7908e-02, -1.5527e-01, -1.6537e-01,  1.4684e-01,  1.1855e-01,\n          5.1190e-02, -1.9740e-01,  3.5208e-01],\n        [-3.3271e-01, -1.0517e-01, -3.0792e-01, -1.1823e-01,  3.8822e-02,\n          1.9965e-01,  2.4444e-02,  9.1596e-03],\n        [ 1.6546e-01,  3.3711e-02,  2.2571e-01,  2.5713e-01,  1.4741e-01,\n          3.3366e-01, -2.2138e-01,  1.8061e-01],\n        [ 2.6740e-01,  1.4232e-02,  3.1999e-01, -2.9254e-01, -5.6907e-02,\n         -1.5437e-01,  2.3097e-01, -1.7708e-02],\n        [ 1.0794e-01,  3.4108e-01,  4.3912e-03, -2.6832e-01,  2.8362e-01,\n          1.7980e-01,  1.2606e-01, -3.7640e-02],\n        [ 3.1967e-01,  2.1871e-01, -1.6560e-01, -1.5258e-01,  1.5834e-01,\n          3.0516e-02,  3.7642e-02, -3.5347e-01],\n        [-3.1562e-01, -2.8459e-01, -1.5309e-03,  1.6388e-01,  3.3396e-01,\n         -7.5530e-02,  1.0627e-01, -2.6842e-01],\n        [-1.7690e-03, -2.5224e-01, -2.2485e-01, -2.5359e-01,  2.1882e-01,\n          3.0849e-01, -1.1099e-01,  1.5357e-01],\n        [ 1.6939e-01,  3.2140e-01, -1.3511e-01,  1.0424e-01, -1.8480e-01,\n          1.5777e-01, -1.1803e-02, -1.3192e-01],\n        [-1.6806e-01,  2.7535e-02, -3.0537e-01, -2.4086e-01,  1.8407e-01,\n         -2.0604e-01,  2.6289e-01, -1.1235e-01],\n        [-1.5354e-01, -1.7874e-01,  3.5165e-02,  3.1525e-01, -3.2249e-01,\n         -1.1689e-01, -2.1943e-01, -1.3357e-01],\n        [-1.9141e-01,  1.5565e-01,  2.7117e-01,  3.6696e-02,  1.2706e-01,\n         -3.1369e-01,  2.3693e-01, -3.0167e-01],\n        [-2.1108e-01, -2.1069e-01, -3.5691e-02,  3.1538e-01, -3.1490e-01,\n         -9.6023e-02,  1.4950e-01,  3.7883e-02],\n        [-1.2162e-01,  5.8197e-02,  1.7346e-01,  3.4748e-01,  3.2137e-04,\n         -7.9469e-02, -1.0383e-01, -6.1253e-02],\n        [-2.8191e-01, -5.8923e-02,  2.8509e-02, -3.3419e-01,  1.4046e-01,\n         -2.4765e-01,  1.2631e-01, -1.1416e-01],\n        [ 2.2142e-01, -3.4260e-01,  8.9769e-02,  9.8026e-03, -1.1831e-02,\n          2.9902e-01,  1.1576e-01,  3.0124e-01],\n        [ 1.2057e-01,  2.0518e-01, -3.3562e-01,  9.1711e-02,  1.9580e-01,\n          3.2825e-02, -7.3790e-02,  3.0942e-02],\n        [-2.2420e-01, -2.2781e-01, -3.3520e-02, -1.4758e-01, -1.9543e-01,\n         -1.2963e-01, -4.4438e-02, -7.2638e-02],\n        [ 1.2773e-01, -1.9965e-01,  1.8127e-01,  2.7369e-01, -3.1053e-01,\n          2.1033e-01, -1.0747e-01,  2.2119e-02],\n        [ 1.6348e-01,  2.7254e-01,  1.5772e-01, -2.1073e-03, -1.3818e-01,\n          2.0972e-01, -2.5590e-01, -2.3592e-01],\n        [ 3.5347e-01,  2.6746e-01, -4.4575e-02, -1.5229e-01,  3.2690e-02,\n         -1.8402e-01, -2.6329e-01,  2.4827e-01],\n        [ 5.9745e-02, -3.0530e-02,  3.1931e-01, -2.4419e-01,  1.7674e-01,\n          3.5007e-01,  1.3304e-01,  3.3420e-01],\n        [-2.5321e-01,  2.2049e-01, -2.2227e-01,  2.3157e-01, -2.4278e-02,\n         -3.4960e-02,  2.0345e-01, -1.9791e-01],\n        [-3.2827e-01, -2.0040e-01, -2.0618e-04,  1.9930e-01, -3.1272e-01,\n          1.7143e-01, -3.5975e-02, -2.2311e-01],\n        [-1.5075e-01,  1.3559e-01,  3.2461e-01,  2.6862e-01, -3.3325e-01,\n          4.1204e-02, -3.0543e-01,  2.7460e-01],\n        [-4.0536e-03, -3.4372e-02,  2.5922e-01, -4.0896e-02,  1.3669e-03,\n          8.5784e-02, -2.4732e-01,  6.8486e-02],\n        [-2.0530e-01, -3.4931e-01,  2.0387e-01, -2.2170e-01,  2.0338e-01,\n         -2.7510e-01, -2.4940e-01,  1.1742e-01],\n        [-9.4980e-02,  1.8782e-01, -1.6181e-01, -1.5314e-01, -7.1072e-03,\n          2.4887e-01,  1.9739e-01, -2.0675e-01],\n        [-1.1418e-01, -2.0766e-01, -3.3514e-01,  3.7418e-02,  1.0773e-01,\n          2.4198e-01,  7.2478e-02, -1.3706e-01],\n        [-2.7435e-01, -1.5455e-01,  8.9023e-02, -1.7309e-01,  1.8425e-01,\n         -8.7541e-02, -1.6240e-01,  2.8427e-01],\n        [-8.5467e-02,  2.4396e-01, -4.1071e-02,  1.0652e-01,  1.0700e-01,\n         -1.3494e-01, -4.1027e-02,  1.7823e-01],\n        [-2.7986e-01, -2.3683e-02, -2.6537e-01, -1.2732e-01,  1.7631e-01,\n          1.6680e-01,  3.1816e-01, -8.9218e-02],\n        [-1.7054e-01, -3.4914e-01,  3.4843e-01,  3.4487e-01, -1.3209e-01,\n         -7.2408e-02, -3.1479e-01,  3.4954e-01],\n        [ 1.9501e-02,  1.5451e-02, -1.8048e-01, -7.0031e-02,  2.9877e-01,\n          6.3828e-02,  6.1491e-03,  5.3356e-02],\n        [ 5.5373e-02,  2.3689e-01,  3.9461e-02, -2.0945e-01, -1.2743e-01,\n          2.3950e-01,  3.0908e-01, -3.5178e-01],\n        [-3.1298e-01, -3.2798e-02, -1.0810e-01, -8.0712e-02, -3.4647e-01,\n          1.4301e-01,  3.3178e-01,  7.1103e-02],\n        [ 3.0970e-02,  2.7236e-01,  2.0120e-01,  1.0558e-01, -1.1977e-01,\n         -1.3241e-01, -6.5320e-02, -1.4339e-01],\n        [-3.1808e-01,  3.3692e-01,  1.8080e-01,  1.4360e-01, -3.0815e-01,\n          2.3528e-01,  2.3100e-01, -7.0868e-02],\n        [-2.6458e-01,  3.4464e-01, -2.9880e-01,  8.4121e-02, -2.7989e-01,\n          3.1478e-01, -2.2226e-01, -7.0509e-02],\n        [-9.3366e-02,  2.2748e-01, -1.9686e-01, -2.6484e-01, -1.9613e-01,\n          4.2614e-02, -4.1827e-02, -1.2885e-01],\n        [-7.0684e-02, -1.9427e-01, -1.0835e-01, -4.3332e-03, -2.2312e-01,\n          2.0998e-01, -3.4862e-02,  1.8536e-01],\n        [ 2.3414e-01, -3.0382e-01,  1.4218e-01,  5.0442e-02, -2.4038e-02,\n         -6.1415e-02,  2.7367e-01, -1.9841e-01],\n        [ 1.5479e-01, -2.3351e-01,  5.3166e-02,  9.3925e-02, -5.2788e-02,\n         -2.3860e-01, -3.5228e-01,  1.9050e-01],\n        [ 3.1541e-01,  2.5426e-01, -1.3145e-01,  3.5228e-01,  2.5785e-01,\n          1.0438e-01, -3.1788e-01, -2.8974e-01],\n        [ 1.8730e-01,  2.3957e-01, -2.0769e-01,  3.3500e-01,  6.5880e-02,\n          8.0244e-02,  2.5361e-01, -3.0189e-01],\n        [-2.1888e-01, -3.2565e-01, -1.2990e-01, -1.3715e-01,  1.4201e-01,\n         -1.0913e-01,  2.8911e-01, -3.8782e-02],\n        [-9.6755e-02,  8.7821e-02,  2.5759e-02,  3.4378e-01,  2.2967e-01,\n         -2.5764e-01,  3.4478e-01,  1.5716e-01],\n        [-1.2460e-01,  1.8678e-01, -2.5812e-01,  1.4501e-01,  5.0575e-02,\n          2.8969e-02, -1.8767e-01, -2.1813e-01],\n        [ 2.8460e-01,  3.2698e-01,  1.1490e-01,  5.9911e-02, -3.1886e-01,\n          7.5435e-02, -8.2079e-02,  1.0861e-01],\n        [ 3.3157e-01,  2.2191e-01,  4.8928e-02,  1.5834e-01, -1.4549e-01,\n          2.3289e-01, -2.4554e-01, -1.4028e-01],\n        [-3.2409e-01,  3.3042e-01,  1.6498e-01, -4.5298e-02, -3.2156e-01,\n         -1.1633e-01,  1.5415e-01, -1.1759e-01],\n        [ 1.9539e-01,  2.3210e-01,  2.2263e-01, -2.0625e-01, -2.4210e-01,\n          1.4621e-01,  2.2772e-01, -1.5552e-01],\n        [ 3.1210e-01, -2.9310e-01, -1.9393e-01,  2.2803e-01,  2.3250e-02,\n         -2.2712e-01, -2.5649e-01, -3.2218e-02],\n        [ 1.5494e-01,  5.3380e-02,  2.1551e-01,  2.7522e-01,  4.5909e-02,\n          1.0232e-01, -3.2694e-01,  1.5668e-01],\n        [ 4.2652e-02, -2.5231e-01,  2.6416e-01,  2.4911e-01,  4.7567e-02,\n          2.7137e-01,  5.1763e-02, -3.0143e-01],\n        [ 1.0171e-01, -9.8774e-02, -3.3491e-01,  1.2146e-02,  2.2164e-01,\n          1.4785e-01,  3.4070e-01, -3.2427e-01],\n        [ 9.9798e-02,  1.6978e-01, -3.3085e-01, -2.1088e-01, -1.4902e-02,\n          1.0814e-01, -2.5966e-01,  2.9027e-01],\n        [ 1.9486e-01, -2.0559e-01, -3.0492e-01,  1.0428e-01, -2.8900e-01,\n         -1.9700e-02,  2.9457e-01, -6.0927e-03],\n        [-2.5604e-01,  5.2337e-02, -3.5110e-01, -9.8497e-02, -4.4926e-02,\n         -7.6212e-02, -1.3961e-01,  2.3674e-01],\n        [ 3.1414e-01,  3.4911e-03,  7.1929e-02, -3.3852e-01, -1.7408e-01,\n         -6.9814e-02,  1.1832e-01, -2.3062e-01],\n        [ 1.0276e-01,  2.6364e-01, -7.1565e-02, -2.3374e-02, -9.1750e-02,\n          2.8528e-01, -3.2967e-01,  2.1809e-01],\n        [ 6.9341e-02,  7.7805e-02,  2.8369e-02, -1.1476e-01,  1.4929e-01,\n          2.3138e-01, -7.6657e-04,  2.0008e-01],\n        [-1.8558e-01,  2.7449e-01, -2.2061e-01, -2.7522e-01, -1.6400e-01,\n          6.8450e-02, -3.3674e-01,  2.3542e-01],\n        [-8.9247e-02, -8.0830e-02,  2.5569e-01, -2.4203e-01, -2.5560e-01,\n          6.1101e-02, -1.6382e-01, -1.9433e-01],\n        [-1.4068e-01, -2.2135e-02,  5.6069e-02, -2.5688e-01, -6.6992e-02,\n         -2.9525e-01, -1.1744e-01,  3.5327e-01],\n        [-9.0105e-02,  7.4319e-02,  2.3129e-01, -2.1945e-01,  2.5038e-01,\n         -1.0419e-01, -1.0968e-01,  5.2642e-02],\n        [ 3.3900e-01, -5.4152e-02, -3.5231e-01,  4.5757e-02,  2.5595e-01,\n         -3.3944e-01, -2.7257e-01, -2.6324e-01],\n        [-1.5208e-01, -3.4020e-01,  3.1490e-01, -4.3713e-02,  6.4194e-02,\n          2.5920e-01, -3.4312e-01,  3.1044e-01],\n        [ 1.7882e-01,  5.8606e-02,  1.1621e-02, -1.4691e-02, -2.8749e-01,\n          9.4207e-03,  1.8655e-01,  2.6741e-01]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	100,
                                "training":	false
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=100, out_features=100, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0429,  0.0171, -0.0854, -0.0824, -0.0024,  0.0228,  0.0931, -0.0832,\n        -0.0436,  0.0444, -0.0824, -0.0099,  0.0338,  0.0946, -0.0469,  0.0913,\n        -0.0149,  0.0591, -0.0493,  0.0238, -0.0739,  0.0762,  0.0231, -0.0696,\n         0.0136,  0.0229,  0.0361,  0.0810,  0.0700,  0.0326,  0.0172,  0.0558,\n         0.0837, -0.0658, -0.0267,  0.0101, -0.0871, -0.0969,  0.0356,  0.0875,\n         0.0730,  0.0900, -0.0333,  0.0740, -0.0508,  0.0905, -0.0267, -0.0716,\n        -0.0999, -0.0811,  0.0403,  0.0766,  0.0264,  0.0797,  0.0660,  0.0259,\n        -0.0208,  0.0984, -0.0110, -0.0566,  0.0247, -0.0326, -0.0835, -0.0333,\n         0.0990, -0.0100,  0.0324,  0.0068, -0.0481, -0.0896,  0.0667, -0.0682,\n        -0.0038, -0.0649, -0.0257, -0.0131, -0.0848, -0.0191,  0.0160,  0.0572,\n         0.0692, -0.0247,  0.0271,  0.0101,  0.0372, -0.0996, -0.0449,  0.0462,\n         0.0435, -0.0925, -0.0842, -0.0133, -0.0384, -0.0390,  0.0035,  0.0868,\n        -0.0220, -0.0894,  0.0778,  0.0721], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.0270,  0.0960,  0.0300,  ...,  0.0333, -0.0567, -0.0411],\n        [-0.0099,  0.0612, -0.0199,  ..., -0.0151,  0.0606,  0.0733],\n        [-0.0937,  0.0396, -0.0090,  ..., -0.0721, -0.0170, -0.0487],\n        ...,\n        [ 0.0440,  0.0176, -0.0304,  ...,  0.0528, -0.0049, -0.0865],\n        [ 0.0952,  0.0511, -0.0270,  ..., -0.0106, -0.0888,  0.0325],\n        [ 0.0498,  0.0847,  0.0542,  ..., -0.0906, -0.0392, -0.0440]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	100,
                                "out_features":	100,
                                "training":	false
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=100, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0228,  0.0834, -0.0500,  0.0038], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0497,  0.0946, -0.0603, -0.0080,  0.0847,  0.0702,  0.0558,  0.0014,\n          0.0660, -0.0961,  0.0122,  0.0470,  0.0444,  0.0654, -0.0650, -0.0153,\n          0.0563, -0.0712,  0.0476,  0.0150,  0.0139, -0.0784, -0.0423,  0.0378,\n         -0.0303,  0.0858, -0.0626,  0.0637,  0.0387, -0.0605,  0.0762, -0.0801,\n          0.0596, -0.0122,  0.0226, -0.0660, -0.0675, -0.0827,  0.0399,  0.0487,\n          0.0211,  0.0585,  0.0813,  0.0512,  0.0980, -0.0518, -0.0879, -0.0421,\n          0.0235,  0.0206,  0.0129, -0.0687, -0.0700, -0.0269,  0.0388,  0.0152,\n         -0.0393, -0.0473,  0.0623,  0.0198, -0.0082,  0.0491,  0.0724,  0.0675,\n         -0.0501, -0.0887, -0.0202, -0.0161, -0.0129,  0.0022, -0.0182,  0.0951,\n         -0.0988,  0.0407,  0.0180, -0.0324,  0.0248,  0.0697,  0.0835, -0.0364,\n         -0.0030, -0.0679,  0.0284,  0.0265,  0.0072,  0.0016, -0.0761,  0.0873,\n         -0.0486, -0.0730, -0.0245, -0.0640,  0.0042, -0.0540,  0.0673,  0.0935,\n          0.0631, -0.0922,  0.0090,  0.0703],\n        [-0.0853, -0.0074,  0.0783,  0.0236,  0.0051, -0.0622, -0.0679, -0.0519,\n         -0.0365,  0.0159,  0.0587,  0.0107, -0.0233,  0.0921, -0.0794, -0.0539,\n          0.0187, -0.0545, -0.0212, -0.0080, -0.0858,  0.0703,  0.0621,  0.0835,\n         -0.0809,  0.0155, -0.0004, -0.0530,  0.0593, -0.0922,  0.0267, -0.0902,\n          0.0695, -0.0856, -0.0731,  0.0282, -0.0656,  0.0515,  0.0896, -0.0864,\n         -0.0691,  0.0892, -0.0868, -0.0698,  0.0389, -0.0862, -0.0645,  0.0430,\n         -0.0850,  0.0811,  0.0546,  0.0967,  0.0848,  0.0452, -0.0719, -0.0048,\n         -0.0568, -0.0626,  0.0779,  0.0653, -0.0745, -0.0878,  0.0822,  0.0537,\n          0.0392, -0.0304,  0.0971,  0.0200, -0.0467, -0.0177,  0.0441, -0.0128,\n         -0.0953,  0.0699, -0.0102,  0.0868,  0.0152,  0.0047, -0.0325,  0.0861,\n          0.0877,  0.0802,  0.0997, -0.0984, -0.0004,  0.0476,  0.0788, -0.0346,\n         -0.0994,  0.0179, -0.0304,  0.0624,  0.0995,  0.0191,  0.0002, -0.0853,\n          0.0940,  0.0414, -0.0650,  0.0565],\n        [ 0.0307, -0.0526,  0.0541, -0.0323, -0.0103, -0.0318, -0.0127, -0.0709,\n          0.0562,  0.0999, -0.0518,  0.0289, -0.0639, -0.0551,  0.0370, -0.0785,\n          0.0618,  0.0727, -0.0032, -0.0463,  0.0143,  0.0969,  0.0778, -0.0582,\n          0.0579,  0.0702, -0.0926,  0.0541, -0.0043,  0.0783, -0.0252, -0.0364,\n         -0.0827, -0.0649,  0.0404,  0.0017, -0.0228,  0.0920, -0.0114, -0.0424,\n         -0.0323, -0.0759, -0.0789, -0.0926, -0.0502, -0.0570,  0.0398,  0.0620,\n         -0.0713, -0.0327,  0.0048, -0.0544,  0.0510, -0.0678, -0.0150,  0.0529,\n         -0.0656,  0.0606,  0.0717, -0.0061,  0.0953,  0.0626,  0.0152,  0.0400,\n         -0.0174, -0.0509, -0.0500, -0.0467,  0.0880, -0.0675,  0.0050,  0.0256,\n          0.0441, -0.0448,  0.0670,  0.0288, -0.0916,  0.0598,  0.0770,  0.0397,\n          0.0632, -0.0090,  0.0558, -0.0919,  0.0136,  0.0151, -0.0455,  0.0800,\n          0.0960,  0.0906,  0.0527,  0.0420, -0.0667, -0.0322, -0.0771,  0.0998,\n         -0.0236, -0.0956, -0.0327,  0.0902],\n        [-0.0698, -0.0427, -0.0571,  0.0300, -0.0420, -0.0759,  0.0164, -0.0157,\n         -0.0715,  0.0387,  0.0334,  0.0414,  0.0364,  0.0301, -0.0253,  0.0291,\n          0.0723, -0.0759, -0.0471, -0.0521,  0.0548,  0.0239,  0.0251,  0.0695,\n         -0.0230, -0.0986,  0.0833, -0.0429,  0.0104, -0.0519, -0.0084, -0.0882,\n          0.0106,  0.0861, -0.0156, -0.0883, -0.0097,  0.0558,  0.0249,  0.0936,\n          0.0657, -0.0356,  0.0852,  0.0161, -0.0190, -0.0966, -0.0406, -0.0011,\n          0.0141,  0.0044, -0.0488,  0.0674,  0.0866,  0.0073, -0.0998,  0.0368,\n         -0.0348, -0.0737,  0.0126,  0.0964, -0.0277, -0.0693,  0.0940, -0.0671,\n         -0.0500, -0.0917, -0.0460, -0.0806,  0.0801,  0.0012, -0.0947,  0.0072,\n         -0.0334,  0.0918, -0.0383,  0.0780,  0.0484, -0.0862, -0.0196, -0.0997,\n          0.0305,  0.0043,  0.0160,  0.0176, -0.0548,  0.0295, -0.0282, -0.0421,\n         -0.0501,  0.0326,  0.0336,  0.0095, -0.0087,  0.0218, -0.0675, -0.0751,\n          0.0019,  0.0987,  0.0550, -0.0760]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	100,
                                "out_features":	4,
                                "training":	false
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	false
                }
            },
            "_target_net_update_freq":	100,
            "_train_q_iters":	80,
            "_train_update_freq":	3,
            "_traj_per_epoch":	6,
            "logger":	{
                "<utils.logger.EpochLogger object at 0x7164709d5f50>":	{
                    "epoch_dict":	{},
                    "exp_name":	"rl4sys-dqn-info",
                    "first_row":	true,
                    "log_current_row":	{},
                    "log_headers":	[],
                    "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s479060000",
                    "output_file":	{
                        "<_io.TextIOWrapper name='/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s479060000/progress.txt' mode='w' encoding='UTF-8'>":	{
                            "mode":	"w"
                        }
                    }
                }
            }
        }
    },
    "target_net_update_freq":	100,
    "train_q_iters":	80,
    "train_update_freq":	3,
    "traj_per_epoch":	6
}