{
    "__class__":	"DQN",
    "act_dim":	4,
    "aux_batch_size":	4,
    "batch_size":	60,
    "buf_size":	50000,
    "buffer_segments":	5,
    "env_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar",
    "epsilon":	1.0,
    "epsilon_decay":	0.0005,
    "epsilon_min":	0.01,
    "exp_name":	"rl4sys-dqn-info",
    "gamma":	0.99,
    "kernel_dim":	4,
    "kernel_size":	2,
    "log_data_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/",
    "logger_kwargs":	{
        "exp_name":	"rl4sys-dqn-info",
        "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s535180000"
    },
    "max_sample_age":	-1,
    "q_lr":	0.0003,
    "sample_decay":	0.8,
    "seed":	535180000,
    "self":	{
        "<algorithms.DQN.DQN.DQN object at 0x763850a74fd0>":	{
            "_act_dim":	4,
            "_aux_batch_size":	4,
            "_batch_size":	60,
            "_buf_size":	50000,
            "_epsilon":	1.0,
            "_epsilon_decay":	0.0005,
            "_epsilon_min":	0.01,
            "_gamma":	0.99,
            "_kernel_dim":	4,
            "_kernel_size":	2,
            "_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.0005,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.1924, -0.0533,  0.0887, -0.0676,  0.2959,  0.1120,  0.2640,  0.1382,\n         0.1973, -0.1865, -0.3505, -0.2749,  0.0081, -0.1239,  0.1859,  0.2404,\n         0.0160,  0.3455, -0.2832,  0.1566, -0.3258,  0.3173, -0.1862,  0.0005,\n         0.3285,  0.0279, -0.2884,  0.1908,  0.2919, -0.1489, -0.0123, -0.1180,\n         0.2004,  0.1597,  0.3417, -0.0422, -0.0754,  0.0773, -0.0086,  0.2963,\n        -0.1290,  0.1676, -0.2846,  0.2935, -0.1741,  0.2592,  0.0671,  0.0697,\n         0.0835, -0.1406,  0.1019, -0.2192, -0.3114, -0.2820,  0.1603, -0.3477,\n        -0.0969,  0.3191,  0.0106,  0.0765, -0.1737,  0.2558,  0.0297, -0.2465,\n        -0.3005,  0.2634, -0.0465,  0.1511,  0.0051, -0.0381,  0.3213, -0.1876,\n        -0.1717,  0.3471,  0.0935, -0.2827,  0.3118,  0.0297,  0.0135, -0.1464,\n         0.3382, -0.3530, -0.2932, -0.1962, -0.2487,  0.2430,  0.3530,  0.1568,\n         0.0028, -0.1887, -0.3046, -0.1054,  0.0669, -0.1895, -0.1485, -0.2559,\n         0.3268,  0.0904,  0.1878,  0.1802, -0.1858, -0.0205,  0.1060,  0.0368,\n         0.2469,  0.1006, -0.2951, -0.2569,  0.1465,  0.0789,  0.0320,  0.3381,\n        -0.2869, -0.3457, -0.2947, -0.0963,  0.2463, -0.3152,  0.0104,  0.1033,\n        -0.3145,  0.2242,  0.3132,  0.1787, -0.0406, -0.1313,  0.2184, -0.2954],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.2212, -0.3346,  0.0987,  ...,  0.1467, -0.0632,  0.1522],\n        [ 0.2313,  0.0962,  0.1838,  ..., -0.0407, -0.2786, -0.2585],\n        [ 0.0599,  0.1724, -0.2844,  ..., -0.1463,  0.1246,  0.1424],\n        ...,\n        [ 0.0461,  0.0812, -0.2347,  ..., -0.3334, -0.0809,  0.1949],\n        [-0.1340, -0.0949,  0.1630,  ...,  0.2549, -0.0329,  0.1628],\n        [-0.2046,  0.1137,  0.3316,  ...,  0.0808, -0.1168, -0.1719]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	128,
                                "training":	true
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=128, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0719,  0.0669, -0.0320,  0.0657,  0.0602,  0.0123,  0.0290,  0.0117,\n        -0.0286, -0.0461,  0.0522,  0.0397,  0.0268, -0.0303,  0.0615,  0.0729,\n        -0.0846,  0.0388,  0.0345,  0.0787, -0.0314,  0.0243, -0.0315, -0.0399,\n         0.0874,  0.0812, -0.0015, -0.0648,  0.0313,  0.0778, -0.0076,  0.0180,\n         0.0811, -0.0728,  0.0495,  0.0563,  0.0716,  0.0059,  0.0601,  0.0470,\n         0.0090,  0.0232,  0.0563,  0.0666, -0.0011,  0.0432,  0.0151,  0.0590,\n        -0.0775, -0.0506,  0.0624,  0.0369,  0.0031, -0.0732, -0.0354, -0.0431,\n         0.0845,  0.0092, -0.0300, -0.0601,  0.0028,  0.0256,  0.0067, -0.0701,\n        -0.0092,  0.0050,  0.0406, -0.0258, -0.0273, -0.0007,  0.0746,  0.0541,\n         0.0554,  0.0570, -0.0720,  0.0772, -0.0354,  0.0560,  0.0666,  0.0313,\n         0.0758, -0.0425, -0.0473, -0.0584,  0.0284,  0.0306,  0.0136,  0.0557,\n        -0.0158,  0.0372, -0.0700,  0.0368, -0.0293,  0.0567, -0.0480,  0.0349,\n        -0.0600, -0.0337, -0.0536,  0.0735,  0.0633, -0.0242,  0.0723, -0.0387,\n         0.0449, -0.0131,  0.0541, -0.0650,  0.0228,  0.0335, -0.0834,  0.0718,\n        -0.0659, -0.0200,  0.0482, -0.0317, -0.0517,  0.0534,  0.0525, -0.0567,\n         0.0222,  0.0389, -0.0301, -0.0663, -0.0695, -0.0194, -0.0180,  0.0259],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0496, -0.0094,  0.0311,  ...,  0.0859, -0.0079, -0.0755],\n        [ 0.0281,  0.0803,  0.0540,  ...,  0.0205,  0.0848, -0.0016],\n        [ 0.0200, -0.0300, -0.0315,  ...,  0.0547, -0.0553, -0.0839],\n        ...,\n        [ 0.0421, -0.0527, -0.0524,  ..., -0.0257,  0.0666, -0.0486],\n        [ 0.0764,  0.0379, -0.0518,  ..., -0.0781, -0.0569,  0.0782],\n        [-0.0689,  0.0558,  0.0544,  ...,  0.0641, -0.0489,  0.0785]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	128,
                                "training":	true
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=128, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0714,  0.0872, -0.0449, -0.0147], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 5.7855e-02,  7.5944e-02,  2.6384e-02, -3.5221e-03, -2.4096e-02,\n          3.5929e-02,  3.7899e-02, -9.2941e-04, -8.8946e-03, -2.9219e-02,\n         -6.8688e-02,  3.4321e-02,  6.7837e-02,  6.2045e-02,  8.6615e-02,\n         -8.1510e-02,  4.9966e-02,  1.6088e-02, -2.0411e-02,  4.2523e-02,\n          3.8433e-02,  8.1274e-02,  7.3628e-03,  4.2666e-03, -3.8230e-02,\n          2.2837e-02, -6.6867e-02, -4.4497e-02,  1.4538e-02, -2.9611e-02,\n         -2.5763e-02, -3.4694e-02, -8.5100e-03,  1.4865e-02,  8.4680e-02,\n         -6.0299e-02,  8.5599e-02, -7.3752e-02,  4.8247e-02,  3.5584e-02,\n          8.2375e-02,  8.1858e-02,  4.9683e-02, -5.0234e-03, -7.8617e-02,\n          2.2104e-04, -5.3499e-02,  2.7322e-02,  6.7154e-03, -8.7106e-02,\n          3.3618e-03, -1.1417e-03,  4.7043e-02, -8.5033e-02,  2.3386e-02,\n          6.9752e-02,  3.6687e-02, -8.3620e-02, -2.8391e-02,  5.7748e-03,\n          3.3411e-02,  6.7318e-02,  6.3145e-02,  7.3475e-04, -8.3524e-02,\n          6.1237e-02, -2.3323e-02, -4.9248e-02,  3.9414e-02,  4.5778e-02,\n         -4.9679e-02, -1.9097e-02,  3.6497e-02, -7.1682e-02,  7.7081e-02,\n         -7.8758e-02,  6.4863e-02, -3.5411e-02, -6.8231e-02, -4.7769e-03,\n         -3.7045e-02,  5.0847e-02,  5.6248e-02,  7.0137e-02,  2.9944e-02,\n          6.9431e-02, -4.4679e-02, -6.1426e-02,  1.0178e-02,  1.1449e-02,\n          8.5036e-02,  7.4453e-02, -1.7451e-02, -7.0812e-02, -2.4337e-02,\n         -4.4722e-02, -1.3843e-02,  2.3662e-02, -1.5513e-02, -1.2344e-02,\n         -2.1728e-02,  3.9071e-02,  6.4617e-02, -5.6264e-02, -5.6918e-02,\n          3.4827e-02, -2.0144e-02, -1.2497e-02, -7.6052e-02,  1.6168e-02,\n         -1.1592e-02,  2.6295e-02,  4.5777e-02,  2.8969e-02,  1.3651e-02,\n         -7.6056e-02,  5.9833e-02, -2.7907e-02,  4.9745e-02,  7.1970e-02,\n         -7.0168e-02, -2.6591e-02, -2.5789e-02,  3.0210e-02, -5.3694e-02,\n          5.8600e-02, -4.2357e-02,  8.1808e-02],\n        [-5.7501e-02,  8.3956e-02, -1.0111e-02,  4.3345e-02, -3.8940e-02,\n         -5.0733e-02,  7.0146e-02,  3.3893e-02,  4.4127e-02, -4.0749e-02,\n         -3.6548e-02, -3.3075e-02,  1.9832e-02, -5.6276e-02, -4.4365e-02,\n          5.8858e-02,  7.6887e-02, -1.0083e-02, -5.5487e-02,  3.0828e-02,\n         -3.1111e-02, -8.0169e-02,  7.9747e-02, -5.8241e-02,  1.0961e-02,\n         -7.8072e-02,  6.1405e-02,  3.4071e-02,  6.3971e-02, -6.9702e-02,\n          3.4932e-02,  8.1816e-02,  4.2378e-03,  3.5950e-03,  5.0788e-02,\n          6.9815e-02, -3.8784e-02,  1.0034e-02,  5.5475e-02,  5.6493e-02,\n          5.0452e-02, -1.4354e-02, -4.7940e-02,  1.9216e-02,  5.1810e-02,\n          5.9970e-02,  7.9068e-02, -3.8850e-02, -4.6821e-02,  3.6293e-02,\n          7.8876e-02,  5.5447e-02,  5.7711e-02, -4.6652e-02,  6.1035e-02,\n          5.1166e-02,  1.1356e-02,  4.6169e-02,  4.0746e-02,  3.3056e-02,\n          7.0106e-02,  6.0899e-02,  4.8373e-02,  7.8386e-02, -6.1761e-03,\n          1.8544e-02, -6.2060e-02, -4.9252e-02,  7.2731e-02, -5.9601e-02,\n          5.1682e-03,  7.8067e-02,  4.0465e-02, -7.9011e-02,  7.4312e-02,\n         -8.3315e-02, -1.2469e-02, -4.6414e-02, -6.6241e-02,  1.8206e-02,\n          5.8749e-02, -2.1096e-03, -8.4165e-02, -1.5342e-03, -4.9821e-02,\n         -5.7168e-02,  2.4452e-02,  4.8981e-02,  2.6851e-02, -5.1581e-02,\n         -1.0621e-02, -6.6054e-02,  3.0077e-02,  7.3994e-02, -5.7439e-02,\n         -7.0681e-02,  4.0542e-02,  5.8319e-02,  2.6144e-02,  5.7717e-02,\n          8.3111e-02,  8.2547e-02, -8.3106e-02, -8.6628e-02,  6.3539e-02,\n          1.5341e-02,  8.4786e-02,  8.1011e-02,  5.6094e-02, -3.6335e-02,\n          1.1515e-02,  3.2165e-02, -1.8057e-02, -3.3251e-02,  6.3797e-02,\n         -5.4827e-02,  3.9077e-02, -5.7944e-02, -5.0117e-02, -7.9332e-03,\n         -5.3958e-02,  6.9981e-02,  5.6276e-02, -4.8507e-02, -1.8171e-03,\n         -3.7489e-02, -1.0545e-02, -4.9181e-02],\n        [-1.6435e-02, -1.6272e-02, -3.2369e-02, -3.0775e-02,  1.2978e-02,\n          5.6644e-02, -7.3635e-02, -4.0464e-02, -5.2371e-03,  5.8262e-02,\n          3.7533e-02, -1.7887e-02,  3.1547e-02, -3.3615e-02, -8.6575e-02,\n         -2.2505e-02,  2.3088e-02,  7.9269e-02, -4.3384e-02, -4.0179e-02,\n          5.9471e-02, -8.6865e-02, -5.4699e-02, -6.0340e-02, -7.5754e-02,\n          6.8685e-02, -4.5759e-02, -4.0610e-02, -6.4448e-02, -3.0387e-02,\n         -6.1922e-02, -2.5952e-02,  2.3787e-02, -5.7915e-02, -6.4031e-02,\n         -8.1659e-02,  2.2815e-02,  4.4292e-02, -5.6540e-02, -5.5088e-02,\n         -7.5135e-02,  8.7922e-02,  9.1040e-03, -3.9632e-02,  6.0752e-02,\n         -7.5014e-02,  4.2068e-02,  1.1740e-02, -6.9654e-02,  8.7813e-02,\n         -6.4578e-02, -4.2667e-03,  7.1869e-02,  1.3925e-02, -4.9461e-02,\n         -4.0898e-02,  4.0307e-02,  6.2367e-02,  5.7688e-02, -7.6545e-02,\n          2.6817e-02,  4.2011e-02, -6.3829e-03,  8.4220e-02, -3.5888e-02,\n          5.3810e-02,  7.2898e-02, -3.3375e-02, -1.0893e-02, -5.3338e-02,\n          1.3620e-03,  4.4781e-02, -4.7096e-02, -3.6536e-02,  9.3670e-03,\n         -6.3091e-02,  2.6412e-02,  4.3533e-02, -4.9652e-02,  4.6034e-02,\n          2.2654e-06,  1.4654e-02, -2.3964e-02, -1.0549e-02, -8.1124e-02,\n          3.8936e-02,  2.0947e-02,  8.7514e-03, -7.2416e-02, -3.8315e-02,\n          9.5320e-03,  4.9152e-02, -9.4876e-03,  3.5771e-02, -5.8691e-02,\n          8.3251e-02, -7.2837e-03,  7.8877e-02, -4.9681e-02, -1.2499e-02,\n          4.7135e-02, -8.6774e-02,  2.2584e-02, -4.8320e-02,  7.4247e-02,\n         -7.3219e-02,  9.1808e-03, -8.0097e-02, -8.3336e-02, -7.0606e-02,\n         -5.8767e-02, -2.6855e-02, -6.7725e-02, -1.2140e-02, -5.3266e-02,\n         -1.4966e-03, -3.3690e-02,  7.7675e-02,  3.1348e-02,  7.7088e-02,\n         -2.3654e-02,  6.6357e-03, -3.6196e-02, -7.6533e-02,  5.2672e-02,\n          3.0410e-02,  4.7519e-03,  5.1248e-02],\n        [ 1.0303e-02,  1.0959e-03,  8.1429e-02, -3.0932e-02, -4.5337e-02,\n          4.6520e-02,  5.0508e-02,  1.2146e-02,  1.8098e-02, -5.4333e-02,\n          8.5414e-02, -7.9053e-02, -6.4146e-02,  7.7450e-02,  5.9051e-02,\n         -7.3816e-02,  8.2212e-02, -2.0481e-02,  1.3469e-02,  1.3817e-02,\n          7.0116e-03, -9.0355e-03, -4.6154e-02,  7.0988e-02,  8.1004e-02,\n         -6.6334e-02, -7.2332e-02,  1.3510e-02,  4.0300e-02, -3.3146e-02,\n         -3.0151e-03,  5.6716e-02, -6.3293e-03, -7.9890e-02, -3.5448e-02,\n         -6.2415e-03,  4.9207e-02,  5.0099e-02, -5.4465e-02, -4.4522e-03,\n         -1.2661e-02,  2.8087e-02, -8.3376e-02,  5.0107e-02,  3.9991e-02,\n         -3.1152e-02, -1.8426e-02,  2.2324e-02,  5.1961e-02,  7.2718e-02,\n         -6.4952e-02,  1.9245e-02,  7.9789e-02, -3.9123e-02,  7.0532e-02,\n          2.8394e-03,  8.5256e-02, -7.0627e-02, -6.1435e-02,  4.8366e-03,\n          9.8573e-03,  2.3576e-02, -7.8728e-02, -7.8082e-02,  1.4346e-02,\n         -1.7262e-02,  1.9094e-02,  7.3734e-03,  7.6479e-02, -8.0038e-02,\n          5.8443e-02, -1.1920e-02, -3.0942e-02,  8.7420e-02,  7.6617e-02,\n         -1.3037e-02, -3.1721e-02, -7.1686e-02,  3.9247e-02,  4.8933e-02,\n         -8.3314e-02,  8.6184e-04,  2.4235e-02,  1.6603e-02, -1.0038e-02,\n         -3.9688e-02, -2.6695e-02,  4.7160e-02,  7.1831e-02,  6.0465e-02,\n         -1.1372e-02,  6.7173e-02,  4.2621e-02,  1.3010e-02,  1.3489e-02,\n          5.2432e-02, -4.9152e-02, -4.7967e-02, -3.0887e-03, -1.3011e-02,\n          3.3312e-02, -5.3654e-02, -1.7918e-02, -7.0364e-02,  3.7960e-02,\n          6.3098e-02,  1.2020e-03, -3.1569e-02,  2.0134e-03, -1.9177e-02,\n         -3.9396e-02, -2.4026e-02,  7.8012e-02, -3.3891e-02,  7.0627e-02,\n         -2.0951e-02,  2.1273e-02, -8.6965e-02, -7.6009e-02,  7.8396e-02,\n          3.7014e-02,  7.6566e-02, -2.6199e-02,  6.6038e-02, -3.4198e-02,\n          6.8122e-02, -8.6748e-02, -2.6473e-02]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	4,
                                "training":	true
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	true
                }
            },
            "_q_optimizer":	{
                "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)":	{
                    "_optimizer_load_state_dict_post_hooks":	{},
                    "_optimizer_load_state_dict_pre_hooks":	{},
                    "_optimizer_state_dict_post_hooks":	{},
                    "_optimizer_state_dict_pre_hooks":	{},
                    "_optimizer_step_post_hooks":	{},
                    "_optimizer_step_pre_hooks":	{},
                    "_warned_capturable_if_run_uncaptured":	true,
                    "_zero_grad_profile_name":	"Optimizer.zero_grad#Adam.zero_grad",
                    "defaults":	{
                        "amsgrad":	false,
                        "betas":	[
                            0.9,
                            0.999
                        ],
                        "capturable":	false,
                        "differentiable":	false,
                        "eps":	1e-08,
                        "foreach":	null,
                        "fused":	null,
                        "lr":	0.0003,
                        "maximize":	false,
                        "weight_decay":	0
                    },
                    "param_groups":	[
                        {
                            "amsgrad":	false,
                            "betas":	[
                                0.9,
                                0.999
                            ],
                            "capturable":	false,
                            "differentiable":	false,
                            "eps":	1e-08,
                            "foreach":	null,
                            "fused":	null,
                            "lr":	0.0003,
                            "maximize":	false,
                            "params":	[
                                "Parameter containing:\ntensor([[ 0.2212, -0.3346,  0.0987,  ...,  0.1467, -0.0632,  0.1522],\n        [ 0.2313,  0.0962,  0.1838,  ..., -0.0407, -0.2786, -0.2585],\n        [ 0.0599,  0.1724, -0.2844,  ..., -0.1463,  0.1246,  0.1424],\n        ...,\n        [ 0.0461,  0.0812, -0.2347,  ..., -0.3334, -0.0809,  0.1949],\n        [-0.1340, -0.0949,  0.1630,  ...,  0.2549, -0.0329,  0.1628],\n        [-0.2046,  0.1137,  0.3316,  ...,  0.0808, -0.1168, -0.1719]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.1924, -0.0533,  0.0887, -0.0676,  0.2959,  0.1120,  0.2640,  0.1382,\n         0.1973, -0.1865, -0.3505, -0.2749,  0.0081, -0.1239,  0.1859,  0.2404,\n         0.0160,  0.3455, -0.2832,  0.1566, -0.3258,  0.3173, -0.1862,  0.0005,\n         0.3285,  0.0279, -0.2884,  0.1908,  0.2919, -0.1489, -0.0123, -0.1180,\n         0.2004,  0.1597,  0.3417, -0.0422, -0.0754,  0.0773, -0.0086,  0.2963,\n        -0.1290,  0.1676, -0.2846,  0.2935, -0.1741,  0.2592,  0.0671,  0.0697,\n         0.0835, -0.1406,  0.1019, -0.2192, -0.3114, -0.2820,  0.1603, -0.3477,\n        -0.0969,  0.3191,  0.0106,  0.0765, -0.1737,  0.2558,  0.0297, -0.2465,\n        -0.3005,  0.2634, -0.0465,  0.1511,  0.0051, -0.0381,  0.3213, -0.1876,\n        -0.1717,  0.3471,  0.0935, -0.2827,  0.3118,  0.0297,  0.0135, -0.1464,\n         0.3382, -0.3530, -0.2932, -0.1962, -0.2487,  0.2430,  0.3530,  0.1568,\n         0.0028, -0.1887, -0.3046, -0.1054,  0.0669, -0.1895, -0.1485, -0.2559,\n         0.3268,  0.0904,  0.1878,  0.1802, -0.1858, -0.0205,  0.1060,  0.0368,\n         0.2469,  0.1006, -0.2951, -0.2569,  0.1465,  0.0789,  0.0320,  0.3381,\n        -0.2869, -0.3457, -0.2947, -0.0963,  0.2463, -0.3152,  0.0104,  0.1033,\n        -0.3145,  0.2242,  0.3132,  0.1787, -0.0406, -0.1313,  0.2184, -0.2954],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[ 0.0496, -0.0094,  0.0311,  ...,  0.0859, -0.0079, -0.0755],\n        [ 0.0281,  0.0803,  0.0540,  ...,  0.0205,  0.0848, -0.0016],\n        [ 0.0200, -0.0300, -0.0315,  ...,  0.0547, -0.0553, -0.0839],\n        ...,\n        [ 0.0421, -0.0527, -0.0524,  ..., -0.0257,  0.0666, -0.0486],\n        [ 0.0764,  0.0379, -0.0518,  ..., -0.0781, -0.0569,  0.0782],\n        [-0.0689,  0.0558,  0.0544,  ...,  0.0641, -0.0489,  0.0785]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.0719,  0.0669, -0.0320,  0.0657,  0.0602,  0.0123,  0.0290,  0.0117,\n        -0.0286, -0.0461,  0.0522,  0.0397,  0.0268, -0.0303,  0.0615,  0.0729,\n        -0.0846,  0.0388,  0.0345,  0.0787, -0.0314,  0.0243, -0.0315, -0.0399,\n         0.0874,  0.0812, -0.0015, -0.0648,  0.0313,  0.0778, -0.0076,  0.0180,\n         0.0811, -0.0728,  0.0495,  0.0563,  0.0716,  0.0059,  0.0601,  0.0470,\n         0.0090,  0.0232,  0.0563,  0.0666, -0.0011,  0.0432,  0.0151,  0.0590,\n        -0.0775, -0.0506,  0.0624,  0.0369,  0.0031, -0.0732, -0.0354, -0.0431,\n         0.0845,  0.0092, -0.0300, -0.0601,  0.0028,  0.0256,  0.0067, -0.0701,\n        -0.0092,  0.0050,  0.0406, -0.0258, -0.0273, -0.0007,  0.0746,  0.0541,\n         0.0554,  0.0570, -0.0720,  0.0772, -0.0354,  0.0560,  0.0666,  0.0313,\n         0.0758, -0.0425, -0.0473, -0.0584,  0.0284,  0.0306,  0.0136,  0.0557,\n        -0.0158,  0.0372, -0.0700,  0.0368, -0.0293,  0.0567, -0.0480,  0.0349,\n        -0.0600, -0.0337, -0.0536,  0.0735,  0.0633, -0.0242,  0.0723, -0.0387,\n         0.0449, -0.0131,  0.0541, -0.0650,  0.0228,  0.0335, -0.0834,  0.0718,\n        -0.0659, -0.0200,  0.0482, -0.0317, -0.0517,  0.0534,  0.0525, -0.0567,\n         0.0222,  0.0389, -0.0301, -0.0663, -0.0695, -0.0194, -0.0180,  0.0259],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[ 5.7855e-02,  7.5944e-02,  2.6384e-02, -3.5221e-03, -2.4096e-02,\n          3.5929e-02,  3.7899e-02, -9.2941e-04, -8.8946e-03, -2.9219e-02,\n         -6.8688e-02,  3.4321e-02,  6.7837e-02,  6.2045e-02,  8.6615e-02,\n         -8.1510e-02,  4.9966e-02,  1.6088e-02, -2.0411e-02,  4.2523e-02,\n          3.8433e-02,  8.1274e-02,  7.3628e-03,  4.2666e-03, -3.8230e-02,\n          2.2837e-02, -6.6867e-02, -4.4497e-02,  1.4538e-02, -2.9611e-02,\n         -2.5763e-02, -3.4694e-02, -8.5100e-03,  1.4865e-02,  8.4680e-02,\n         -6.0299e-02,  8.5599e-02, -7.3752e-02,  4.8247e-02,  3.5584e-02,\n          8.2375e-02,  8.1858e-02,  4.9683e-02, -5.0234e-03, -7.8617e-02,\n          2.2104e-04, -5.3499e-02,  2.7322e-02,  6.7154e-03, -8.7106e-02,\n          3.3618e-03, -1.1417e-03,  4.7043e-02, -8.5033e-02,  2.3386e-02,\n          6.9752e-02,  3.6687e-02, -8.3620e-02, -2.8391e-02,  5.7748e-03,\n          3.3411e-02,  6.7318e-02,  6.3145e-02,  7.3475e-04, -8.3524e-02,\n          6.1237e-02, -2.3323e-02, -4.9248e-02,  3.9414e-02,  4.5778e-02,\n         -4.9679e-02, -1.9097e-02,  3.6497e-02, -7.1682e-02,  7.7081e-02,\n         -7.8758e-02,  6.4863e-02, -3.5411e-02, -6.8231e-02, -4.7769e-03,\n         -3.7045e-02,  5.0847e-02,  5.6248e-02,  7.0137e-02,  2.9944e-02,\n          6.9431e-02, -4.4679e-02, -6.1426e-02,  1.0178e-02,  1.1449e-02,\n          8.5036e-02,  7.4453e-02, -1.7451e-02, -7.0812e-02, -2.4337e-02,\n         -4.4722e-02, -1.3843e-02,  2.3662e-02, -1.5513e-02, -1.2344e-02,\n         -2.1728e-02,  3.9071e-02,  6.4617e-02, -5.6264e-02, -5.6918e-02,\n          3.4827e-02, -2.0144e-02, -1.2497e-02, -7.6052e-02,  1.6168e-02,\n         -1.1592e-02,  2.6295e-02,  4.5777e-02,  2.8969e-02,  1.3651e-02,\n         -7.6056e-02,  5.9833e-02, -2.7907e-02,  4.9745e-02,  7.1970e-02,\n         -7.0168e-02, -2.6591e-02, -2.5789e-02,  3.0210e-02, -5.3694e-02,\n          5.8600e-02, -4.2357e-02,  8.1808e-02],\n        [-5.7501e-02,  8.3956e-02, -1.0111e-02,  4.3345e-02, -3.8940e-02,\n         -5.0733e-02,  7.0146e-02,  3.3893e-02,  4.4127e-02, -4.0749e-02,\n         -3.6548e-02, -3.3075e-02,  1.9832e-02, -5.6276e-02, -4.4365e-02,\n          5.8858e-02,  7.6887e-02, -1.0083e-02, -5.5487e-02,  3.0828e-02,\n         -3.1111e-02, -8.0169e-02,  7.9747e-02, -5.8241e-02,  1.0961e-02,\n         -7.8072e-02,  6.1405e-02,  3.4071e-02,  6.3971e-02, -6.9702e-02,\n          3.4932e-02,  8.1816e-02,  4.2378e-03,  3.5950e-03,  5.0788e-02,\n          6.9815e-02, -3.8784e-02,  1.0034e-02,  5.5475e-02,  5.6493e-02,\n          5.0452e-02, -1.4354e-02, -4.7940e-02,  1.9216e-02,  5.1810e-02,\n          5.9970e-02,  7.9068e-02, -3.8850e-02, -4.6821e-02,  3.6293e-02,\n          7.8876e-02,  5.5447e-02,  5.7711e-02, -4.6652e-02,  6.1035e-02,\n          5.1166e-02,  1.1356e-02,  4.6169e-02,  4.0746e-02,  3.3056e-02,\n          7.0106e-02,  6.0899e-02,  4.8373e-02,  7.8386e-02, -6.1761e-03,\n          1.8544e-02, -6.2060e-02, -4.9252e-02,  7.2731e-02, -5.9601e-02,\n          5.1682e-03,  7.8067e-02,  4.0465e-02, -7.9011e-02,  7.4312e-02,\n         -8.3315e-02, -1.2469e-02, -4.6414e-02, -6.6241e-02,  1.8206e-02,\n          5.8749e-02, -2.1096e-03, -8.4165e-02, -1.5342e-03, -4.9821e-02,\n         -5.7168e-02,  2.4452e-02,  4.8981e-02,  2.6851e-02, -5.1581e-02,\n         -1.0621e-02, -6.6054e-02,  3.0077e-02,  7.3994e-02, -5.7439e-02,\n         -7.0681e-02,  4.0542e-02,  5.8319e-02,  2.6144e-02,  5.7717e-02,\n          8.3111e-02,  8.2547e-02, -8.3106e-02, -8.6628e-02,  6.3539e-02,\n          1.5341e-02,  8.4786e-02,  8.1011e-02,  5.6094e-02, -3.6335e-02,\n          1.1515e-02,  3.2165e-02, -1.8057e-02, -3.3251e-02,  6.3797e-02,\n         -5.4827e-02,  3.9077e-02, -5.7944e-02, -5.0117e-02, -7.9332e-03,\n         -5.3958e-02,  6.9981e-02,  5.6276e-02, -4.8507e-02, -1.8171e-03,\n         -3.7489e-02, -1.0545e-02, -4.9181e-02],\n        [-1.6435e-02, -1.6272e-02, -3.2369e-02, -3.0775e-02,  1.2978e-02,\n          5.6644e-02, -7.3635e-02, -4.0464e-02, -5.2371e-03,  5.8262e-02,\n          3.7533e-02, -1.7887e-02,  3.1547e-02, -3.3615e-02, -8.6575e-02,\n         -2.2505e-02,  2.3088e-02,  7.9269e-02, -4.3384e-02, -4.0179e-02,\n          5.9471e-02, -8.6865e-02, -5.4699e-02, -6.0340e-02, -7.5754e-02,\n          6.8685e-02, -4.5759e-02, -4.0610e-02, -6.4448e-02, -3.0387e-02,\n         -6.1922e-02, -2.5952e-02,  2.3787e-02, -5.7915e-02, -6.4031e-02,\n         -8.1659e-02,  2.2815e-02,  4.4292e-02, -5.6540e-02, -5.5088e-02,\n         -7.5135e-02,  8.7922e-02,  9.1040e-03, -3.9632e-02,  6.0752e-02,\n         -7.5014e-02,  4.2068e-02,  1.1740e-02, -6.9654e-02,  8.7813e-02,\n         -6.4578e-02, -4.2667e-03,  7.1869e-02,  1.3925e-02, -4.9461e-02,\n         -4.0898e-02,  4.0307e-02,  6.2367e-02,  5.7688e-02, -7.6545e-02,\n          2.6817e-02,  4.2011e-02, -6.3829e-03,  8.4220e-02, -3.5888e-02,\n          5.3810e-02,  7.2898e-02, -3.3375e-02, -1.0893e-02, -5.3338e-02,\n          1.3620e-03,  4.4781e-02, -4.7096e-02, -3.6536e-02,  9.3670e-03,\n         -6.3091e-02,  2.6412e-02,  4.3533e-02, -4.9652e-02,  4.6034e-02,\n          2.2654e-06,  1.4654e-02, -2.3964e-02, -1.0549e-02, -8.1124e-02,\n          3.8936e-02,  2.0947e-02,  8.7514e-03, -7.2416e-02, -3.8315e-02,\n          9.5320e-03,  4.9152e-02, -9.4876e-03,  3.5771e-02, -5.8691e-02,\n          8.3251e-02, -7.2837e-03,  7.8877e-02, -4.9681e-02, -1.2499e-02,\n          4.7135e-02, -8.6774e-02,  2.2584e-02, -4.8320e-02,  7.4247e-02,\n         -7.3219e-02,  9.1808e-03, -8.0097e-02, -8.3336e-02, -7.0606e-02,\n         -5.8767e-02, -2.6855e-02, -6.7725e-02, -1.2140e-02, -5.3266e-02,\n         -1.4966e-03, -3.3690e-02,  7.7675e-02,  3.1348e-02,  7.7088e-02,\n         -2.3654e-02,  6.6357e-03, -3.6196e-02, -7.6533e-02,  5.2672e-02,\n          3.0410e-02,  4.7519e-03,  5.1248e-02],\n        [ 1.0303e-02,  1.0959e-03,  8.1429e-02, -3.0932e-02, -4.5337e-02,\n          4.6520e-02,  5.0508e-02,  1.2146e-02,  1.8098e-02, -5.4333e-02,\n          8.5414e-02, -7.9053e-02, -6.4146e-02,  7.7450e-02,  5.9051e-02,\n         -7.3816e-02,  8.2212e-02, -2.0481e-02,  1.3469e-02,  1.3817e-02,\n          7.0116e-03, -9.0355e-03, -4.6154e-02,  7.0988e-02,  8.1004e-02,\n         -6.6334e-02, -7.2332e-02,  1.3510e-02,  4.0300e-02, -3.3146e-02,\n         -3.0151e-03,  5.6716e-02, -6.3293e-03, -7.9890e-02, -3.5448e-02,\n         -6.2415e-03,  4.9207e-02,  5.0099e-02, -5.4465e-02, -4.4522e-03,\n         -1.2661e-02,  2.8087e-02, -8.3376e-02,  5.0107e-02,  3.9991e-02,\n         -3.1152e-02, -1.8426e-02,  2.2324e-02,  5.1961e-02,  7.2718e-02,\n         -6.4952e-02,  1.9245e-02,  7.9789e-02, -3.9123e-02,  7.0532e-02,\n          2.8394e-03,  8.5256e-02, -7.0627e-02, -6.1435e-02,  4.8366e-03,\n          9.8573e-03,  2.3576e-02, -7.8728e-02, -7.8082e-02,  1.4346e-02,\n         -1.7262e-02,  1.9094e-02,  7.3734e-03,  7.6479e-02, -8.0038e-02,\n          5.8443e-02, -1.1920e-02, -3.0942e-02,  8.7420e-02,  7.6617e-02,\n         -1.3037e-02, -3.1721e-02, -7.1686e-02,  3.9247e-02,  4.8933e-02,\n         -8.3314e-02,  8.6184e-04,  2.4235e-02,  1.6603e-02, -1.0038e-02,\n         -3.9688e-02, -2.6695e-02,  4.7160e-02,  7.1831e-02,  6.0465e-02,\n         -1.1372e-02,  6.7173e-02,  4.2621e-02,  1.3010e-02,  1.3489e-02,\n          5.2432e-02, -4.9152e-02, -4.7967e-02, -3.0887e-03, -1.3011e-02,\n          3.3312e-02, -5.3654e-02, -1.7918e-02, -7.0364e-02,  3.7960e-02,\n          6.3098e-02,  1.2020e-03, -3.1569e-02,  2.0134e-03, -1.9177e-02,\n         -3.9396e-02, -2.4026e-02,  7.8012e-02, -3.3891e-02,  7.0627e-02,\n         -2.0951e-02,  2.1273e-02, -8.6965e-02, -7.6009e-02,  7.8396e-02,\n          3.7014e-02,  7.6566e-02, -2.6199e-02,  6.6038e-02, -3.4198e-02,\n          6.8122e-02, -8.6748e-02, -2.6473e-02]], requires_grad=True)",
                                "Parameter containing:\ntensor([-0.0714,  0.0872, -0.0449, -0.0147], requires_grad=True)"
                            ],
                            "weight_decay":	0
                        }
                    ],
                    "state":	{}
                }
            },
            "_replay_buffer":	{
                "<algorithms.DQN.replay_buffer.EnhancedReplayBuffer object at 0x7638ca7c6e50>":	{
                    "act_buf":	"[0 0 0 ... 0 0 0]",
                    "alpha":	0.6,
                    "aux_buf_size":	5000,
                    "beta":	0.4,
                    "beta_increment_per_sampling":	0.0001,
                    "buf_size":	50000,
                    "done_buf":	"[False False False ... False False False]",
                    "epsilon":	-1,
                    "epsilon_per_priority":	1e-06,
                    "gamma":	5,
                    "markers":	"[False False False ... False False False]",
                    "mask_buf":	"[[0. 0.]\n [0. 0.]\n [0. 0.]\n ...\n [0. 0.]\n [0. 0.]\n [0. 0.]]",
                    "mask_dim":	2,
                    "max_age":	20000,
                    "next_obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "num_segments":	0,
                    "obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "obs_dim":	8,
                    "ptr":	0,
                    "rew_buf":	"[0. 0. 0. ... 0. 0. 0.]",
                    "sample_decay":	0.5,
                    "segment_size":	62499.0,
                    "step":	0,
                    "sum_tree":	{
                        "<algorithms.DQN.replay_buffer.SumTree object at 0x763850fb0850>":	{
                            "capacity":	5000,
                            "data":	"[0 0 0 ... 0 0 0]",
                            "n_entries":	0,
                            "tree":	"[0. 0. 0. ... 0. 0. 0.]",
                            "write":	0
                        }
                    },
                    "timestamps":	"[0 0 0 ... 0 0 0]"
                }
            },
            "_target_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.0005,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.1924, -0.0533,  0.0887, -0.0676,  0.2959,  0.1120,  0.2640,  0.1382,\n         0.1973, -0.1865, -0.3505, -0.2749,  0.0081, -0.1239,  0.1859,  0.2404,\n         0.0160,  0.3455, -0.2832,  0.1566, -0.3258,  0.3173, -0.1862,  0.0005,\n         0.3285,  0.0279, -0.2884,  0.1908,  0.2919, -0.1489, -0.0123, -0.1180,\n         0.2004,  0.1597,  0.3417, -0.0422, -0.0754,  0.0773, -0.0086,  0.2963,\n        -0.1290,  0.1676, -0.2846,  0.2935, -0.1741,  0.2592,  0.0671,  0.0697,\n         0.0835, -0.1406,  0.1019, -0.2192, -0.3114, -0.2820,  0.1603, -0.3477,\n        -0.0969,  0.3191,  0.0106,  0.0765, -0.1737,  0.2558,  0.0297, -0.2465,\n        -0.3005,  0.2634, -0.0465,  0.1511,  0.0051, -0.0381,  0.3213, -0.1876,\n        -0.1717,  0.3471,  0.0935, -0.2827,  0.3118,  0.0297,  0.0135, -0.1464,\n         0.3382, -0.3530, -0.2932, -0.1962, -0.2487,  0.2430,  0.3530,  0.1568,\n         0.0028, -0.1887, -0.3046, -0.1054,  0.0669, -0.1895, -0.1485, -0.2559,\n         0.3268,  0.0904,  0.1878,  0.1802, -0.1858, -0.0205,  0.1060,  0.0368,\n         0.2469,  0.1006, -0.2951, -0.2569,  0.1465,  0.0789,  0.0320,  0.3381,\n        -0.2869, -0.3457, -0.2947, -0.0963,  0.2463, -0.3152,  0.0104,  0.1033,\n        -0.3145,  0.2242,  0.3132,  0.1787, -0.0406, -0.1313,  0.2184, -0.2954],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.2212, -0.3346,  0.0987,  ...,  0.1467, -0.0632,  0.1522],\n        [ 0.2313,  0.0962,  0.1838,  ..., -0.0407, -0.2786, -0.2585],\n        [ 0.0599,  0.1724, -0.2844,  ..., -0.1463,  0.1246,  0.1424],\n        ...,\n        [ 0.0461,  0.0812, -0.2347,  ..., -0.3334, -0.0809,  0.1949],\n        [-0.1340, -0.0949,  0.1630,  ...,  0.2549, -0.0329,  0.1628],\n        [-0.2046,  0.1137,  0.3316,  ...,  0.0808, -0.1168, -0.1719]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	128,
                                "training":	false
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=128, out_features=128, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0719,  0.0669, -0.0320,  0.0657,  0.0602,  0.0123,  0.0290,  0.0117,\n        -0.0286, -0.0461,  0.0522,  0.0397,  0.0268, -0.0303,  0.0615,  0.0729,\n        -0.0846,  0.0388,  0.0345,  0.0787, -0.0314,  0.0243, -0.0315, -0.0399,\n         0.0874,  0.0812, -0.0015, -0.0648,  0.0313,  0.0778, -0.0076,  0.0180,\n         0.0811, -0.0728,  0.0495,  0.0563,  0.0716,  0.0059,  0.0601,  0.0470,\n         0.0090,  0.0232,  0.0563,  0.0666, -0.0011,  0.0432,  0.0151,  0.0590,\n        -0.0775, -0.0506,  0.0624,  0.0369,  0.0031, -0.0732, -0.0354, -0.0431,\n         0.0845,  0.0092, -0.0300, -0.0601,  0.0028,  0.0256,  0.0067, -0.0701,\n        -0.0092,  0.0050,  0.0406, -0.0258, -0.0273, -0.0007,  0.0746,  0.0541,\n         0.0554,  0.0570, -0.0720,  0.0772, -0.0354,  0.0560,  0.0666,  0.0313,\n         0.0758, -0.0425, -0.0473, -0.0584,  0.0284,  0.0306,  0.0136,  0.0557,\n        -0.0158,  0.0372, -0.0700,  0.0368, -0.0293,  0.0567, -0.0480,  0.0349,\n        -0.0600, -0.0337, -0.0536,  0.0735,  0.0633, -0.0242,  0.0723, -0.0387,\n         0.0449, -0.0131,  0.0541, -0.0650,  0.0228,  0.0335, -0.0834,  0.0718,\n        -0.0659, -0.0200,  0.0482, -0.0317, -0.0517,  0.0534,  0.0525, -0.0567,\n         0.0222,  0.0389, -0.0301, -0.0663, -0.0695, -0.0194, -0.0180,  0.0259],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0496, -0.0094,  0.0311,  ...,  0.0859, -0.0079, -0.0755],\n        [ 0.0281,  0.0803,  0.0540,  ...,  0.0205,  0.0848, -0.0016],\n        [ 0.0200, -0.0300, -0.0315,  ...,  0.0547, -0.0553, -0.0839],\n        ...,\n        [ 0.0421, -0.0527, -0.0524,  ..., -0.0257,  0.0666, -0.0486],\n        [ 0.0764,  0.0379, -0.0518,  ..., -0.0781, -0.0569,  0.0782],\n        [-0.0689,  0.0558,  0.0544,  ...,  0.0641, -0.0489,  0.0785]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	128,
                                "training":	false
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=128, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([-0.0714,  0.0872, -0.0449, -0.0147], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 5.7855e-02,  7.5944e-02,  2.6384e-02, -3.5221e-03, -2.4096e-02,\n          3.5929e-02,  3.7899e-02, -9.2941e-04, -8.8946e-03, -2.9219e-02,\n         -6.8688e-02,  3.4321e-02,  6.7837e-02,  6.2045e-02,  8.6615e-02,\n         -8.1510e-02,  4.9966e-02,  1.6088e-02, -2.0411e-02,  4.2523e-02,\n          3.8433e-02,  8.1274e-02,  7.3628e-03,  4.2666e-03, -3.8230e-02,\n          2.2837e-02, -6.6867e-02, -4.4497e-02,  1.4538e-02, -2.9611e-02,\n         -2.5763e-02, -3.4694e-02, -8.5100e-03,  1.4865e-02,  8.4680e-02,\n         -6.0299e-02,  8.5599e-02, -7.3752e-02,  4.8247e-02,  3.5584e-02,\n          8.2375e-02,  8.1858e-02,  4.9683e-02, -5.0234e-03, -7.8617e-02,\n          2.2104e-04, -5.3499e-02,  2.7322e-02,  6.7154e-03, -8.7106e-02,\n          3.3618e-03, -1.1417e-03,  4.7043e-02, -8.5033e-02,  2.3386e-02,\n          6.9752e-02,  3.6687e-02, -8.3620e-02, -2.8391e-02,  5.7748e-03,\n          3.3411e-02,  6.7318e-02,  6.3145e-02,  7.3475e-04, -8.3524e-02,\n          6.1237e-02, -2.3323e-02, -4.9248e-02,  3.9414e-02,  4.5778e-02,\n         -4.9679e-02, -1.9097e-02,  3.6497e-02, -7.1682e-02,  7.7081e-02,\n         -7.8758e-02,  6.4863e-02, -3.5411e-02, -6.8231e-02, -4.7769e-03,\n         -3.7045e-02,  5.0847e-02,  5.6248e-02,  7.0137e-02,  2.9944e-02,\n          6.9431e-02, -4.4679e-02, -6.1426e-02,  1.0178e-02,  1.1449e-02,\n          8.5036e-02,  7.4453e-02, -1.7451e-02, -7.0812e-02, -2.4337e-02,\n         -4.4722e-02, -1.3843e-02,  2.3662e-02, -1.5513e-02, -1.2344e-02,\n         -2.1728e-02,  3.9071e-02,  6.4617e-02, -5.6264e-02, -5.6918e-02,\n          3.4827e-02, -2.0144e-02, -1.2497e-02, -7.6052e-02,  1.6168e-02,\n         -1.1592e-02,  2.6295e-02,  4.5777e-02,  2.8969e-02,  1.3651e-02,\n         -7.6056e-02,  5.9833e-02, -2.7907e-02,  4.9745e-02,  7.1970e-02,\n         -7.0168e-02, -2.6591e-02, -2.5789e-02,  3.0210e-02, -5.3694e-02,\n          5.8600e-02, -4.2357e-02,  8.1808e-02],\n        [-5.7501e-02,  8.3956e-02, -1.0111e-02,  4.3345e-02, -3.8940e-02,\n         -5.0733e-02,  7.0146e-02,  3.3893e-02,  4.4127e-02, -4.0749e-02,\n         -3.6548e-02, -3.3075e-02,  1.9832e-02, -5.6276e-02, -4.4365e-02,\n          5.8858e-02,  7.6887e-02, -1.0083e-02, -5.5487e-02,  3.0828e-02,\n         -3.1111e-02, -8.0169e-02,  7.9747e-02, -5.8241e-02,  1.0961e-02,\n         -7.8072e-02,  6.1405e-02,  3.4071e-02,  6.3971e-02, -6.9702e-02,\n          3.4932e-02,  8.1816e-02,  4.2378e-03,  3.5950e-03,  5.0788e-02,\n          6.9815e-02, -3.8784e-02,  1.0034e-02,  5.5475e-02,  5.6493e-02,\n          5.0452e-02, -1.4354e-02, -4.7940e-02,  1.9216e-02,  5.1810e-02,\n          5.9970e-02,  7.9068e-02, -3.8850e-02, -4.6821e-02,  3.6293e-02,\n          7.8876e-02,  5.5447e-02,  5.7711e-02, -4.6652e-02,  6.1035e-02,\n          5.1166e-02,  1.1356e-02,  4.6169e-02,  4.0746e-02,  3.3056e-02,\n          7.0106e-02,  6.0899e-02,  4.8373e-02,  7.8386e-02, -6.1761e-03,\n          1.8544e-02, -6.2060e-02, -4.9252e-02,  7.2731e-02, -5.9601e-02,\n          5.1682e-03,  7.8067e-02,  4.0465e-02, -7.9011e-02,  7.4312e-02,\n         -8.3315e-02, -1.2469e-02, -4.6414e-02, -6.6241e-02,  1.8206e-02,\n          5.8749e-02, -2.1096e-03, -8.4165e-02, -1.5342e-03, -4.9821e-02,\n         -5.7168e-02,  2.4452e-02,  4.8981e-02,  2.6851e-02, -5.1581e-02,\n         -1.0621e-02, -6.6054e-02,  3.0077e-02,  7.3994e-02, -5.7439e-02,\n         -7.0681e-02,  4.0542e-02,  5.8319e-02,  2.6144e-02,  5.7717e-02,\n          8.3111e-02,  8.2547e-02, -8.3106e-02, -8.6628e-02,  6.3539e-02,\n          1.5341e-02,  8.4786e-02,  8.1011e-02,  5.6094e-02, -3.6335e-02,\n          1.1515e-02,  3.2165e-02, -1.8057e-02, -3.3251e-02,  6.3797e-02,\n         -5.4827e-02,  3.9077e-02, -5.7944e-02, -5.0117e-02, -7.9332e-03,\n         -5.3958e-02,  6.9981e-02,  5.6276e-02, -4.8507e-02, -1.8171e-03,\n         -3.7489e-02, -1.0545e-02, -4.9181e-02],\n        [-1.6435e-02, -1.6272e-02, -3.2369e-02, -3.0775e-02,  1.2978e-02,\n          5.6644e-02, -7.3635e-02, -4.0464e-02, -5.2371e-03,  5.8262e-02,\n          3.7533e-02, -1.7887e-02,  3.1547e-02, -3.3615e-02, -8.6575e-02,\n         -2.2505e-02,  2.3088e-02,  7.9269e-02, -4.3384e-02, -4.0179e-02,\n          5.9471e-02, -8.6865e-02, -5.4699e-02, -6.0340e-02, -7.5754e-02,\n          6.8685e-02, -4.5759e-02, -4.0610e-02, -6.4448e-02, -3.0387e-02,\n         -6.1922e-02, -2.5952e-02,  2.3787e-02, -5.7915e-02, -6.4031e-02,\n         -8.1659e-02,  2.2815e-02,  4.4292e-02, -5.6540e-02, -5.5088e-02,\n         -7.5135e-02,  8.7922e-02,  9.1040e-03, -3.9632e-02,  6.0752e-02,\n         -7.5014e-02,  4.2068e-02,  1.1740e-02, -6.9654e-02,  8.7813e-02,\n         -6.4578e-02, -4.2667e-03,  7.1869e-02,  1.3925e-02, -4.9461e-02,\n         -4.0898e-02,  4.0307e-02,  6.2367e-02,  5.7688e-02, -7.6545e-02,\n          2.6817e-02,  4.2011e-02, -6.3829e-03,  8.4220e-02, -3.5888e-02,\n          5.3810e-02,  7.2898e-02, -3.3375e-02, -1.0893e-02, -5.3338e-02,\n          1.3620e-03,  4.4781e-02, -4.7096e-02, -3.6536e-02,  9.3670e-03,\n         -6.3091e-02,  2.6412e-02,  4.3533e-02, -4.9652e-02,  4.6034e-02,\n          2.2654e-06,  1.4654e-02, -2.3964e-02, -1.0549e-02, -8.1124e-02,\n          3.8936e-02,  2.0947e-02,  8.7514e-03, -7.2416e-02, -3.8315e-02,\n          9.5320e-03,  4.9152e-02, -9.4876e-03,  3.5771e-02, -5.8691e-02,\n          8.3251e-02, -7.2837e-03,  7.8877e-02, -4.9681e-02, -1.2499e-02,\n          4.7135e-02, -8.6774e-02,  2.2584e-02, -4.8320e-02,  7.4247e-02,\n         -7.3219e-02,  9.1808e-03, -8.0097e-02, -8.3336e-02, -7.0606e-02,\n         -5.8767e-02, -2.6855e-02, -6.7725e-02, -1.2140e-02, -5.3266e-02,\n         -1.4966e-03, -3.3690e-02,  7.7675e-02,  3.1348e-02,  7.7088e-02,\n         -2.3654e-02,  6.6357e-03, -3.6196e-02, -7.6533e-02,  5.2672e-02,\n          3.0410e-02,  4.7519e-03,  5.1248e-02],\n        [ 1.0303e-02,  1.0959e-03,  8.1429e-02, -3.0932e-02, -4.5337e-02,\n          4.6520e-02,  5.0508e-02,  1.2146e-02,  1.8098e-02, -5.4333e-02,\n          8.5414e-02, -7.9053e-02, -6.4146e-02,  7.7450e-02,  5.9051e-02,\n         -7.3816e-02,  8.2212e-02, -2.0481e-02,  1.3469e-02,  1.3817e-02,\n          7.0116e-03, -9.0355e-03, -4.6154e-02,  7.0988e-02,  8.1004e-02,\n         -6.6334e-02, -7.2332e-02,  1.3510e-02,  4.0300e-02, -3.3146e-02,\n         -3.0151e-03,  5.6716e-02, -6.3293e-03, -7.9890e-02, -3.5448e-02,\n         -6.2415e-03,  4.9207e-02,  5.0099e-02, -5.4465e-02, -4.4522e-03,\n         -1.2661e-02,  2.8087e-02, -8.3376e-02,  5.0107e-02,  3.9991e-02,\n         -3.1152e-02, -1.8426e-02,  2.2324e-02,  5.1961e-02,  7.2718e-02,\n         -6.4952e-02,  1.9245e-02,  7.9789e-02, -3.9123e-02,  7.0532e-02,\n          2.8394e-03,  8.5256e-02, -7.0627e-02, -6.1435e-02,  4.8366e-03,\n          9.8573e-03,  2.3576e-02, -7.8728e-02, -7.8082e-02,  1.4346e-02,\n         -1.7262e-02,  1.9094e-02,  7.3734e-03,  7.6479e-02, -8.0038e-02,\n          5.8443e-02, -1.1920e-02, -3.0942e-02,  8.7420e-02,  7.6617e-02,\n         -1.3037e-02, -3.1721e-02, -7.1686e-02,  3.9247e-02,  4.8933e-02,\n         -8.3314e-02,  8.6184e-04,  2.4235e-02,  1.6603e-02, -1.0038e-02,\n         -3.9688e-02, -2.6695e-02,  4.7160e-02,  7.1831e-02,  6.0465e-02,\n         -1.1372e-02,  6.7173e-02,  4.2621e-02,  1.3010e-02,  1.3489e-02,\n          5.2432e-02, -4.9152e-02, -4.7967e-02, -3.0887e-03, -1.3011e-02,\n          3.3312e-02, -5.3654e-02, -1.7918e-02, -7.0364e-02,  3.7960e-02,\n          6.3098e-02,  1.2020e-03, -3.1569e-02,  2.0134e-03, -1.9177e-02,\n         -3.9396e-02, -2.4026e-02,  7.8012e-02, -3.3891e-02,  7.0627e-02,\n         -2.0951e-02,  2.1273e-02, -8.6965e-02, -7.6009e-02,  7.8396e-02,\n          3.7014e-02,  7.6566e-02, -2.6199e-02,  6.6038e-02, -3.4198e-02,\n          6.8122e-02, -8.6748e-02, -2.6473e-02]], requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	128,
                                "out_features":	4,
                                "training":	false
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	false
                }
            },
            "_target_net_update_freq":	100,
            "_train_q_iters":	80,
            "_train_update_freq":	3,
            "_traj_per_epoch":	6,
            "logger":	{
                "<utils.logger.EpochLogger object at 0x76384ebeb490>":	{
                    "epoch_dict":	{},
                    "exp_name":	"rl4sys-dqn-info",
                    "first_row":	true,
                    "log_current_row":	{},
                    "log_headers":	[],
                    "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s535180000",
                    "output_file":	{
                        "<_io.TextIOWrapper name='/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s535180000/progress.txt' mode='w' encoding='UTF-8'>":	{
                            "mode":	"w"
                        }
                    }
                }
            }
        }
    },
    "target_net_update_freq":	100,
    "train_q_iters":	80,
    "train_update_freq":	3,
    "traj_per_epoch":	6
}