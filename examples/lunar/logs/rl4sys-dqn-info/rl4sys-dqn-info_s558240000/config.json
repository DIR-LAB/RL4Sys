{
    "__class__":	"DQN",
    "act_dim":	4,
    "batch_size":	64,
    "buf_size":	50000,
    "env_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar",
    "epsilon":	1.0,
    "epsilon_decay":	0.01,
    "epsilon_min":	0.01,
    "exp_name":	"rl4sys-dqn-info",
    "gamma":	0.99,
    "kernel_dim":	4,
    "kernel_size":	2,
    "log_data_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/",
    "logger_kwargs":	{
        "exp_name":	"rl4sys-dqn-info",
        "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s558240000"
    },
    "q_lr":	0.003,
    "seed":	558240000,
    "self":	{
        "<algorithms.DQN.DQN.DQN object at 0x7d04c4752990>":	{
            "_act_dim":	4,
            "_batch_size":	64,
            "_buf_size":	50000,
            "_epsilon":	1.0,
            "_epsilon_decay":	0.01,
            "_epsilon_min":	0.01,
            "_gamma":	0.99,
            "_kernel_dim":	4,
            "_kernel_size":	2,
            "_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=400, bias=True)\n  (fc2): Linear(in_features=400, out_features=300, bias=True)\n  (fc3): Linear(in_features=300, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.01,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=400, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 1.6928e-01,  3.2521e-01, -2.6988e-02, -1.7304e-01,  2.3175e-01,\n        -1.4957e-01, -2.3459e-01,  1.3969e-01,  1.5298e-01,  8.4966e-02,\n        -3.5250e-01,  2.9814e-01, -2.9054e-01, -3.0610e-01, -2.8172e-01,\n         1.7564e-01, -1.2834e-01,  2.0284e-01,  1.4204e-01,  2.4222e-01,\n        -2.4576e-01, -5.4914e-02,  3.4916e-01, -2.4841e-01, -2.8597e-01,\n         1.0110e-03,  3.5018e-01,  2.6042e-01,  2.9367e-01,  1.4308e-01,\n         9.9545e-02, -7.7350e-02, -2.8926e-01, -2.5428e-01,  2.9726e-02,\n         2.5020e-01, -2.7427e-01,  3.1036e-01,  2.9712e-01, -4.2161e-02,\n         1.2566e-01,  1.5209e-01, -2.6807e-01,  3.3019e-01, -1.3468e-01,\n        -3.3487e-01, -2.4819e-01, -2.6656e-01, -2.4615e-01,  1.2741e-01,\n        -2.9459e-01,  2.4572e-01, -5.9275e-02, -1.4177e-01,  2.5381e-01,\n         1.3958e-01, -4.2778e-02,  5.8254e-02,  6.0866e-02,  2.2249e-01,\n         2.5223e-02,  2.4261e-01,  1.0293e-01,  2.6550e-01,  3.8835e-02,\n        -6.2035e-02, -9.6507e-02, -1.2906e-01, -1.0433e-01,  3.1201e-01,\n        -1.3122e-02, -9.3311e-02,  3.4098e-01,  1.6960e-02, -3.4212e-01,\n        -1.5547e-01,  3.1246e-01, -8.0172e-02,  2.1996e-02,  3.3319e-01,\n         1.8682e-01,  2.2537e-01, -2.5453e-01, -1.2110e-01,  3.1096e-01,\n         3.0950e-01,  2.9394e-01,  1.4250e-01,  2.7402e-01,  2.9193e-01,\n        -1.8581e-01, -3.1854e-01, -1.7568e-01, -2.1451e-01,  2.6626e-01,\n        -2.4289e-01, -2.4868e-01, -3.4445e-01,  2.2485e-01, -4.2598e-02,\n        -4.9565e-02,  8.1915e-02, -1.3994e-01, -1.4421e-01, -2.4371e-01,\n         3.5237e-01, -1.3067e-01, -1.5284e-01, -1.2086e-01,  1.1913e-01,\n         1.5677e-01, -5.1693e-02,  2.4563e-01, -6.8017e-02,  2.4425e-02,\n         1.7851e-01, -4.0518e-02, -4.7387e-02, -1.5696e-01, -1.4019e-01,\n        -3.3476e-01, -1.3228e-01, -1.4319e-01, -2.8882e-01,  2.0463e-01,\n         2.2407e-01,  3.4451e-01,  1.3074e-01, -3.1364e-01, -2.1335e-01,\n        -3.4141e-01, -2.4677e-01,  3.3416e-01,  1.4679e-01,  1.7834e-01,\n        -1.7045e-01, -2.3320e-01,  1.0449e-01,  1.6678e-01, -2.1656e-01,\n         1.8789e-02,  3.2583e-01, -2.2201e-01,  1.3676e-01,  1.1755e-04,\n        -1.1769e-02,  3.2639e-01,  2.9008e-01, -2.1403e-01, -8.1160e-03,\n        -1.5283e-01, -2.4495e-01,  5.0625e-02, -3.4880e-01,  1.9838e-01,\n         1.9542e-01,  1.5277e-01,  5.0640e-02,  8.4793e-02,  9.8116e-02,\n         2.3825e-01, -4.2964e-02,  6.6475e-02, -1.6235e-01,  1.1251e-01,\n         1.4428e-01,  2.5663e-01,  2.0861e-01,  3.1427e-01,  2.8165e-01,\n        -1.2991e-01, -1.2189e-01,  2.0899e-01,  1.2470e-01, -3.5257e-01,\n         2.4260e-02, -2.6441e-01,  1.7329e-01, -9.5292e-02, -3.5421e-02,\n         2.9934e-01,  5.7577e-02,  1.8596e-01,  3.4463e-01,  7.4980e-02,\n         3.2252e-01, -1.3257e-01, -4.3122e-02,  1.0286e-01, -1.1545e-01,\n        -2.6279e-01, -3.0700e-01,  2.3594e-01,  8.1867e-02, -3.4007e-01,\n        -2.0441e-01, -5.0201e-02,  2.6533e-01, -2.5679e-01,  8.7034e-02,\n         3.0536e-01, -4.2287e-02, -2.4048e-01, -3.0230e-01,  2.9463e-01,\n         2.7325e-01, -1.3004e-01,  5.6275e-02,  1.7031e-01,  2.9789e-01,\n        -2.0214e-01,  1.0431e-01,  3.4247e-01,  6.5520e-02,  3.4761e-01,\n        -9.9071e-02, -6.9092e-02, -1.9942e-01, -2.2113e-01,  2.5984e-01,\n        -1.6938e-01,  9.7861e-02,  1.4865e-01, -3.2608e-01, -5.2628e-02,\n        -1.8198e-01,  1.8704e-01,  2.9894e-01,  2.2021e-02, -1.9153e-01,\n         2.2494e-01,  1.8812e-01,  1.9619e-01, -4.1992e-02,  1.2357e-01,\n         3.0500e-01, -3.3782e-02, -6.2357e-02,  1.6575e-01, -1.7271e-01,\n         1.1159e-01, -2.1975e-01, -2.3268e-01, -2.8421e-01, -3.0847e-01,\n        -3.3837e-01,  2.6264e-01,  6.8359e-02, -1.0479e-01,  1.4060e-01,\n         1.6063e-01,  2.4696e-02, -1.1584e-01,  2.4132e-01,  7.8921e-02,\n        -2.4671e-01,  3.4075e-01,  2.2351e-02, -2.4339e-01,  1.5629e-01,\n         4.4455e-02,  5.3772e-02,  1.7742e-01, -1.4985e-01, -1.7545e-01,\n         5.5184e-02,  3.3734e-01,  1.4197e-01, -3.3746e-01,  1.0503e-01,\n         6.5678e-02,  9.1120e-02,  2.6869e-01,  2.7420e-02,  1.0892e-01,\n        -2.2925e-01, -3.0821e-01,  2.0573e-01, -3.0960e-02,  1.4246e-02,\n         1.0740e-01,  3.4207e-01,  5.8712e-02,  9.9604e-02, -2.9282e-01,\n        -3.0780e-01, -1.4563e-01, -1.8584e-01, -2.6728e-01, -3.4431e-01,\n         2.6736e-01,  2.7774e-01, -1.1257e-01, -9.4208e-02,  3.5336e-01,\n        -8.7333e-02, -1.8085e-01,  5.4745e-03, -1.1620e-03, -1.9392e-01,\n        -1.1474e-01, -5.5521e-02, -2.2608e-01,  2.3834e-01,  1.3473e-01,\n         5.2200e-02, -3.1412e-01,  2.7282e-01,  1.0593e-01,  2.1800e-01,\n         1.3150e-01,  3.1345e-01,  2.0129e-01, -2.0885e-01, -3.3780e-01,\n        -9.0494e-02, -1.8191e-01, -3.1239e-01, -1.1454e-01, -1.5464e-01,\n        -5.7015e-02,  4.5356e-03, -1.9579e-01, -3.1309e-01, -2.3786e-01,\n         2.3500e-02, -6.5720e-02,  8.6583e-02,  6.4685e-02,  1.1707e-01,\n        -5.0499e-02,  2.5837e-01, -3.2634e-01,  1.9375e-01, -3.1607e-01,\n        -1.1657e-01,  1.1293e-01,  3.7859e-03, -5.8765e-02,  7.5232e-02,\n        -1.2129e-01,  3.2567e-01,  6.8804e-02,  1.9635e-01, -7.2018e-02,\n        -3.2684e-01,  4.0576e-02, -3.1593e-01, -5.5133e-03, -1.5265e-01,\n        -3.5078e-01,  1.1759e-01, -2.2536e-01,  3.1866e-01,  9.0442e-02,\n         2.3809e-01, -1.2002e-01,  1.5672e-01, -2.5126e-01,  2.8008e-01,\n        -2.4784e-01, -7.1332e-02,  5.4203e-03, -3.0832e-02,  6.3752e-02,\n        -3.2637e-01,  1.7580e-01, -4.3077e-03, -3.3657e-01, -2.4818e-01,\n         1.3380e-01,  2.5523e-01,  5.5468e-02,  1.6040e-01, -2.7776e-01,\n        -1.9689e-01, -2.7001e-01,  2.9551e-02,  3.1044e-01, -1.1236e-01,\n         1.6855e-01,  2.4305e-01,  3.4123e-01, -1.9958e-01, -3.9364e-03,\n         2.7268e-01,  4.3291e-02,  2.8499e-01, -1.7999e-01, -2.7907e-01,\n        -4.1541e-02,  1.7641e-01, -2.6184e-01, -1.2728e-01,  1.6940e-01,\n        -3.0378e-01,  5.7271e-02, -2.2257e-01, -2.9773e-01,  9.8253e-02],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.2899,  0.2609, -0.1778,  ...,  0.2445,  0.0644,  0.0075],\n        [ 0.2931,  0.2139,  0.0261,  ..., -0.2397,  0.0223,  0.2035],\n        [-0.2688, -0.3201, -0.1898,  ..., -0.0008,  0.3423,  0.1924],\n        ...,\n        [-0.1723, -0.3075, -0.1593,  ...,  0.2345,  0.1131,  0.0038],\n        [ 0.1035,  0.2223, -0.0456,  ...,  0.1174, -0.0259,  0.1535],\n        [-0.1632, -0.2198,  0.0985,  ...,  0.1539, -0.1751,  0.2852]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	400,
                                "training":	true
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=400, out_features=300, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0341, -0.0237,  0.0112, -0.0159, -0.0140, -0.0341,  0.0371, -0.0392,\n         0.0056, -0.0077,  0.0449, -0.0140,  0.0017, -0.0444,  0.0027,  0.0288,\n         0.0480, -0.0131, -0.0368,  0.0245, -0.0433,  0.0283, -0.0016, -0.0386,\n        -0.0455,  0.0020,  0.0364,  0.0287,  0.0276, -0.0046,  0.0073, -0.0161,\n         0.0041,  0.0143, -0.0155, -0.0307, -0.0437, -0.0078, -0.0132, -0.0283,\n         0.0311,  0.0311,  0.0257,  0.0106,  0.0234,  0.0337, -0.0409,  0.0141,\n         0.0348,  0.0286, -0.0008,  0.0004,  0.0277, -0.0182, -0.0355, -0.0301,\n        -0.0388,  0.0285, -0.0443, -0.0379, -0.0079, -0.0321,  0.0253, -0.0410,\n         0.0261, -0.0419, -0.0360,  0.0352,  0.0436, -0.0352,  0.0283,  0.0382,\n        -0.0252,  0.0144, -0.0383,  0.0215,  0.0100, -0.0397, -0.0358, -0.0360,\n         0.0243, -0.0455,  0.0014, -0.0174,  0.0115, -0.0493, -0.0058, -0.0247,\n        -0.0371, -0.0445, -0.0352,  0.0349,  0.0400,  0.0436,  0.0299,  0.0134,\n        -0.0434, -0.0268,  0.0027,  0.0424,  0.0138,  0.0334, -0.0366, -0.0491,\n         0.0309, -0.0093, -0.0359, -0.0287,  0.0118, -0.0205,  0.0310,  0.0177,\n         0.0277, -0.0015, -0.0365,  0.0497, -0.0348, -0.0385,  0.0121, -0.0273,\n         0.0058, -0.0037,  0.0073, -0.0376, -0.0041,  0.0453,  0.0230, -0.0019,\n        -0.0133,  0.0307, -0.0371, -0.0430,  0.0196, -0.0474,  0.0216, -0.0359,\n         0.0223, -0.0080, -0.0364,  0.0421,  0.0343,  0.0302,  0.0004,  0.0241,\n         0.0020,  0.0377,  0.0114,  0.0340,  0.0063, -0.0286,  0.0309,  0.0342,\n         0.0207,  0.0240, -0.0409, -0.0045,  0.0495, -0.0058, -0.0048, -0.0439,\n        -0.0095,  0.0499,  0.0329, -0.0313,  0.0030,  0.0149,  0.0399,  0.0261,\n         0.0363, -0.0326, -0.0035, -0.0303, -0.0270,  0.0201,  0.0042,  0.0349,\n         0.0383, -0.0346, -0.0264,  0.0272,  0.0358, -0.0148,  0.0439, -0.0163,\n         0.0020,  0.0499, -0.0297, -0.0284, -0.0437,  0.0058,  0.0077,  0.0469,\n        -0.0476, -0.0459,  0.0054,  0.0231, -0.0439,  0.0336,  0.0136, -0.0422,\n        -0.0142,  0.0071, -0.0285,  0.0368,  0.0235,  0.0066,  0.0159,  0.0307,\n         0.0102,  0.0265, -0.0281, -0.0098,  0.0486, -0.0325,  0.0126,  0.0295,\n         0.0494,  0.0389, -0.0438, -0.0078, -0.0248, -0.0473, -0.0411, -0.0092,\n        -0.0325, -0.0294,  0.0169,  0.0316, -0.0292, -0.0089, -0.0035,  0.0310,\n         0.0388, -0.0390,  0.0086,  0.0040, -0.0057,  0.0369,  0.0153,  0.0298,\n        -0.0380, -0.0391, -0.0498, -0.0498, -0.0064, -0.0338,  0.0269,  0.0009,\n        -0.0027,  0.0389, -0.0044,  0.0302, -0.0467, -0.0022, -0.0454, -0.0003,\n        -0.0377, -0.0346,  0.0380, -0.0214,  0.0226,  0.0476, -0.0261,  0.0070,\n         0.0218, -0.0404,  0.0401, -0.0005,  0.0366,  0.0387, -0.0403,  0.0295,\n         0.0098,  0.0376,  0.0015,  0.0092, -0.0395, -0.0306,  0.0225, -0.0477,\n        -0.0486, -0.0340, -0.0343,  0.0086, -0.0453,  0.0442,  0.0399,  0.0141,\n         0.0142,  0.0179, -0.0200, -0.0482,  0.0210,  0.0108, -0.0013, -0.0364,\n        -0.0231, -0.0076, -0.0109,  0.0363], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0422,  0.0012, -0.0289,  ..., -0.0115, -0.0313,  0.0162],\n        [-0.0362, -0.0411, -0.0301,  ..., -0.0034, -0.0305, -0.0038],\n        [-0.0028, -0.0113,  0.0158,  ..., -0.0056,  0.0417, -0.0394],\n        ...,\n        [-0.0292, -0.0143, -0.0189,  ..., -0.0126, -0.0332,  0.0351],\n        [ 0.0492,  0.0107,  0.0414,  ..., -0.0173, -0.0352, -0.0227],\n        [ 0.0134, -0.0095, -0.0372,  ...,  0.0239,  0.0253,  0.0111]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	400,
                                "out_features":	300,
                                "training":	true
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=300, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0174,  0.0310, -0.0070,  0.0326], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.0274, -0.0222,  0.0567,  ..., -0.0476,  0.0140,  0.0131],\n        [-0.0411,  0.0484, -0.0350,  ..., -0.0410,  0.0233,  0.0216],\n        [ 0.0566,  0.0047, -0.0133,  ..., -0.0523, -0.0151,  0.0166],\n        [ 0.0031,  0.0543, -0.0271,  ...,  0.0207, -0.0397, -0.0174]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	300,
                                "out_features":	4,
                                "training":	true
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	true
                }
            },
            "_q_optimizer":	{
                "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.003\n    maximize: False\n    weight_decay: 0\n)":	{
                    "_optimizer_load_state_dict_post_hooks":	{},
                    "_optimizer_load_state_dict_pre_hooks":	{},
                    "_optimizer_state_dict_post_hooks":	{},
                    "_optimizer_state_dict_pre_hooks":	{},
                    "_optimizer_step_post_hooks":	{},
                    "_optimizer_step_pre_hooks":	{},
                    "_warned_capturable_if_run_uncaptured":	true,
                    "_zero_grad_profile_name":	"Optimizer.zero_grad#Adam.zero_grad",
                    "defaults":	{
                        "amsgrad":	false,
                        "betas":	[
                            0.9,
                            0.999
                        ],
                        "capturable":	false,
                        "differentiable":	false,
                        "eps":	1e-08,
                        "foreach":	null,
                        "fused":	null,
                        "lr":	0.003,
                        "maximize":	false,
                        "weight_decay":	0
                    },
                    "param_groups":	[
                        {
                            "amsgrad":	false,
                            "betas":	[
                                0.9,
                                0.999
                            ],
                            "capturable":	false,
                            "differentiable":	false,
                            "eps":	1e-08,
                            "foreach":	null,
                            "fused":	null,
                            "lr":	0.003,
                            "maximize":	false,
                            "params":	[
                                "Parameter containing:\ntensor([[ 0.2899,  0.2609, -0.1778,  ...,  0.2445,  0.0644,  0.0075],\n        [ 0.2931,  0.2139,  0.0261,  ..., -0.2397,  0.0223,  0.2035],\n        [-0.2688, -0.3201, -0.1898,  ..., -0.0008,  0.3423,  0.1924],\n        ...,\n        [-0.1723, -0.3075, -0.1593,  ...,  0.2345,  0.1131,  0.0038],\n        [ 0.1035,  0.2223, -0.0456,  ...,  0.1174, -0.0259,  0.1535],\n        [-0.1632, -0.2198,  0.0985,  ...,  0.1539, -0.1751,  0.2852]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([ 1.6928e-01,  3.2521e-01, -2.6988e-02, -1.7304e-01,  2.3175e-01,\n        -1.4957e-01, -2.3459e-01,  1.3969e-01,  1.5298e-01,  8.4966e-02,\n        -3.5250e-01,  2.9814e-01, -2.9054e-01, -3.0610e-01, -2.8172e-01,\n         1.7564e-01, -1.2834e-01,  2.0284e-01,  1.4204e-01,  2.4222e-01,\n        -2.4576e-01, -5.4914e-02,  3.4916e-01, -2.4841e-01, -2.8597e-01,\n         1.0110e-03,  3.5018e-01,  2.6042e-01,  2.9367e-01,  1.4308e-01,\n         9.9545e-02, -7.7350e-02, -2.8926e-01, -2.5428e-01,  2.9726e-02,\n         2.5020e-01, -2.7427e-01,  3.1036e-01,  2.9712e-01, -4.2161e-02,\n         1.2566e-01,  1.5209e-01, -2.6807e-01,  3.3019e-01, -1.3468e-01,\n        -3.3487e-01, -2.4819e-01, -2.6656e-01, -2.4615e-01,  1.2741e-01,\n        -2.9459e-01,  2.4572e-01, -5.9275e-02, -1.4177e-01,  2.5381e-01,\n         1.3958e-01, -4.2778e-02,  5.8254e-02,  6.0866e-02,  2.2249e-01,\n         2.5223e-02,  2.4261e-01,  1.0293e-01,  2.6550e-01,  3.8835e-02,\n        -6.2035e-02, -9.6507e-02, -1.2906e-01, -1.0433e-01,  3.1201e-01,\n        -1.3122e-02, -9.3311e-02,  3.4098e-01,  1.6960e-02, -3.4212e-01,\n        -1.5547e-01,  3.1246e-01, -8.0172e-02,  2.1996e-02,  3.3319e-01,\n         1.8682e-01,  2.2537e-01, -2.5453e-01, -1.2110e-01,  3.1096e-01,\n         3.0950e-01,  2.9394e-01,  1.4250e-01,  2.7402e-01,  2.9193e-01,\n        -1.8581e-01, -3.1854e-01, -1.7568e-01, -2.1451e-01,  2.6626e-01,\n        -2.4289e-01, -2.4868e-01, -3.4445e-01,  2.2485e-01, -4.2598e-02,\n        -4.9565e-02,  8.1915e-02, -1.3994e-01, -1.4421e-01, -2.4371e-01,\n         3.5237e-01, -1.3067e-01, -1.5284e-01, -1.2086e-01,  1.1913e-01,\n         1.5677e-01, -5.1693e-02,  2.4563e-01, -6.8017e-02,  2.4425e-02,\n         1.7851e-01, -4.0518e-02, -4.7387e-02, -1.5696e-01, -1.4019e-01,\n        -3.3476e-01, -1.3228e-01, -1.4319e-01, -2.8882e-01,  2.0463e-01,\n         2.2407e-01,  3.4451e-01,  1.3074e-01, -3.1364e-01, -2.1335e-01,\n        -3.4141e-01, -2.4677e-01,  3.3416e-01,  1.4679e-01,  1.7834e-01,\n        -1.7045e-01, -2.3320e-01,  1.0449e-01,  1.6678e-01, -2.1656e-01,\n         1.8789e-02,  3.2583e-01, -2.2201e-01,  1.3676e-01,  1.1755e-04,\n        -1.1769e-02,  3.2639e-01,  2.9008e-01, -2.1403e-01, -8.1160e-03,\n        -1.5283e-01, -2.4495e-01,  5.0625e-02, -3.4880e-01,  1.9838e-01,\n         1.9542e-01,  1.5277e-01,  5.0640e-02,  8.4793e-02,  9.8116e-02,\n         2.3825e-01, -4.2964e-02,  6.6475e-02, -1.6235e-01,  1.1251e-01,\n         1.4428e-01,  2.5663e-01,  2.0861e-01,  3.1427e-01,  2.8165e-01,\n        -1.2991e-01, -1.2189e-01,  2.0899e-01,  1.2470e-01, -3.5257e-01,\n         2.4260e-02, -2.6441e-01,  1.7329e-01, -9.5292e-02, -3.5421e-02,\n         2.9934e-01,  5.7577e-02,  1.8596e-01,  3.4463e-01,  7.4980e-02,\n         3.2252e-01, -1.3257e-01, -4.3122e-02,  1.0286e-01, -1.1545e-01,\n        -2.6279e-01, -3.0700e-01,  2.3594e-01,  8.1867e-02, -3.4007e-01,\n        -2.0441e-01, -5.0201e-02,  2.6533e-01, -2.5679e-01,  8.7034e-02,\n         3.0536e-01, -4.2287e-02, -2.4048e-01, -3.0230e-01,  2.9463e-01,\n         2.7325e-01, -1.3004e-01,  5.6275e-02,  1.7031e-01,  2.9789e-01,\n        -2.0214e-01,  1.0431e-01,  3.4247e-01,  6.5520e-02,  3.4761e-01,\n        -9.9071e-02, -6.9092e-02, -1.9942e-01, -2.2113e-01,  2.5984e-01,\n        -1.6938e-01,  9.7861e-02,  1.4865e-01, -3.2608e-01, -5.2628e-02,\n        -1.8198e-01,  1.8704e-01,  2.9894e-01,  2.2021e-02, -1.9153e-01,\n         2.2494e-01,  1.8812e-01,  1.9619e-01, -4.1992e-02,  1.2357e-01,\n         3.0500e-01, -3.3782e-02, -6.2357e-02,  1.6575e-01, -1.7271e-01,\n         1.1159e-01, -2.1975e-01, -2.3268e-01, -2.8421e-01, -3.0847e-01,\n        -3.3837e-01,  2.6264e-01,  6.8359e-02, -1.0479e-01,  1.4060e-01,\n         1.6063e-01,  2.4696e-02, -1.1584e-01,  2.4132e-01,  7.8921e-02,\n        -2.4671e-01,  3.4075e-01,  2.2351e-02, -2.4339e-01,  1.5629e-01,\n         4.4455e-02,  5.3772e-02,  1.7742e-01, -1.4985e-01, -1.7545e-01,\n         5.5184e-02,  3.3734e-01,  1.4197e-01, -3.3746e-01,  1.0503e-01,\n         6.5678e-02,  9.1120e-02,  2.6869e-01,  2.7420e-02,  1.0892e-01,\n        -2.2925e-01, -3.0821e-01,  2.0573e-01, -3.0960e-02,  1.4246e-02,\n         1.0740e-01,  3.4207e-01,  5.8712e-02,  9.9604e-02, -2.9282e-01,\n        -3.0780e-01, -1.4563e-01, -1.8584e-01, -2.6728e-01, -3.4431e-01,\n         2.6736e-01,  2.7774e-01, -1.1257e-01, -9.4208e-02,  3.5336e-01,\n        -8.7333e-02, -1.8085e-01,  5.4745e-03, -1.1620e-03, -1.9392e-01,\n        -1.1474e-01, -5.5521e-02, -2.2608e-01,  2.3834e-01,  1.3473e-01,\n         5.2200e-02, -3.1412e-01,  2.7282e-01,  1.0593e-01,  2.1800e-01,\n         1.3150e-01,  3.1345e-01,  2.0129e-01, -2.0885e-01, -3.3780e-01,\n        -9.0494e-02, -1.8191e-01, -3.1239e-01, -1.1454e-01, -1.5464e-01,\n        -5.7015e-02,  4.5356e-03, -1.9579e-01, -3.1309e-01, -2.3786e-01,\n         2.3500e-02, -6.5720e-02,  8.6583e-02,  6.4685e-02,  1.1707e-01,\n        -5.0499e-02,  2.5837e-01, -3.2634e-01,  1.9375e-01, -3.1607e-01,\n        -1.1657e-01,  1.1293e-01,  3.7859e-03, -5.8765e-02,  7.5232e-02,\n        -1.2129e-01,  3.2567e-01,  6.8804e-02,  1.9635e-01, -7.2018e-02,\n        -3.2684e-01,  4.0576e-02, -3.1593e-01, -5.5133e-03, -1.5265e-01,\n        -3.5078e-01,  1.1759e-01, -2.2536e-01,  3.1866e-01,  9.0442e-02,\n         2.3809e-01, -1.2002e-01,  1.5672e-01, -2.5126e-01,  2.8008e-01,\n        -2.4784e-01, -7.1332e-02,  5.4203e-03, -3.0832e-02,  6.3752e-02,\n        -3.2637e-01,  1.7580e-01, -4.3077e-03, -3.3657e-01, -2.4818e-01,\n         1.3380e-01,  2.5523e-01,  5.5468e-02,  1.6040e-01, -2.7776e-01,\n        -1.9689e-01, -2.7001e-01,  2.9551e-02,  3.1044e-01, -1.1236e-01,\n         1.6855e-01,  2.4305e-01,  3.4123e-01, -1.9958e-01, -3.9364e-03,\n         2.7268e-01,  4.3291e-02,  2.8499e-01, -1.7999e-01, -2.7907e-01,\n        -4.1541e-02,  1.7641e-01, -2.6184e-01, -1.2728e-01,  1.6940e-01,\n        -3.0378e-01,  5.7271e-02, -2.2257e-01, -2.9773e-01,  9.8253e-02],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([[ 0.0422,  0.0012, -0.0289,  ..., -0.0115, -0.0313,  0.0162],\n        [-0.0362, -0.0411, -0.0301,  ..., -0.0034, -0.0305, -0.0038],\n        [-0.0028, -0.0113,  0.0158,  ..., -0.0056,  0.0417, -0.0394],\n        ...,\n        [-0.0292, -0.0143, -0.0189,  ..., -0.0126, -0.0332,  0.0351],\n        [ 0.0492,  0.0107,  0.0414,  ..., -0.0173, -0.0352, -0.0227],\n        [ 0.0134, -0.0095, -0.0372,  ...,  0.0239,  0.0253,  0.0111]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.0341, -0.0237,  0.0112, -0.0159, -0.0140, -0.0341,  0.0371, -0.0392,\n         0.0056, -0.0077,  0.0449, -0.0140,  0.0017, -0.0444,  0.0027,  0.0288,\n         0.0480, -0.0131, -0.0368,  0.0245, -0.0433,  0.0283, -0.0016, -0.0386,\n        -0.0455,  0.0020,  0.0364,  0.0287,  0.0276, -0.0046,  0.0073, -0.0161,\n         0.0041,  0.0143, -0.0155, -0.0307, -0.0437, -0.0078, -0.0132, -0.0283,\n         0.0311,  0.0311,  0.0257,  0.0106,  0.0234,  0.0337, -0.0409,  0.0141,\n         0.0348,  0.0286, -0.0008,  0.0004,  0.0277, -0.0182, -0.0355, -0.0301,\n        -0.0388,  0.0285, -0.0443, -0.0379, -0.0079, -0.0321,  0.0253, -0.0410,\n         0.0261, -0.0419, -0.0360,  0.0352,  0.0436, -0.0352,  0.0283,  0.0382,\n        -0.0252,  0.0144, -0.0383,  0.0215,  0.0100, -0.0397, -0.0358, -0.0360,\n         0.0243, -0.0455,  0.0014, -0.0174,  0.0115, -0.0493, -0.0058, -0.0247,\n        -0.0371, -0.0445, -0.0352,  0.0349,  0.0400,  0.0436,  0.0299,  0.0134,\n        -0.0434, -0.0268,  0.0027,  0.0424,  0.0138,  0.0334, -0.0366, -0.0491,\n         0.0309, -0.0093, -0.0359, -0.0287,  0.0118, -0.0205,  0.0310,  0.0177,\n         0.0277, -0.0015, -0.0365,  0.0497, -0.0348, -0.0385,  0.0121, -0.0273,\n         0.0058, -0.0037,  0.0073, -0.0376, -0.0041,  0.0453,  0.0230, -0.0019,\n        -0.0133,  0.0307, -0.0371, -0.0430,  0.0196, -0.0474,  0.0216, -0.0359,\n         0.0223, -0.0080, -0.0364,  0.0421,  0.0343,  0.0302,  0.0004,  0.0241,\n         0.0020,  0.0377,  0.0114,  0.0340,  0.0063, -0.0286,  0.0309,  0.0342,\n         0.0207,  0.0240, -0.0409, -0.0045,  0.0495, -0.0058, -0.0048, -0.0439,\n        -0.0095,  0.0499,  0.0329, -0.0313,  0.0030,  0.0149,  0.0399,  0.0261,\n         0.0363, -0.0326, -0.0035, -0.0303, -0.0270,  0.0201,  0.0042,  0.0349,\n         0.0383, -0.0346, -0.0264,  0.0272,  0.0358, -0.0148,  0.0439, -0.0163,\n         0.0020,  0.0499, -0.0297, -0.0284, -0.0437,  0.0058,  0.0077,  0.0469,\n        -0.0476, -0.0459,  0.0054,  0.0231, -0.0439,  0.0336,  0.0136, -0.0422,\n        -0.0142,  0.0071, -0.0285,  0.0368,  0.0235,  0.0066,  0.0159,  0.0307,\n         0.0102,  0.0265, -0.0281, -0.0098,  0.0486, -0.0325,  0.0126,  0.0295,\n         0.0494,  0.0389, -0.0438, -0.0078, -0.0248, -0.0473, -0.0411, -0.0092,\n        -0.0325, -0.0294,  0.0169,  0.0316, -0.0292, -0.0089, -0.0035,  0.0310,\n         0.0388, -0.0390,  0.0086,  0.0040, -0.0057,  0.0369,  0.0153,  0.0298,\n        -0.0380, -0.0391, -0.0498, -0.0498, -0.0064, -0.0338,  0.0269,  0.0009,\n        -0.0027,  0.0389, -0.0044,  0.0302, -0.0467, -0.0022, -0.0454, -0.0003,\n        -0.0377, -0.0346,  0.0380, -0.0214,  0.0226,  0.0476, -0.0261,  0.0070,\n         0.0218, -0.0404,  0.0401, -0.0005,  0.0366,  0.0387, -0.0403,  0.0295,\n         0.0098,  0.0376,  0.0015,  0.0092, -0.0395, -0.0306,  0.0225, -0.0477,\n        -0.0486, -0.0340, -0.0343,  0.0086, -0.0453,  0.0442,  0.0399,  0.0141,\n         0.0142,  0.0179, -0.0200, -0.0482,  0.0210,  0.0108, -0.0013, -0.0364,\n        -0.0231, -0.0076, -0.0109,  0.0363], requires_grad=True)",
                                "Parameter containing:\ntensor([[-0.0274, -0.0222,  0.0567,  ..., -0.0476,  0.0140,  0.0131],\n        [-0.0411,  0.0484, -0.0350,  ..., -0.0410,  0.0233,  0.0216],\n        [ 0.0566,  0.0047, -0.0133,  ..., -0.0523, -0.0151,  0.0166],\n        [ 0.0031,  0.0543, -0.0271,  ...,  0.0207, -0.0397, -0.0174]],\n       requires_grad=True)",
                                "Parameter containing:\ntensor([ 0.0174,  0.0310, -0.0070,  0.0326], requires_grad=True)"
                            ],
                            "weight_decay":	0
                        }
                    ],
                    "state":	{}
                }
            },
            "_replay_buffer":	{
                "<algorithms.DQN.replay_buffer.ReplayBuffer object at 0x7d04c4568250>":	{
                    "act_buf":	"[0 0 0 ... 0 0 0]",
                    "capacity":	50000,
                    "done_buf":	"[False False False ... False False False]",
                    "epsilon":	1.0,
                    "gamma":	0.99,
                    "mask_buf":	"[[0. 0.]\n [0. 0.]\n [0. 0.]\n ...\n [0. 0.]\n [0. 0.]\n [0. 0.]]",
                    "max_size":	50000,
                    "next_obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "obs_buf":	"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]",
                    "path_start_idx":	0,
                    "ptr":	0,
                    "ret_buf":	"[0. 0. 0. ... 0. 0. 0.]",
                    "rew_buf":	"[0. 0. 0. ... 0. 0. 0.]"
                }
            },
            "_target_model":	{
                "DeepQNetwork(\n  (fc1): Linear(in_features=8, out_features=400, bias=True)\n  (fc2): Linear(in_features=400, out_features=300, bias=True)\n  (fc3): Linear(in_features=300, out_features=4, bias=True)\n)":	{
                    "_backward_hooks":	{},
                    "_backward_pre_hooks":	{},
                    "_buffers":	{},
                    "_epsilon":	1.0,
                    "_epsilon_decay":	0.01,
                    "_epsilon_min":	0.01,
                    "_forward_hooks":	{},
                    "_forward_hooks_always_called":	{},
                    "_forward_hooks_with_kwargs":	{},
                    "_forward_pre_hooks":	{},
                    "_forward_pre_hooks_with_kwargs":	{},
                    "_is_full_backward_hook":	null,
                    "_load_state_dict_post_hooks":	{},
                    "_load_state_dict_pre_hooks":	{},
                    "_modules":	{
                        "fc1":	{
                            "Linear(in_features=8, out_features=400, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 1.6928e-01,  3.2521e-01, -2.6988e-02, -1.7304e-01,  2.3175e-01,\n        -1.4957e-01, -2.3459e-01,  1.3969e-01,  1.5298e-01,  8.4966e-02,\n        -3.5250e-01,  2.9814e-01, -2.9054e-01, -3.0610e-01, -2.8172e-01,\n         1.7564e-01, -1.2834e-01,  2.0284e-01,  1.4204e-01,  2.4222e-01,\n        -2.4576e-01, -5.4914e-02,  3.4916e-01, -2.4841e-01, -2.8597e-01,\n         1.0110e-03,  3.5018e-01,  2.6042e-01,  2.9367e-01,  1.4308e-01,\n         9.9545e-02, -7.7350e-02, -2.8926e-01, -2.5428e-01,  2.9726e-02,\n         2.5020e-01, -2.7427e-01,  3.1036e-01,  2.9712e-01, -4.2161e-02,\n         1.2566e-01,  1.5209e-01, -2.6807e-01,  3.3019e-01, -1.3468e-01,\n        -3.3487e-01, -2.4819e-01, -2.6656e-01, -2.4615e-01,  1.2741e-01,\n        -2.9459e-01,  2.4572e-01, -5.9275e-02, -1.4177e-01,  2.5381e-01,\n         1.3958e-01, -4.2778e-02,  5.8254e-02,  6.0866e-02,  2.2249e-01,\n         2.5223e-02,  2.4261e-01,  1.0293e-01,  2.6550e-01,  3.8835e-02,\n        -6.2035e-02, -9.6507e-02, -1.2906e-01, -1.0433e-01,  3.1201e-01,\n        -1.3122e-02, -9.3311e-02,  3.4098e-01,  1.6960e-02, -3.4212e-01,\n        -1.5547e-01,  3.1246e-01, -8.0172e-02,  2.1996e-02,  3.3319e-01,\n         1.8682e-01,  2.2537e-01, -2.5453e-01, -1.2110e-01,  3.1096e-01,\n         3.0950e-01,  2.9394e-01,  1.4250e-01,  2.7402e-01,  2.9193e-01,\n        -1.8581e-01, -3.1854e-01, -1.7568e-01, -2.1451e-01,  2.6626e-01,\n        -2.4289e-01, -2.4868e-01, -3.4445e-01,  2.2485e-01, -4.2598e-02,\n        -4.9565e-02,  8.1915e-02, -1.3994e-01, -1.4421e-01, -2.4371e-01,\n         3.5237e-01, -1.3067e-01, -1.5284e-01, -1.2086e-01,  1.1913e-01,\n         1.5677e-01, -5.1693e-02,  2.4563e-01, -6.8017e-02,  2.4425e-02,\n         1.7851e-01, -4.0518e-02, -4.7387e-02, -1.5696e-01, -1.4019e-01,\n        -3.3476e-01, -1.3228e-01, -1.4319e-01, -2.8882e-01,  2.0463e-01,\n         2.2407e-01,  3.4451e-01,  1.3074e-01, -3.1364e-01, -2.1335e-01,\n        -3.4141e-01, -2.4677e-01,  3.3416e-01,  1.4679e-01,  1.7834e-01,\n        -1.7045e-01, -2.3320e-01,  1.0449e-01,  1.6678e-01, -2.1656e-01,\n         1.8789e-02,  3.2583e-01, -2.2201e-01,  1.3676e-01,  1.1755e-04,\n        -1.1769e-02,  3.2639e-01,  2.9008e-01, -2.1403e-01, -8.1160e-03,\n        -1.5283e-01, -2.4495e-01,  5.0625e-02, -3.4880e-01,  1.9838e-01,\n         1.9542e-01,  1.5277e-01,  5.0640e-02,  8.4793e-02,  9.8116e-02,\n         2.3825e-01, -4.2964e-02,  6.6475e-02, -1.6235e-01,  1.1251e-01,\n         1.4428e-01,  2.5663e-01,  2.0861e-01,  3.1427e-01,  2.8165e-01,\n        -1.2991e-01, -1.2189e-01,  2.0899e-01,  1.2470e-01, -3.5257e-01,\n         2.4260e-02, -2.6441e-01,  1.7329e-01, -9.5292e-02, -3.5421e-02,\n         2.9934e-01,  5.7577e-02,  1.8596e-01,  3.4463e-01,  7.4980e-02,\n         3.2252e-01, -1.3257e-01, -4.3122e-02,  1.0286e-01, -1.1545e-01,\n        -2.6279e-01, -3.0700e-01,  2.3594e-01,  8.1867e-02, -3.4007e-01,\n        -2.0441e-01, -5.0201e-02,  2.6533e-01, -2.5679e-01,  8.7034e-02,\n         3.0536e-01, -4.2287e-02, -2.4048e-01, -3.0230e-01,  2.9463e-01,\n         2.7325e-01, -1.3004e-01,  5.6275e-02,  1.7031e-01,  2.9789e-01,\n        -2.0214e-01,  1.0431e-01,  3.4247e-01,  6.5520e-02,  3.4761e-01,\n        -9.9071e-02, -6.9092e-02, -1.9942e-01, -2.2113e-01,  2.5984e-01,\n        -1.6938e-01,  9.7861e-02,  1.4865e-01, -3.2608e-01, -5.2628e-02,\n        -1.8198e-01,  1.8704e-01,  2.9894e-01,  2.2021e-02, -1.9153e-01,\n         2.2494e-01,  1.8812e-01,  1.9619e-01, -4.1992e-02,  1.2357e-01,\n         3.0500e-01, -3.3782e-02, -6.2357e-02,  1.6575e-01, -1.7271e-01,\n         1.1159e-01, -2.1975e-01, -2.3268e-01, -2.8421e-01, -3.0847e-01,\n        -3.3837e-01,  2.6264e-01,  6.8359e-02, -1.0479e-01,  1.4060e-01,\n         1.6063e-01,  2.4696e-02, -1.1584e-01,  2.4132e-01,  7.8921e-02,\n        -2.4671e-01,  3.4075e-01,  2.2351e-02, -2.4339e-01,  1.5629e-01,\n         4.4455e-02,  5.3772e-02,  1.7742e-01, -1.4985e-01, -1.7545e-01,\n         5.5184e-02,  3.3734e-01,  1.4197e-01, -3.3746e-01,  1.0503e-01,\n         6.5678e-02,  9.1120e-02,  2.6869e-01,  2.7420e-02,  1.0892e-01,\n        -2.2925e-01, -3.0821e-01,  2.0573e-01, -3.0960e-02,  1.4246e-02,\n         1.0740e-01,  3.4207e-01,  5.8712e-02,  9.9604e-02, -2.9282e-01,\n        -3.0780e-01, -1.4563e-01, -1.8584e-01, -2.6728e-01, -3.4431e-01,\n         2.6736e-01,  2.7774e-01, -1.1257e-01, -9.4208e-02,  3.5336e-01,\n        -8.7333e-02, -1.8085e-01,  5.4745e-03, -1.1620e-03, -1.9392e-01,\n        -1.1474e-01, -5.5521e-02, -2.2608e-01,  2.3834e-01,  1.3473e-01,\n         5.2200e-02, -3.1412e-01,  2.7282e-01,  1.0593e-01,  2.1800e-01,\n         1.3150e-01,  3.1345e-01,  2.0129e-01, -2.0885e-01, -3.3780e-01,\n        -9.0494e-02, -1.8191e-01, -3.1239e-01, -1.1454e-01, -1.5464e-01,\n        -5.7015e-02,  4.5356e-03, -1.9579e-01, -3.1309e-01, -2.3786e-01,\n         2.3500e-02, -6.5720e-02,  8.6583e-02,  6.4685e-02,  1.1707e-01,\n        -5.0499e-02,  2.5837e-01, -3.2634e-01,  1.9375e-01, -3.1607e-01,\n        -1.1657e-01,  1.1293e-01,  3.7859e-03, -5.8765e-02,  7.5232e-02,\n        -1.2129e-01,  3.2567e-01,  6.8804e-02,  1.9635e-01, -7.2018e-02,\n        -3.2684e-01,  4.0576e-02, -3.1593e-01, -5.5133e-03, -1.5265e-01,\n        -3.5078e-01,  1.1759e-01, -2.2536e-01,  3.1866e-01,  9.0442e-02,\n         2.3809e-01, -1.2002e-01,  1.5672e-01, -2.5126e-01,  2.8008e-01,\n        -2.4784e-01, -7.1332e-02,  5.4203e-03, -3.0832e-02,  6.3752e-02,\n        -3.2637e-01,  1.7580e-01, -4.3077e-03, -3.3657e-01, -2.4818e-01,\n         1.3380e-01,  2.5523e-01,  5.5468e-02,  1.6040e-01, -2.7776e-01,\n        -1.9689e-01, -2.7001e-01,  2.9551e-02,  3.1044e-01, -1.1236e-01,\n         1.6855e-01,  2.4305e-01,  3.4123e-01, -1.9958e-01, -3.9364e-03,\n         2.7268e-01,  4.3291e-02,  2.8499e-01, -1.7999e-01, -2.7907e-01,\n        -4.1541e-02,  1.7641e-01, -2.6184e-01, -1.2728e-01,  1.6940e-01,\n        -3.0378e-01,  5.7271e-02, -2.2257e-01, -2.9773e-01,  9.8253e-02],\n       requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.2899,  0.2609, -0.1778,  ...,  0.2445,  0.0644,  0.0075],\n        [ 0.2931,  0.2139,  0.0261,  ..., -0.2397,  0.0223,  0.2035],\n        [-0.2688, -0.3201, -0.1898,  ..., -0.0008,  0.3423,  0.1924],\n        ...,\n        [-0.1723, -0.3075, -0.1593,  ...,  0.2345,  0.1131,  0.0038],\n        [ 0.1035,  0.2223, -0.0456,  ...,  0.1174, -0.0259,  0.1535],\n        [-0.1632, -0.2198,  0.0985,  ...,  0.1539, -0.1751,  0.2852]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	8,
                                "out_features":	400,
                                "training":	false
                            }
                        },
                        "fc2":	{
                            "Linear(in_features=400, out_features=300, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0341, -0.0237,  0.0112, -0.0159, -0.0140, -0.0341,  0.0371, -0.0392,\n         0.0056, -0.0077,  0.0449, -0.0140,  0.0017, -0.0444,  0.0027,  0.0288,\n         0.0480, -0.0131, -0.0368,  0.0245, -0.0433,  0.0283, -0.0016, -0.0386,\n        -0.0455,  0.0020,  0.0364,  0.0287,  0.0276, -0.0046,  0.0073, -0.0161,\n         0.0041,  0.0143, -0.0155, -0.0307, -0.0437, -0.0078, -0.0132, -0.0283,\n         0.0311,  0.0311,  0.0257,  0.0106,  0.0234,  0.0337, -0.0409,  0.0141,\n         0.0348,  0.0286, -0.0008,  0.0004,  0.0277, -0.0182, -0.0355, -0.0301,\n        -0.0388,  0.0285, -0.0443, -0.0379, -0.0079, -0.0321,  0.0253, -0.0410,\n         0.0261, -0.0419, -0.0360,  0.0352,  0.0436, -0.0352,  0.0283,  0.0382,\n        -0.0252,  0.0144, -0.0383,  0.0215,  0.0100, -0.0397, -0.0358, -0.0360,\n         0.0243, -0.0455,  0.0014, -0.0174,  0.0115, -0.0493, -0.0058, -0.0247,\n        -0.0371, -0.0445, -0.0352,  0.0349,  0.0400,  0.0436,  0.0299,  0.0134,\n        -0.0434, -0.0268,  0.0027,  0.0424,  0.0138,  0.0334, -0.0366, -0.0491,\n         0.0309, -0.0093, -0.0359, -0.0287,  0.0118, -0.0205,  0.0310,  0.0177,\n         0.0277, -0.0015, -0.0365,  0.0497, -0.0348, -0.0385,  0.0121, -0.0273,\n         0.0058, -0.0037,  0.0073, -0.0376, -0.0041,  0.0453,  0.0230, -0.0019,\n        -0.0133,  0.0307, -0.0371, -0.0430,  0.0196, -0.0474,  0.0216, -0.0359,\n         0.0223, -0.0080, -0.0364,  0.0421,  0.0343,  0.0302,  0.0004,  0.0241,\n         0.0020,  0.0377,  0.0114,  0.0340,  0.0063, -0.0286,  0.0309,  0.0342,\n         0.0207,  0.0240, -0.0409, -0.0045,  0.0495, -0.0058, -0.0048, -0.0439,\n        -0.0095,  0.0499,  0.0329, -0.0313,  0.0030,  0.0149,  0.0399,  0.0261,\n         0.0363, -0.0326, -0.0035, -0.0303, -0.0270,  0.0201,  0.0042,  0.0349,\n         0.0383, -0.0346, -0.0264,  0.0272,  0.0358, -0.0148,  0.0439, -0.0163,\n         0.0020,  0.0499, -0.0297, -0.0284, -0.0437,  0.0058,  0.0077,  0.0469,\n        -0.0476, -0.0459,  0.0054,  0.0231, -0.0439,  0.0336,  0.0136, -0.0422,\n        -0.0142,  0.0071, -0.0285,  0.0368,  0.0235,  0.0066,  0.0159,  0.0307,\n         0.0102,  0.0265, -0.0281, -0.0098,  0.0486, -0.0325,  0.0126,  0.0295,\n         0.0494,  0.0389, -0.0438, -0.0078, -0.0248, -0.0473, -0.0411, -0.0092,\n        -0.0325, -0.0294,  0.0169,  0.0316, -0.0292, -0.0089, -0.0035,  0.0310,\n         0.0388, -0.0390,  0.0086,  0.0040, -0.0057,  0.0369,  0.0153,  0.0298,\n        -0.0380, -0.0391, -0.0498, -0.0498, -0.0064, -0.0338,  0.0269,  0.0009,\n        -0.0027,  0.0389, -0.0044,  0.0302, -0.0467, -0.0022, -0.0454, -0.0003,\n        -0.0377, -0.0346,  0.0380, -0.0214,  0.0226,  0.0476, -0.0261,  0.0070,\n         0.0218, -0.0404,  0.0401, -0.0005,  0.0366,  0.0387, -0.0403,  0.0295,\n         0.0098,  0.0376,  0.0015,  0.0092, -0.0395, -0.0306,  0.0225, -0.0477,\n        -0.0486, -0.0340, -0.0343,  0.0086, -0.0453,  0.0442,  0.0399,  0.0141,\n         0.0142,  0.0179, -0.0200, -0.0482,  0.0210,  0.0108, -0.0013, -0.0364,\n        -0.0231, -0.0076, -0.0109,  0.0363], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[ 0.0422,  0.0012, -0.0289,  ..., -0.0115, -0.0313,  0.0162],\n        [-0.0362, -0.0411, -0.0301,  ..., -0.0034, -0.0305, -0.0038],\n        [-0.0028, -0.0113,  0.0158,  ..., -0.0056,  0.0417, -0.0394],\n        ...,\n        [-0.0292, -0.0143, -0.0189,  ..., -0.0126, -0.0332,  0.0351],\n        [ 0.0492,  0.0107,  0.0414,  ..., -0.0173, -0.0352, -0.0227],\n        [ 0.0134, -0.0095, -0.0372,  ...,  0.0239,  0.0253,  0.0111]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	400,
                                "out_features":	300,
                                "training":	false
                            }
                        },
                        "fc3":	{
                            "Linear(in_features=300, out_features=4, bias=True)":	{
                                "_backward_hooks":	{},
                                "_backward_pre_hooks":	{},
                                "_buffers":	{},
                                "_forward_hooks":	{},
                                "_forward_hooks_always_called":	{},
                                "_forward_hooks_with_kwargs":	{},
                                "_forward_pre_hooks":	{},
                                "_forward_pre_hooks_with_kwargs":	{},
                                "_is_full_backward_hook":	null,
                                "_load_state_dict_post_hooks":	{},
                                "_load_state_dict_pre_hooks":	{},
                                "_modules":	{},
                                "_non_persistent_buffers_set":	"set()",
                                "_parameters":	{
                                    "bias":	"Parameter containing:\ntensor([ 0.0174,  0.0310, -0.0070,  0.0326], requires_grad=True)",
                                    "weight":	"Parameter containing:\ntensor([[-0.0274, -0.0222,  0.0567,  ..., -0.0476,  0.0140,  0.0131],\n        [-0.0411,  0.0484, -0.0350,  ..., -0.0410,  0.0233,  0.0216],\n        [ 0.0566,  0.0047, -0.0133,  ..., -0.0523, -0.0151,  0.0166],\n        [ 0.0031,  0.0543, -0.0271,  ...,  0.0207, -0.0397, -0.0174]],\n       requires_grad=True)"
                                },
                                "_state_dict_hooks":	{},
                                "_state_dict_pre_hooks":	{},
                                "in_features":	300,
                                "out_features":	4,
                                "training":	false
                            }
                        }
                    },
                    "_non_persistent_buffers_set":	"set()",
                    "_parameters":	{},
                    "_state_dict_hooks":	{},
                    "_state_dict_pre_hooks":	{},
                    "act_dim":	4,
                    "custom_network_flag":	false,
                    "kernel_dim":	2,
                    "kernel_size":	4,
                    "training":	false
                }
            },
            "_target_net_update_freq":	100,
            "_train_q_iters":	80,
            "_train_update_freq":	3,
            "_traj_per_epoch":	6,
            "logger":	{
                "<utils.logger.EpochLogger object at 0x7d04bb0c7bd0>":	{
                    "epoch_dict":	{},
                    "exp_name":	"rl4sys-dqn-info",
                    "first_row":	true,
                    "log_current_row":	{},
                    "log_headers":	[],
                    "output_dir":	"/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s558240000",
                    "output_file":	{
                        "<_io.TextIOWrapper name='/home/tybg/Documents/GitHub/RL4Sys/examples/lunar/./logs/rl4sys-dqn-info/rl4sys-dqn-info_s558240000/progress.txt' mode='w' encoding='UTF-8'>":	{
                            "mode":	"w"
                        }
                    }
                }
            }
        }
    },
    "target_net_update_freq":	100,
    "train_q_iters":	80,
    "train_update_freq":	3,
    "traj_per_epoch":	6
}